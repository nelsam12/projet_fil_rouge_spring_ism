/** @file mdb.c
 *	@brief Lightning memory-mapped database library
 *
 *	A Btree-based database management library modeled loosely on the
 *	BerkeleyDB API, but much simplified.
 */
/*
 * Copyright 2011-2021 Howard Chu, Symas Corp.
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted only as authorized by the OpenLDAP
 * Public License.
 *
 * A copy of this license is available in the file LICENSE in the
 * top-level directory of the distribution or, alternatively, at
 * <http://www.OpenLDAP.org/license.html>.
 *
 * This code is derived from btree.c written by Martin Hedenfalk.
 *
 * Copyright (c) 2009, 2010 Martin Hedenfalk <martin@bzero.se>
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */
#ifndef _GNU_SOURCE
#define _GNU_SOURCE 1
#endif
#if defined(MDB_VL32) || defined(__WIN64__)
#define _FILE_OFFSET_BITS	64
#endif
#ifdef _WIN32
#include <malloc.h>
#include <windows.h>
#include <wchar.h>				/* get wcscpy() */

/* We use native NT APIs to setup the memory map, so that we can
 * let the DB file grow incrementally instead of always preallocating
 * the full size. These APIs are defined in <wdm.h> and <ntifs.h>
 * but those headers are meant for driver-level development and
 * conflict with the regular user-level headers, so we explicitly
 * declare them here. We get pointers to these functions from
 * NTDLL.DLL at runtime, to avoid buildtime dependencies on any
 * NTDLL import libraries.
 */
typedef NTSTATUS (WINAPI NtCreateSectionFunc)
	(OUT PHANDLE sh, IN ACCESS_MASK acc,
	IN void * oa OPTIONAL,
	IN PLARGE_INTEGER ms OPTIONAL,
	IN ULONG pp, IN ULONG aa, IN HANDLE fh OPTIONAL);

static NtCreateSectionFunc *NtCreateSection;

typedef enum _SECTION_INHERIT {
	ViewShare = 1,
	ViewUnmap = 2
} SECTION_INHERIT;

typedef NTSTATUS (WINAPI NtMapViewOfSectionFunc)
	(IN PHANDLE sh, IN HANDLE ph,
	IN OUT PVOID *addr, IN ULONG_PTR zbits,
	IN SIZE_T cs, IN OUT PLARGE_INTEGER off OPTIONAL,
	IN OUT PSIZE_T vs, IN SECTION_INHERIT ih,
	IN ULONG at, IN ULONG pp);

static NtMapViewOfSectionFunc *NtMapViewOfSection;

typedef NTSTATUS (WINAPI NtCloseFunc)(HANDLE h);

static NtCloseFunc *NtClose;

/** getpid() returns int; MinGW defines pid_t but MinGW64 typedefs it
 *	as int64 which is wrong. MSVC doesn't define it at all, so just
 *	don't use it.
 */
#define MDB_PID_T	int
#define MDB_THR_T	DWORD
#include <sys/types.h>
#include <sys/stat.h>
#ifdef __GNUC__
# include <sys/param.h>
#else
# define LITTLE_ENDIAN	1234
# define BIG_ENDIAN	4321
# define BYTE_ORDER	LITTLE_ENDIAN
# ifndef SSIZE_MAX
#	define SSIZE_MAX	INT_MAX
# endif
#endif
#define MDB_OFF_T	int64_t
#else
#include <sys/types.h>
#include <sys/stat.h>
#define MDB_PID_T	pid_t
#define MDB_THR_T	pthread_t
#include <sys/param.h>
#include <sys/uio.h>
#include <sys/mman.h>
#ifdef HAVE_SYS_FILE_H
#include <sys/file.h>
#endif
#include <fcntl.h>
#define MDB_OFF_T	off_t
#endif
#if defined(__APPLE__) || defined(__MACH__)
#include <sys/sysctl.h>
#endif

#if defined(__mips) && defined(__linux)
/* MIPS has cache coherency issues, requires explicit cache control */
#include <asm/cachectl.h>
extern int cacheflush(char *addr, int nbytes, int cache);
#define CACHEFLUSH(addr, bytes, cache)	cacheflush(addr, bytes, cache)
#else
#define CACHEFLUSH(addr, bytes, cache)
#endif

#if defined(__linux) && !defined(MDB_FDATASYNC_WORKS)
/** fdatasync is broken on ext3/ext4fs on older kernels, see
 *	description in #mdb_env_open2 comments. You can safely
 *	define MDB_FDATASYNC_WORKS if this code will only be run
 *	on kernels 3.6 and newer.
 */
#define	BROKEN_FDATASYNC
#endif

#include <errno.h>
#include <limits.h>
#include <stddef.h>
#include <inttypes.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <time.h>

#ifdef _MSC_VER
#include <io.h>
typedef SSIZE_T	ssize_t;
#else
#include <unistd.h>
#endif

#if defined(__sun) || defined(__ANDROID__)
/* Most platforms have posix_memalign, older may only have memalign */
#define HAVE_MEMALIGN	1
#include <malloc.h>
/* On Solaris, we need the POSIX sigwait function */
#if defined (__sun)
# define _POSIX_PTHREAD_SEMANTICS	1
#endif
#endif

#if !(defined(BYTE_ORDER) || defined(__BYTE_ORDER))
#include <netinet/in.h>
#include <resolv.h>	/* defines BYTE_ORDER on HPUX and Solaris */
#endif

#if defined(__FreeBSD__) && defined(__FreeBSD_version) && __FreeBSD_version >= 1100110
# define MDB_USE_POSIX_MUTEX	1
# define MDB_USE_ROBUST	1
#elif defined(__APPLE__) && !defined(MDB_USE_ROBUST)
# define MDB_USE_POSIX_SEM	1
#elif defined(__APPLE__) || defined (BSD) || defined(__FreeBSD_kernel__)
# if !(defined(MDB_USE_POSIX_MUTEX) || defined(MDB_USE_POSIX_SEM))
# define MDB_USE_SYSV_SEM	1
# endif
# define MDB_FDATASYNC		fsync
#elif defined(__ANDROID__)
# define MDB_FDATASYNC		fsync
#endif
#if defined(__APPLE__)
# define MDB_FDATASYNC(fd)		fcntl(fd, F_FULLFSYNC) && \
	fcntl(fd, 85 /* F_BARRIERFSYNC */) &&	/* fsync + barrier */ \
		fsync(fd)
#endif

#ifndef _WIN32
#include <pthread.h>
#include <signal.h>
#ifdef MDB_USE_POSIX_SEM
# define MDB_USE_HASH		1
#include <semaphore.h>
#elif defined(MDB_USE_SYSV_SEM)
#include <sys/ipc.h>
#include <sys/sem.h>
#ifdef _SEM_SEMUN_UNDEFINED
union semun {
	int val;
	struct semid_ds *buf;
	unsigned short *array;
};
#endif /* _SEM_SEMUN_UNDEFINED */
#else
#define MDB_USE_POSIX_MUTEX	1
#endif /* MDB_USE_POSIX_SEM */
#endif /* !_WIN32 */

#if defined(_WIN32) + defined(MDB_USE_POSIX_SEM) + defined(MDB_USE_SYSV_SEM) \
	+ defined(MDB_USE_POSIX_MUTEX) != 1
# error "Ambiguous shared-lock implementation"
#endif

#ifdef USE_VALGRIND
#include <valgrind/memcheck.h>
#define VGMEMP_CREATE(h,r,z)		VALGRIND_CREATE_MEMPOOL(h,r,z)
#define VGMEMP_ALLOC(h,a,s) VALGRIND_MEMPOOL_ALLOC(h,a,s)
#define VGMEMP_FREE(h,a) VALGRIND_MEMPOOL_FREE(h,a)
#define VGMEMP_DESTROY(h)	VALGRIND_DESTROY_MEMPOOL(h)
#define VGMEMP_DEFINED(a,s)	VALGRIND_MAKE_MEM_DEFINED(a,s)
#else
#define VGMEMP_CREATE(h,r,z)
#define VGMEMP_ALLOC(h,a,s)
#define VGMEMP_FREE(h,a)
#define VGMEMP_DESTROY(h)
#define VGMEMP_DEFINED(a,s)
#endif

#ifndef BYTE_ORDER
# if (defined(_LITTLE_ENDIAN) || defined(_BIG_ENDIAN)) && !(defined(_LITTLE_ENDIAN) && defined(_BIG_ENDIAN))
/* Solaris just defines one or the other */
#	define LITTLE_ENDIAN	1234
#	define BIG_ENDIAN	4321
#	ifdef _LITTLE_ENDIAN
#	 define BYTE_ORDER	LITTLE_ENDIAN
#	else
#	 define BYTE_ORDER	BIG_ENDIAN
#	endif
# else
#	define BYTE_ORDER	 __BYTE_ORDER
# endif
#endif

#ifndef LITTLE_ENDIAN
#define LITTLE_ENDIAN	__LITTLE_ENDIAN
#endif
#ifndef BIG_ENDIAN
#define BIG_ENDIAN	__BIG_ENDIAN
#endif

#if defined(__i386) || defined(__x86_64) || defined(_M_IX86)
#define MISALIGNED_OK	1
#endif

#include "lmdb.h"
#include "midl.h"

#if (BYTE_ORDER == LITTLE_ENDIAN) == (BYTE_ORDER == BIG_ENDIAN)
# error "Unknown or unsupported endianness (BYTE_ORDER)"
#elif (-6 & 5) || CHAR_BIT!=8 || UINT_MAX!=0xffffffff || MDB_SIZE_MAX%UINT_MAX
# error "Two's complement, reasonably sized integer types, please"
#endif

#ifdef __GNUC__
/** Put infrequently used env functions in separate section */
# ifdef __APPLE__
#	define	ESECT	__attribute__ ((section("__TEXT,text_env")))
# else
#	define	ESECT	__attribute__ ((section("text_env")))
# endif
#else
#define ESECT
#endif

#ifdef _WIN32
#define CALL_CONV WINAPI
#else
#define CALL_CONV
#endif

/** @defgroup internal	LMDB Internals
 *	@{
 */
/** @defgroup compat	Compatibility Macros
 *	A bunch of macros to minimize the amount of platform-specific ifdefs
 *	needed throughout the rest of the code. When the features this library
 *	needs are similar enough to POSIX to be hidden in a one-or-two line
 *	replacement, this macro approach is used.
 *	@{
 */

	/** Features under development */
#ifndef MDB_DEVEL
#define MDB_DEVEL 0
#endif

	/** Wrapper around __func__, which is a C99 feature */
#if __STDC_VERSION__ >= 199901L
# define mdb_func_	__func__
#elif __GNUC__ >= 2 || _MSC_VER >= 1300
# define mdb_func_	__FUNCTION__
#else
/* If a debug message says <mdb_unknown>(), update the #if statements above */
# define mdb_func_	"<mdb_unknown>"
#endif

/* Internal error codes, not exposed outside liblmdb */
#define	MDB_NO_ROOT		(MDB_LAST_ERRCODE + 10)
#ifdef _WIN32
#define MDB_OWNERDEAD	((int) WAIT_ABANDONED)
#elif defined MDB_USE_SYSV_SEM
#define MDB_OWNERDEAD	(MDB_LAST_ERRCODE + 11)
#elif defined(MDB_USE_POSIX_MUTEX) && defined(EOWNERDEAD)
#define MDB_OWNERDEAD	EOWNERDEAD	/**< #LOCK_MUTEX0() result if dead owner */
#endif

#ifdef __GLIBC__
#define	GLIBC_VER	((__GLIBC__ << 16 )| __GLIBC_MINOR__)
#endif
/** Some platforms define the EOWNERDEAD error code
 * even though they don't support Robust Mutexes.
 * Compile with -DMDB_USE_ROBUST=0, or use some other
 * mechanism like -DMDB_USE_SYSV_SEM instead of
 * -DMDB_USE_POSIX_MUTEX. (SysV semaphores are
 * also Robust, but some systems don't support them
 * either.)
 */
#ifndef MDB_USE_ROBUST
/* Android currently lacks Robust Mutex support. So does glibc < 2.4. */
# if defined(MDB_USE_POSIX_MUTEX) && (defined(__ANDROID__) || \
	(defined(__GLIBC__) && GLIBC_VER < 0x020004))
#	define MDB_USE_ROBUST	0
# else
#	define MDB_USE_ROBUST	1
# endif
#endif /* !MDB_USE_ROBUST */

#if defined(MDB_USE_POSIX_MUTEX) && (MDB_USE_ROBUST)
/* glibc < 2.12 only provided _np API */
#	if (defined(__GLIBC__) && GLIBC_VER < 0x02000c) || \
	(defined(PTHREAD_MUTEX_ROBUST_NP) && !defined(PTHREAD_MUTEX_ROBUST))
#	 define PTHREAD_MUTEX_ROBUST	PTHREAD_MUTEX_ROBUST_NP
#	 define pthread_mutexattr_setrobust(attr, flag)	pthread_mutexattr_setrobust_np(attr, flag)
#	 define pthread_mutex_consistent(mutex)	pthread_mutex_consistent_np(mutex)
#	endif
#endif /* MDB_USE_POSIX_MUTEX && MDB_USE_ROBUST */

#if defined(MDB_OWNERDEAD) && (MDB_USE_ROBUST)
#define MDB_ROBUST_SUPPORTED	1
#endif

#ifdef _WIN32
#define MDB_USE_HASH	1
#define MDB_PIDLOCK	0
#define THREAD_RET	DWORD
#define pthread_t	HANDLE
#define pthread_mutex_t	HANDLE
#define pthread_cond_t	HANDLE
typedef HANDLE mdb_mutex_t, mdb_mutexref_t;
#define pthread_key_t	DWORD
#define pthread_self()	GetCurrentThreadId()
#define pthread_key_create(x,y)	\
	((*(x) = TlsAlloc()) == TLS_OUT_OF_INDEXES ? ErrCode() : 0)
#define pthread_key_delete(x)	TlsFree(x)
#define pthread_getspecific(x)	TlsGetValue(x)
#define pthread_setspecific(x,y)	(TlsSetValue(x,y) ? 0 : ErrCode())
#define pthread_mutex_unlock(x)	ReleaseMutex(*x)
#define pthread_mutex_lock(x)	WaitForSingleObject(*x, INFINITE)
#define pthread_cond_signal(x)	SetEvent(*x)
#define pthread_cond_wait(cond,mutex)	do{SignalObjectAndWait(*mutex, *cond, INFINITE, FALSE); WaitForSingleObject(*mutex, INFINITE);}while(0)
#define THREAD_CREATE(thr,start,arg) \
	(((thr) = CreateThread(NULL, 0, start, arg, 0, NULL)) ? 0 : ErrCode())
#define THREAD_FINISH(thr) \
	(WaitForSingleObject(thr, INFINITE) ? ErrCode() : 0)
#define LOCK_MUTEX0(mutex)		WaitForSingleObject(mutex, INFINITE)
#define UNLOCK_MUTEX(mutex)		ReleaseMutex(mutex)
#define mdb_mutex_consistent(mutex)	0
#define getpid()	GetCurrentProcessId()
#define	MDB_FDATASYNC(fd)	(!FlushFileBuffers(fd))
#define	MDB_MSYNC(addr,len,flags)	(!FlushViewOfFile(addr,len))
#define	ErrCode()	GetLastError()
#define GET_PAGESIZE(x) {SYSTEM_INFO si; GetSystemInfo(&si); (x) = si.dwPageSize;}
#define	close(fd)	(CloseHandle(fd) ? 0 : -1)
#define	munmap(ptr,len)	UnmapViewOfFile(ptr)
#ifdef PROCESS_QUERY_LIMITED_INFORMATION
#define MDB_PROCESS_QUERY_LIMITED_INFORMATION PROCESS_QUERY_LIMITED_INFORMATION
#else
#define MDB_PROCESS_QUERY_LIMITED_INFORMATION 0x1000
#endif
#else
#define THREAD_RET	void *
#define THREAD_CREATE(thr,start,arg)	pthread_create(&thr,NULL,start,arg)
#define THREAD_FINISH(thr)	pthread_join(thr,NULL)

	/** For MDB_LOCK_FORMAT: True if readers take a pid lock in the lockfile */
#define MDB_PIDLOCK			1

#ifdef MDB_USE_POSIX_SEM

typedef sem_t *mdb_mutex_t, *mdb_mutexref_t;
#define LOCK_MUTEX0(mutex)		mdb_sem_wait(mutex)
#define UNLOCK_MUTEX(mutex)		sem_post(mutex)

static int
mdb_sem_wait(sem_t *sem)
{
	 int rc;
	 while ((rc = sem_wait(sem)) && (rc = errno) == EINTR) ;
	 return rc;
}

#elif defined MDB_USE_SYSV_SEM

typedef struct mdb_mutex {
	int semid;
	int semnum;
	int *locked;
} mdb_mutex_t[1], *mdb_mutexref_t;

#define LOCK_MUTEX0(mutex)		mdb_sem_wait(mutex)
#define UNLOCK_MUTEX(mutex)		do { \
	struct sembuf sb = { 0, 1, SEM_UNDO }; \
	sb.sem_num = (mutex)->semnum; \
	*(mutex)->locked = 0; \
	semop((mutex)->semid, &sb, 1); \
} while(0)

static int
mdb_sem_wait(mdb_mutexref_t sem)
{
	int rc, *locked = sem->locked;
	struct sembuf sb = { 0, -1, SEM_UNDO };
	sb.sem_num = sem->semnum;
	do {
		if (!semop(sem->semid, &sb, 1)) {
			rc = *locked ? MDB_OWNERDEAD : MDB_SUCCESS;
			*locked = 1;
			break;
		}
	} while ((rc = errno) == EINTR);
	return rc;
}

#define mdb_mutex_consistent(mutex)	0

#else	/* MDB_USE_POSIX_MUTEX: */
	/** Shared mutex/semaphore as the original is stored.
	 *
	 *	Not for copies.	Instead it can be assigned to an #mdb_mutexref_t.
	 *	When mdb_mutexref_t is a pointer and mdb_mutex_t is not, then it
	 *	is array[size 1] so it can be assigned to the pointer.
	 */
typedef pthread_mutex_t mdb_mutex_t[1];
	/** Reference to an #mdb_mutex_t */
typedef pthread_mutex_t *mdb_mutexref_t;
	/** Lock the reader or writer mutex.
	 *	Returns 0 or a code to give #mdb_mutex_failed(), as in #LOCK_MUTEX().
	 */
#define LOCK_MUTEX0(mutex)	pthread_mutex_lock(mutex)
	/** Unlock the reader or writer mutex.
	 */
#define UNLOCK_MUTEX(mutex)	pthread_mutex_unlock(mutex)
	/** Mark mutex-protected data as repaired, after death of previous owner.
	 */
#define mdb_mutex_consistent(mutex)	pthread_mutex_consistent(mutex)
#endif	/* MDB_USE_POSIX_SEM || MDB_USE_SYSV_SEM */

	/** Get the error code for the last failed system function.
	 */
#define	ErrCode()	errno

	/** An abstraction for a file handle.
	 *	On POSIX systems file handles are small integers. On Windows
	 *	they're opaque pointers.
	 */
#define	HANDLE	int

	/**	A value for an invalid file handle.
	 *	Mainly used to initialize file variables and signify that they are
	 *	unused.
	 */
#define INVALID_HANDLE_VALUE	(-1)

	/** Get the size of a memory page for the system.
	 *	This is the basic size that the platform's memory manager uses, and is
	 *	fundamental to the use of memory-mapped files.
	 */
#define	GET_PAGESIZE(x)	((x) = sysconf(_SC_PAGE_SIZE))
#endif

#define	Z	MDB_FMT_Z	/**< printf/scanf format modifier for size_t */
#define	Yu	MDB_PRIy(u)	/**< printf format for #mdb_size_t */
#define	Yd	MDB_PRIy(d)	/**< printf format for 'signed #mdb_size_t' */

#ifdef MDB_USE_SYSV_SEM
#define MNAME_LEN	(sizeof(int))
#else
#define MNAME_LEN	(sizeof(pthread_mutex_t))
#endif

/** Initial part of #MDB_env.%me_mutexname[].
 *	Changes to this code must be reflected in #MDB_LOCK_FORMAT.
 */
#ifdef _WIN32
#define MUTEXNAME_PREFIX		"Global\\MDB"
#elif defined MDB_USE_POSIX_SEM
#define MUTEXNAME_PREFIX		"/MDB"
#endif

/** @} */

#ifdef MDB_ROBUST_SUPPORTED
	/** Lock mutex, handle any error, set rc = result.
	 *	Return 0 on success, nonzero (not rc) on error.
	 */
#define LOCK_MUTEX(rc, env, mutex) \
	(((rc) = LOCK_MUTEX0(mutex)) && \
	 ((rc) = mdb_mutex_failed(env, mutex, rc)))
static int mdb_mutex_failed(MDB_env *env, mdb_mutexref_t mutex, int rc);
#else
#define LOCK_MUTEX(rc, env, mutex) ((rc) = LOCK_MUTEX0(mutex))
#define mdb_mutex_failed(env, mutex, rc) (rc)
#endif

#ifndef _WIN32
/**	A flag for opening a file and requesting synchronous data writes.
 *	This is only used when writing a meta page. It's not strictly needed;
 *	we could just do a normal write and then immediately perform a flush.
 *	But if this flag is available it saves us an extra system call.
 *
 *	@note If O_DSYNC is undefined but exists in /usr/include,
 * preferably set some compiler flag to get the definition.
 */
#ifndef MDB_DSYNC
# ifdef O_DSYNC
# define MDB_DSYNC	O_DSYNC
# else
# define MDB_DSYNC	O_SYNC
# endif
#endif
#endif

/** Function for flushing the data of a file. Define this to fsync
 *	if fdatasync() is not supported.
 */
#ifndef MDB_FDATASYNC
# define MDB_FDATASYNC	fdatasync
#endif

#ifndef MDB_MSYNC
# define MDB_MSYNC(addr,len,flags)	msync(addr,len,flags)
#endif

#ifndef MS_SYNC
#define	MS_SYNC	1
#endif

#ifndef MS_ASYNC
#define	MS_ASYNC	0
#endif

	/** A page number in the database.
	 *	Note that 64 bit page numbers are overkill, since pages themselves
	 *	already represent 12-13 bits of addressable memory, and the OS will
	 *	always limit applications to a maximum of 63 bits of address space.
	 *
	 *	@note In the #MDB_node structure, we only store 48 bits of this value,
	 *	which thus limits us to only 60 bits of addressable data.
	 */
typedef MDB_ID	pgno_t;

	/** A transaction ID.
	 *	See struct MDB_txn.mt_txnid for details.
	 */
typedef MDB_ID	txnid_t;

/** @defgroup debug	Debug Macros
 *	@{
 */
#ifndef MDB_DEBUG
	/**	Enable debug output.	Needs variable argument macros (a C99 feature).
	 *	Set this to 1 for copious tracing. Set to 2 to add dumps of all IDLs
	 *	read from and written to the database (used for free space management).
	 */
#define MDB_DEBUG 0
#endif

#if MDB_DEBUG
static int mdb_debug;
static txnid_t mdb_debug_start;

	/**	Print a debug message with printf formatting.
	 *	Requires double parenthesis around 2 or more args.
	 */
# define DPRINTF(args) ((void) ((mdb_debug) && DPRINTF0 args))
# define DPRINTF0(fmt, ...) \
	fprintf(stderr, "%s:%d " fmt "\n", mdb_func_, __LINE__, __VA_ARGS__)
#else
# define DPRINTF(args)	((void) 0)
#endif
	/**	Print a debug string.
	 *	The string is printed literally, with no format processing.
	 */
#define DPUTS(arg)	DPRINTF(("%s", arg))
	/** Debugging output value of a cursor DBI: Negative in a sub-cursor. */
#define DDBI(mc) \
	(((mc)->mc_flags & C_SUB) ? -(int)(mc)->mc_dbi : (int)(mc)->mc_dbi)
/** @} */

	/**	@brief The maximum size of a database page.
	 *
	 *	It is 32k or 64k, since value-PAGEBASE must fit in
	 *	#MDB_page.%mp_upper.
	 *
	 *	LMDB will use database pages < OS pages if needed.
	 *	That causes more I/O in write transactions: The OS must
	 *	know (read) the whole page before writing a partial page.
	 *
	 *	Note that we don't currently support Huge pages. On Linux,
	 *	regular data files cannot use Huge pages, and in general
	 *	Huge pages aren't actually pageable. We rely on the OS
	 *	demand-pager to read our data and page it out when memory
	 *	pressure from other processes is high. So until OSs have
	 *	actual paging support for Huge pages, they're not viable.
	 */
#define MAX_PAGESIZE	 (PAGEBASE ? 0x10000 : 0x8000)

	/** The minimum number of keys required in a database page.
	 *	Setting this to a larger value will place a smaller bound on the
	 *	maximum size of a data item. Data items larger than this size will
	 *	be pushed into overflow pages instead of being stored directly in
	 *	the B-tree node. This value used to default to 4. With a page size
	 *	of 4096 bytes that meant that any item larger than 1024 bytes would
	 *	go into an overflow page. That also meant that on average 2-3KB of
	 *	each overflow page was wasted space. The value cannot be lower than
	 *	2 because then there would no longer be a tree structure. With this
	 *	value, items larger than 2KB will go into overflow pages, and on
	 *	average only 1KB will be wasted.
	 */
#define MDB_MINKEYS	 2

	/**	A stamp that identifies a file as an LMDB file.
	 *	There's nothing special about this value other than that it is easily
	 *	recognizable, and it will reflect any byte order mismatches.
	 */
#define MDB_MAGIC	 0xBEEFC0DE

	/**	The version number for a database's datafile format. */
#define MDB_DATA_VERSION	 ((MDB_DEVEL) ? 999 : 2)
	/**	The version number for a database's lockfile format. */
#define MDB_LOCK_VERSION	 ((MDB_DEVEL) ? 999 : 2)
	/** Number of bits representing #MDB_LOCK_VERSION in #MDB_LOCK_FORMAT.
	 *	The remaining bits must leave room for #MDB_lock_desc.
	 */
#define MDB_LOCK_VERSION_BITS 12

	/**	@brief The max size of a key we can write, or 0 for computed max.
	 *
	 *	This macro should normally be left alone or set to 0.
	 *	Note that a database with big keys or dupsort data cannot be
	 *	reliably modified by a liblmdb which uses a smaller max.
	 *	The default is 511 for backwards compat, or 0 when #MDB_DEVEL.
	 *
	 *	Other values are allowed, for backwards compat.	However:
	 *	A value bigger than the computed max can break if you do not
	 *	know what you are doing, and liblmdb <= 0.9.10 can break when
	 *	modifying a DB with keys/dupsort data bigger than its max.
	 *
	 *	Data items in an #MDB_DUPSORT database are also limited to
	 *	this size, since they're actually keys of a sub-DB.	Keys and
	 *	#MDB_DUPSORT data items must fit on a node in a regular page.
	 */
#ifndef MDB_MAXKEYSIZE
#define MDB_MAXKEYSIZE	 ((MDB_DEVEL) ? 0 : 511)
#endif

	/**	The maximum size of a key we can write to the environment. */
#if MDB_MAXKEYSIZE
#define ENV_MAXKEY(env)	(MDB_MAXKEYSIZE)
#else
#define ENV_MAXKEY(env)	((env)->me_maxkey)
#endif

	/**	@brief The maximum size of a data item.
	 *
	 *	We only store a 32 bit value for node sizes.
	 */
#define MAXDATASIZE	0xffffffffUL

#if MDB_DEBUG
	/**	Key size which fits in a #DKBUF.
	 *	@ingroup debug
	 */
#define DKBUF_MAXKEYSIZE ((MDB_MAXKEYSIZE) > 0 ? (MDB_MAXKEYSIZE) : 511)
	/**	A key buffer.
	 *	@ingroup debug
	 *	This is used for printing a hex dump of a key's contents.
	 */
#define DKBUF	char kbuf[DKBUF_MAXKEYSIZE*2+1]
	/**	Display a key in hex.
	 *	@ingroup debug
	 *	Invoke a function to display a key in hex.
	 */
#define	DKEY(x)	mdb_dkey(x, kbuf)
#else
#define	DKBUF
#define DKEY(x)	0
#endif

	/** An invalid page number.
	 *	Mainly used to denote an empty tree.
	 */
#define P_INVALID	 (~(pgno_t)0)

	/** Test if the flags \b f are set in a flag word \b w. */
#define F_ISSET(w, f)	 (((w) & (f)) == (f))

	/** Round \b n up to an even number. */
#define EVEN(n)		(((n) + 1U) & -2) /* sign-extending -2 to match n+1U */

	/** Least significant 1-bit of \b n.	n must be of an unsigned type. */
#define LOW_BIT(n)		((n) & (-(n)))

	/** (log2(\b p2) % \b n), for p2 = power of 2 and 0 < n < 8. */
#define LOG2_MOD(p2, n)	(7 - 86 / ((p2) % ((1U<<(n))-1) + 11))
	/* Explanation: Let p2 = 2**(n*y + x), x<n and M = (1U<<n)-1. Now p2 =
	 * (M+1)**y * 2**x = 2**x (mod M). Finally "/" "happens" to return 7-x.
	 */

	/** Should be alignment of \b type. Ensure it is a power of 2. */
#define ALIGNOF2(type) \
	LOW_BIT(offsetof(struct { char ch_; type align_; }, align_))

	/**	Used for offsets within a single page.
	 *	Since memory pages are typically 4 or 8KB in size, 12-13 bits,
	 *	this is plenty.
	 */
typedef uint16_t	 indx_t;

typedef unsigned long long	mdb_hash_t;

	/**	Default size of memory map.
	 *	This is certainly too small for any actual applications. Apps should always set
	 *	the size explicitly using #mdb_env_set_mapsize().
	 */
#define DEFAULT_MAPSIZE	1048576

/**	@defgroup readers	Reader Lock Table
 *	Readers don't acquire any locks for their data access. Instead, they
 *	simply record their transaction ID in the reader table. The reader
 *	mutex is needed just to find an empty slot in the reader table. The
 *	slot's address is saved in thread-specific data so that subsequent read
 *	transactions started by the same thread need no further locking to proceed.
 *
 *	If #MDB_NOTLS is set, the slot address is not saved in thread-specific data.
 *
 *	No reader table is used if the database is on a read-only filesystem, or
 *	if #MDB_NOLOCK is set.
 *
 *	Since the database uses multi-version concurrency control, readers don't
 *	actually need any locking. This table is used to keep track of which
 *	readers are using data from which old transactions, so that we'll know
 *	when a particular old transaction is no longer in use. Old transactions
 *	that have discarded any data pages can then have those pages reclaimed
 *	for use by a later write transaction.
 *
 *	The lock table is constructed such that reader slots are aligned with the
 *	processor's cache line size. Any slot is only ever used by one thread.
 *	This alignment guarantees that there will be no contention or cache
 *	thrashing as threads update their own slot info, and also eliminates
 *	any need for locking when accessing a slot.
 *
 *	A writer thread will scan every slot in the table to determine the oldest
 *	outstanding reader transaction. Any freed pages older than this will be
 *	reclaimed by the writer. The writer doesn't use any locks when scanning
 *	this table. This means that there's no guarantee that the writer will
 *	see the most up-to-date reader info, but that's not required for correct
 *	operation - all we need is to know the upper bound on the oldest reader,
 *	we don't care at all about the newest reader. So the only consequence of
 *	reading stale information here is that old pages might hang around a
 *	while longer before being reclaimed. That's actually good anyway, because
 *	the longer we delay reclaiming old pages, the more likely it is that a
 *	string of contiguous pages can be found after coalescing old pages from
 *	many old transactions together.
 *	@{
 */
	/**	Number of slots in the reader table.
	 *	This value was chosen somewhat arbitrarily. 126 readers plus a
	 *	couple mutexes fit exactly into 8KB on my development machine.
	 *	Applications should set the table size using #mdb_env_set_maxreaders().
	 */
#define DEFAULT_READERS	126

	/**	The size of a CPU cache line in bytes. We want our lock structures
	 *	aligned to this size to avoid false cache line sharing in the
	 *	lock table.
	 *	This value works for most CPUs. For Itanium this should be 128.
	 */
#ifndef CACHELINE
#define CACHELINE	64
#endif

	/**	The information we store in a single slot of the reader table.
	 *	In addition to a transaction ID, we also record the process and
	 *	thread ID that owns a slot, so that we can detect stale information,
	 *	e.g. threads or processes that went away without cleaning up.
	 *	@note We currently don't check for stale records. We simply re-init
	 *	the table when we know that we're the only process opening the
	 *	lock file.
	 */
typedef struct MDB_rxbody {
	/**	Current Transaction ID when this transaction began, or (txnid_t)-1.
	 *	Multiple readers that start at the same time will probably have the
	 *	same ID here. Again, it's not important to exclude them from
	 *	anything; all we need to know is which version of the DB they
	 *	started from so we can avoid overwriting any data used in that
	 *	particular version.
	 */
	volatile txnid_t		mrb_txnid;
	/** The process ID of the process owning this reader txn. */
	volatile MDB_PID_T	mrb_pid;
	/** The thread ID of the thread owning this txn. */
	volatile MDB_THR_T	mrb_tid;
} MDB_rxbody;

	/** The actual reader record, with cacheline padding. */
typedef struct MDB_reader {
	union {
		MDB_rxbody mrx;
		/** shorthand for mrb_txnid */
#define	mr_txnid	mru.mrx.mrb_txnid
#define	mr_pid	mru.mrx.mrb_pid
#define	mr_tid	mru.mrx.mrb_tid
		/** cache line alignment */
		char pad[(sizeof(MDB_rxbody)+CACHELINE-1) & ~(CACHELINE-1)];
	} mru;
} MDB_reader;

	/** The header for the reader table.
	 *	The table resides in a memory-mapped file. (This is a different file
	 *	than is used for the main database.)
	 *
	 *	For POSIX the actual mutexes reside in the shared memory of this
	 *	mapped file. On Windows, mutexes are named objects allocated by the
	 *	kernel; we store the mutex names in this mapped file so that other
	 *	processes can grab them. This same approach is also used on
	 *	MacOSX/Darwin (using named semaphores) since MacOSX doesn't support
	 *	process-shared POSIX mutexes. For these cases where a named object
	 *	is used, the object name is derived from a 64 bit FNV hash of the
	 *	environment pathname. As such, naming collisions are extremely
	 *	unlikely. If a collision occurs, the results are unpredictable.
	 */
typedef struct MDB_txbody {
		/** Stamp identifying this as an LMDB file. It must be set
		 *	to #MDB_MAGIC. */
	uint32_t	mtb_magic;
		/** Format of this lock file. Must be set to #MDB_LOCK_FORMAT. */
	uint32_t	mtb_format;
		/**	The ID of the last transaction committed to the database.
		 *	This is recorded here only for convenience; the value can always
		 *	be determined by reading the main database meta pages.
		 */
	volatile txnid_t		mtb_txnid;
		/** The number of slots that have been used in the reader table.
		 *	This always records the maximum count, it is not decremented
		 *	when readers release their slots.
		 */
	volatile unsigned	mtb_numreaders;
#if defined(_WIN32) || defined(MDB_USE_POSIX_SEM)
		/** Binary form of names of the reader/writer locks */
	mdb_hash_t			mtb_mutexid;
#elif defined(MDB_USE_SYSV_SEM)
	int 	mtb_semid;
	int		mtb_rlocked;
#else
		/** Mutex protecting access to this table.
		 *	This is the reader table lock used with LOCK_MUTEX().
		 */
	mdb_mutex_t	mtb_rmutex;
#endif
} MDB_txbody;

	/** The actual reader table definition. */
typedef struct MDB_txninfo {
	union {
		MDB_txbody mtb;
#define mti_magic	mt1.mtb.mtb_magic
#define mti_format	mt1.mtb.mtb_format
#define mti_rmutex	mt1.mtb.mtb_rmutex
#define mti_txnid	mt1.mtb.mtb_txnid
#define mti_numreaders	mt1.mtb.mtb_numreaders
#define mti_mutexid	mt1.mtb.mtb_mutexid
#ifdef MDB_USE_SYSV_SEM
#define	mti_semid	mt1.mtb.mtb_semid
#define	mti_rlocked	mt1.mtb.mtb_rlocked
#endif
		char pad[(sizeof(MDB_txbody)+CACHELINE-1) & ~(CACHELINE-1)];
	} mt1;
#if !(defined(_WIN32) || defined(MDB_USE_POSIX_SEM))
	union { struct {
#ifdef MDB_USE_SYSV_SEM
		int mt2_wlocked;
		int mt2_sync_locked;
#define mti_wlocked	mt2.mt2_wlocked
#define mti_sync_locked	mt2.mt2_sync_locked
#else
		mdb_mutex_t	mt2_wmutex;
		mdb_mutex_t	mt2_sync_mutex;
#define mti_wmutex	mt2.mt2_wmutex
#define mti_sync_mutex	mt2.mt2_sync_mutex
#endif
		char pad[(MNAME_LEN+CACHELINE-1) & ~(CACHELINE-1)];
		};	} mt2;
#endif
	MDB_reader	mti_readers[1];
} MDB_txninfo;

	/** Lockfile format signature: version, features and field layout */
#define MDB_LOCK_FORMAT \
	((uint32_t)				 \
	 (((MDB_LOCK_VERSION) % (1U << MDB_LOCK_VERSION_BITS)) \
		+ MDB_lock_desc		 * (1U << MDB_LOCK_VERSION_BITS)))

	/** Lock type and layout. Values 0-119. _WIN32 implies #MDB_PIDLOCK.
	 *	Some low values are reserved for future tweaks.
	 */
#ifdef _WIN32
# define MDB_LOCK_TYPE	(0 + ALIGNOF2(mdb_hash_t)/8 % 2)
#elif defined MDB_USE_POSIX_SEM
# define MDB_LOCK_TYPE	(4 + ALIGNOF2(mdb_hash_t)/8 % 2)
#elif defined MDB_USE_SYSV_SEM
# define MDB_LOCK_TYPE	(8)
#elif defined MDB_USE_POSIX_MUTEX
/* We do not know the inside of a POSIX mutex and how to check if mutexes
 * used by two executables are compatible. Just check alignment and size.
 */
# define MDB_LOCK_TYPE	(10 + \
		LOG2_MOD(ALIGNOF2(pthread_mutex_t), 5) + \
		sizeof(pthread_mutex_t) / 4U % 22 * 5)
#endif

enum {
	/** Magic number for lockfile layout and features.
	 *
	 *	This *attempts* to stop liblmdb variants compiled with conflicting
	 *	options from using the lockfile at the same time and thus breaking
	 *	it.	It describes locking types, and sizes and sometimes alignment
	 *	of the various lockfile items.
	 *
	 *	The detected ranges are mostly guesswork, or based simply on how
	 *	big they could be without using more bits.	So we can tweak them
	 *	in good conscience when updating #MDB_LOCK_VERSION.
	 */
	MDB_lock_desc =
	/* Default CACHELINE=64 vs. other values (have seen mention of 32-256) */
	(CACHELINE==64 ? 0 : 1 + LOG2_MOD(CACHELINE >> (CACHELINE>64), 5))
	+ 6	* (sizeof(MDB_PID_T)/4 % 3)		/* legacy(2) to word(4/8)? */
	+ 18 * (sizeof(pthread_t)/4 % 5)		/* can be struct{id, active data} */
	+ 90 * (sizeof(MDB_txbody) / CACHELINE % 3)
	+ 270 * (MDB_LOCK_TYPE % 120)
	/* The above is < 270*120 < 2**15 */
	+ ((sizeof(txnid_t) == 8) << 15)		/* 32bit/64bit */
	+ ((sizeof(MDB_reader) > CACHELINE) << 16)
	/* Not really needed - implied by MDB_LOCK_TYPE != (_WIN32 locking) */
	+ (((MDB_PIDLOCK) != 0)	 << 17)
	/* 18 bits total: Must be <= (32 - MDB_LOCK_VERSION_BITS). */
};
/** @} */

/** Common header for all page types. The page type depends on #mp_flags.
 *
 * #P_BRANCH and #P_LEAF pages have unsorted '#MDB_node's at the end, with
 * sorted #mp_ptrs[] entries referring to them. Exception: #P_LEAF2 pages
 * omit mp_ptrs and pack sorted #MDB_DUPFIXED values after the page header.
 *
 * #P_OVERFLOW records occupy one or more contiguous pages where only the
 * first has a page header. They hold the real data of #F_BIGDATA nodes,
 * and the node stores the pgno and number of pages used by the record.
 *
 * #P_SUBP sub-pages are small leaf "pages" with duplicate data.
 * A node with flag #F_DUPDATA but not #F_SUBDATA contains a sub-page.
 * (Duplicate data can also go in sub-databases, which use normal pages.)
 *
 * #P_META pages contain #MDB_meta, the start point of an LMDB snapshot.
 *
 * Each non-metapage up to #MDB_meta.%mm_last_pg is reachable exactly once
 * in the snapshot: Either used by a database or listed in a freeDB record.
 */
typedef struct MDB_page_header {
#define	mp_pgno	mp_p.p_pgno
#define	mp_next	mp_p.p_next
	union {
		pgno_t		p_pgno;	/**< page number */
		struct MDB_page *p_next; /**< for in-memory list of freed pages */
	} mh_p;

	/** If page is clean: snapshot txnid, dirty: txn workid, metapage: unused.
	 *
	 *	The value indicates which snapshot/#MDB_txn the page belongs to.
	 *	Tested with #IS_MUTABLE(), #IS_WRITABLE(), #IS_DIRTY_NW().
	 *
	 *	(clean page).mp_txnid == txnid of creator < txnid of later txns.
	 *	(dirty page).mp_txnid >= mt_workid of creator txn.
	 *	(dirty page).mt_txnid	< mt_workid of children of creator txn.
	 *
	 *	Thus an #MDB_txn can write to pages with mp_txnid >= txn.mt_workid.
	 *	A page with smaller mp_txnid is dirty in an ancestor txn or clean.
	 *
	 *	Non-#MDB_WRITEMAP sets txn.mt_workid > txn.mt_txnid, to tell apart
	 *	spilled and dirty pages.	WRITEMAP sets mt_workid = mt_txnid, since
	 *	it does not copy/spill pages.	Thus (page.mp_txnid == txn.mt_txnid)
	 *	says "spilled page" without WRITEMAP, "dirty page" with WRITEMAP.
	 *
	 *	Finally, ((dirty page).mp_txnid & #MDB_PGTXNID_FLAGMASK) can be used
	 *	for flags with non-WRITEMAP; it keeps low bits in workid = 0.
	 */
	txnid_t		mh_txnid;

	uint16_t	mh_pad;			/**< key size if this is a LEAF2 page */
/**	@defgroup mdb_page	Page Flags
 *	@ingroup internal
 *	Flags for the page headers.
 *	@{
 */
#define	P_BRANCH	 0x01		/**< branch page */
#define	P_LEAF		 0x02		/**< leaf page */
#define	P_OVERFLOW	 0x04		/**< overflow page */
#define	P_META		 0x08		/**< meta page */
#define	P_LEAF2		 0x20		/**< for #MDB_DUPFIXED records */
#define	P_SUBP		 0x40		/**< for #MDB_DUPSORT sub-pages */
#define	P_DIRTY_OVF	 0x2000		/**< page has dirty overflow nodes */
#define	P_LOOSE		 0x4000		/**< page was dirtied then freed, can be reused */
#define	P_KEEP		 0x8000		/**< leave this page alone during spill */
/** Persistent flags for page administration rather than page contents */
#define	P_ADM_FLAGS	 0 /* later... */
/** @} */
	uint16_t	mh_flags;		/**< @ref mdb_page */
#define mp_lower	mp_pb.pb.pb_lower
#define mp_upper	mp_pb.pb.pb_upper
#define mp_pages	mp_pb.pb_pages
	union {
		struct {
			indx_t		pb_lower;		/**< lower bound of free space */
			indx_t		pb_upper;		/**< upper bound of free space */
		} pb;
		uint32_t	pb_pages;	/**< number of overflow pages */
	} mh_pb;
} MDB_page_header;

typedef struct MDB_page {
	MDB_page_header	mp_hdr;
#define mp_p	mp_hdr.mh_p
#define mp_txnid	mp_hdr.mh_txnid
#define mp_pad		mp_hdr.mh_pad
#define mp_flags	mp_hdr.mh_flags
#define mp_pb		mp_hdr.mh_pb
	indx_t		mp_ptrs[1];		/**< dynamic size */
} MDB_page;

	/** Size of the page header, excluding dynamic data at the end */
#define PAGEHDRSZ	 ((unsigned)sizeof(MDB_page_header))

	/** Address of first usable data byte in a page, after the header */
#define METADATA(p)	 ((void *)((char *)(p) + PAGEHDRSZ))

	/** ITS#7713, change PAGEBASE to handle 65536 byte pages */
#define	PAGEBASE	PAGEHDRSZ

	/** Number of nodes on a page */
#define NUMKEYS(p)	 (((p)->mp_lower - (PAGEHDRSZ-PAGEBASE)) >> 1)

	/** The amount of space remaining in the page */
#define SIZELEFT(p)	 (indx_t)((p)->mp_upper - (p)->mp_lower)

	/** The percentage of space used in the page, in tenths of a percent. */
#define PAGEFILL(env, p) (1000L * ((env)->me_psize - PAGEHDRSZ - SIZELEFT(p)) / \
				((env)->me_psize - PAGEHDRSZ))
	/** The minimum page fill factor, in tenths of a percent.
	 *	Pages emptier than this are candidates for merging.
	 */
#define FILL_THRESHOLD	 250

	/** Test if a page is a leaf page */
#define IS_LEAF(p)	 F_ISSET((p)->mp_flags, P_LEAF)
	/** Test if a page is a LEAF2 page */
#define IS_LEAF2(p)	 F_ISSET((p)->mp_flags, P_LEAF2)
	/** Test if a page is a branch page */
#define IS_BRANCH(p)	 F_ISSET((p)->mp_flags, P_BRANCH)
	/** Test if a page is an overflow page */
#define IS_OVERFLOW(p)	 F_ISSET((p)->mp_flags, P_OVERFLOW)
	/** Test if a page is a sub page */
#define IS_SUBP(p)	 F_ISSET((p)->mp_flags, P_SUBP)

	/** Test if (this non-sub page is dirty && env is non-#MDB_WRITEMAP) */
#define IS_DIRTY_NW(txn, p)	((p)->mp_txnid > (txn)->mt_txnid)
	/** Test if this non-sub page belongs to the current snapshot */
#define IS_MUTABLE(txn, p)	((p)->mp_txnid >= (txn)->mt_txnid)
	/** Test if this non-sub page is writable in this txn (not an ancestor) */
#define IS_WRITABLE(txn, p)	((p)->mp_txnid >= (txn)->mt_workid)

	/** Info about overflow page, stored in an F_BIGDATA node */
typedef struct MDB_ovpage {
	pgno_t	op_pgno;
	txnid_t	op_txnid;
	mdb_size_t	op_pages;
} MDB_ovpage;

#if OVERFLOW_NOTYET
	/** Header for a dirty overflow page in memory */
typedef struct MDB_dovpage {
	MDB_page_header mp_hdr;
	void	*mp_ptr;
} MDB_dovpage;
#endif

	/** The number of overflow pages needed to store the given size. */
#define OVPAGES(size, psize)	((PAGEHDRSZ-1 + (size)) / (psize) + 1)

	/** Link in #MDB_txn.%mt_loose_pgs list.
	 *	Kept outside the page header, which is needed when reusing the page.
	 */
#define NEXT_LOOSE_PAGE(p)		(*(MDB_page **)((p) + 2))

	/** Mark the page as writable by this txn */
#ifndef MDB_TEST
#define SET_PGTXNID(txn, mp)	((mp)->mp_txnid = (txn)->mt_workid)
#else
#define SET_PGTXNID(txn, mp) \
	((mp)->mp_txnid = (txn)->mt_workid \
	 /* random unused "flags" added when not WRITEMAP for debugging */ \
	 | (((txn)->mt_flags & MDB_TXN_WRITEMAP) ? 0 : \
		(MDB_RAND((size_t)(txn)) >> (32-MDB_PGTXNID_FLAGBITS))))
#define MDB_RAND(x) (mdb_rnd = (mdb_rnd + (unsigned)(x)) * 987654321 + 54321)
static volatile unsigned mdb_rnd;
#endif

	/** mp_txnid bits reserved in dirty pages for flags.
	 *	TODO: For future code with header-free ovpages, if we omit mp_flags
	 *	from the "header" kept elsewhere.	Otherwise, drop this code.
	 */
#define MDB_PGTXNID_FLAGBITS	4
#define MDB_PGTXNID_STEP		((txnid_t)1 << MDB_PGTXNID_FLAGBITS)
#define MDB_PGTXNID_FLAGMASK	(MDB_PGTXNID_STEP-1)

	/** Header for a single key/data pair within a page.
	 * Used in pages of type #P_BRANCH and #P_LEAF without #P_LEAF2.
	 * We guarantee 2-byte alignment for 'MDB_node's.
	 *
	 * #mn_lo and #mn_hi are used for data size on leaf nodes, and for child
	 * pgno on branch nodes.	On 64 bit platforms, #mn_flags is also used
	 * for pgno.	(Branch nodes have no flags).	Lo and hi are in host byte
	 * order in case some accesses can be optimized to 32-bit word access.
	 *
	 * Leaf node flags describe node contents.	#F_BIGDATA says the node's
	 * data part is an MDB_ovpage struct pointing to a page with actual data.
	 * #F_DUPDATA and #F_SUBDATA can be combined giving duplicate data in
	 * a sub-page/sub-database, and named databases (just #F_SUBDATA).
	 */
typedef struct MDB_node {
	/** part of data size or pgno
	 *	@{ */
#if BYTE_ORDER == LITTLE_ENDIAN
	unsigned short	mn_lo, mn_hi;
#else
	unsigned short	mn_hi, mn_lo;
#endif
	/** @} */
/** @defgroup mdb_node Node Flags
 *	@ingroup internal
 *	Flags for node headers.
 *	@{
 */
#define F_BIGDATA	 0x01			/**< data put on overflow page */
#define F_SUBDATA	 0x02			/**< data is a sub-database */
#define F_DUPDATA	 0x04			/**< data has duplicates */

/** valid flags for #mdb_node_add() */
#define	NODE_ADD_FLAGS	(F_DUPDATA|F_SUBDATA|MDB_RESERVE|MDB_APPEND)

/** @} */
	unsigned short	mn_flags;		/**< @ref mdb_node */
	unsigned short	mn_ksize;		/**< key size */
	char		mn_data[1];			/**< key and data are appended here */
} MDB_node;

	/** Size of the node header, excluding dynamic data at the end */
#define NODESIZE	 offsetof(MDB_node, mn_data)

	/** Bit position of top word in page number, for shifting mn_flags */
#define PGNO_TOPWORD ((pgno_t)-1 > 0xffffffffu ? 32 : 0)

	/** Size of a node in a branch page with a given key.
	 *	This is just the node header plus the key, there is no data.
	 */
#define INDXSIZE(k)	 (NODESIZE + ((k) == NULL ? 0 : (k)->mv_size))

	/** Size of a node in a leaf page with a given key and data.
	 *	This is node header plus key plus data size.
	 */
#define LEAFSIZE(k, d)	 (NODESIZE + (k)->mv_size + (d)->mv_size)

	/** Address of node \b i in page \b p */
#define NODEPTR(p, i)	 ((MDB_node *)((char *)(p) + (p)->mp_ptrs[i] + PAGEBASE))

	/** Address of the key for the node */
#define NODEKEY(node)	 (void *)((node)->mn_data)

	/** Address of the data for a node */
#define NODEDATA(node)	 (void *)((char *)(node)->mn_data + (node)->mn_ksize)

	/** Get the page number pointed to by a branch node */
#define NODEPGNO(node) \
	((node)->mn_lo | ((pgno_t) (node)->mn_hi << 16) | \
	 (PGNO_TOPWORD ? ((pgno_t) (node)->mn_flags << PGNO_TOPWORD) : 0))
	/** Set the page number in a branch node */
#define SETPGNO(node,pgno)	do { \
	(node)->mn_lo = (pgno) & 0xffff; (node)->mn_hi = (pgno) >> 16; \
	if (PGNO_TOPWORD) (node)->mn_flags = (pgno) >> PGNO_TOPWORD; } while(0)

	/** Get the size of the data in a leaf node */
#define NODEDSZ(node)	 ((node)->mn_lo | ((unsigned)(node)->mn_hi << 16))
	/** Set the size of the data for a leaf node */
#define SETDSZ(node,size)	do { \
	(node)->mn_lo = (size) & 0xffff; (node)->mn_hi = (size) >> 16;} while(0)
	/** The size of a key in a node */
#define NODEKSZ(node)	 ((node)->mn_ksize)

	/** Copy a page number from src to dst */
#ifdef MISALIGNED_OK
#define COPY_PGNO(dst,src)	dst = src
#else
#if MDB_SIZE_MAX > 0xffffffffU
#define COPY_PGNO(dst,src)	do { \
	unsigned short *s, *d;	\
	s = (unsigned short *)&(src);	\
	d = (unsigned short *)&(dst);	\
	*d++ = *s++;	\
	*d++ = *s++;	\
	*d++ = *s++;	\
	*d = *s;	\
} while (0)
#else
#define COPY_PGNO(dst,src)	do { \
	unsigned short *s, *d;	\
	s = (unsigned short *)&(src);	\
	d = (unsigned short *)&(dst);	\
	*d++ = *s++;	\
	*d = *s;	\
} while (0)
#endif
#endif
	/** The address of a key in a LEAF2 page.
	 *	LEAF2 pages are used for #MDB_DUPFIXED sorted-duplicate sub-DBs.
	 *	There are no node headers, keys are stored contiguously.
	 */
#define LEAF2KEY(p, i, ks)	((char *)(p) + PAGEHDRSZ + ((i)*(ks)))

	/** Set the \b node's key into \b keyptr, if requested. */
#define MDB_GET_KEY(node, keyptr)	{ if ((keyptr) != NULL) { \
	(keyptr)->mv_size = NODEKSZ(node); (keyptr)->mv_data = NODEKEY(node); } }

	/** Set the \b node's key into \b key. */
#define MDB_GET_KEY2(node, key)	{ key.mv_size = NODEKSZ(node); key.mv_data = NODEKEY(node); }

	/** Information about a single database in the environment. */
typedef struct MDB_db {
	uint32_t	md_pad;		/**< also ksize for LEAF2 pages */
	uint16_t	md_flags;	/**< @ref mdb_dbi_open */
	uint16_t	md_depth;	/**< depth of this tree */
	pgno_t		md_branch_pages;	/**< number of internal pages */
	pgno_t		md_leaf_pages;		/**< number of leaf pages */
	pgno_t		md_overflow_pages;	/**< number of overflow pages */
	mdb_size_t	md_entries;		/**< number of data items */
	pgno_t		md_root;		/**< the root page of this tree */
} MDB_db;

#define MDB_VALID	0x8000		/**< DB handle is valid, for me_dbflags */
#define PERSISTENT_FLAGS	(0xffff & ~(MDB_VALID))
	/** #mdb_dbi_open() flags */
#define VALID_FLAGS	(MDB_REVERSEKEY|MDB_DUPSORT|MDB_INTEGERKEY|MDB_DUPFIXED|\
	MDB_INTEGERDUP|MDB_REVERSEDUP|MDB_CREATE|0x100)

	/** Handle for the DB used to track free pages. */
#define	FREE_DBI	0
	/** Handle for the default DB. */
#define	MAIN_DBI	1
	/** Number of DBs in metapage (free and main) - also hardcoded elsewhere */
#define CORE_DBS	2

	/** Number of meta pages - also hardcoded elsewhere */
#define NUM_METAS	2

	/** Meta page content.
	 *	A meta page is the start point for accessing a database snapshot.
	 *	Pages 0-1 are meta pages. Transaction N writes meta page #(N % 2).
	 */
typedef struct MDB_meta {
		/** Stamp identifying this as an LMDB file. It must be set
		 *	to #MDB_MAGIC. */
	uint32_t	mm_magic;
		/** Version number of this file. Must be set to #MDB_DATA_VERSION. */
	uint32_t	mm_version;
#ifdef MDB_VL32
	union {		/* always zero since we don't support fixed mapping in MDB_VL32 */
		MDB_ID	mmun_ull;
		void *mmun_address;
	} mm_un;
#define	mm_address mm_un.mmun_address
#else
	void		*mm_address;		/**< address for fixed mapping */
#endif
	mdb_size_t	mm_mapsize;			/**< size of mmap region */
	MDB_db		mm_dbs[CORE_DBS];	/**< first is free space, 2nd is main db */
	/** The size of pages used in this DB */
#define	mm_psize	mm_dbs[FREE_DBI].md_pad
	/** Any persistent environment flags. @ref mdb_env */
#define	mm_flags	mm_dbs[FREE_DBI].md_flags
	/** Last used page in the datafile.
	 *	Actually the file may be shorter if the freeDB lists the final pages.
	 */
	pgno_t		mm_last_pg;
	volatile txnid_t	mm_txnid;	/**< txnid that committed this page */
	int64_t		boot_id;
} MDB_meta;

	/** Buffer for a stack-allocated meta page.
	 *	The members define size and alignment, and silence type
	 *	aliasing warnings.	They are not used directly; that could
	 *	mean incorrectly using several union members in parallel.
	 */
typedef union MDB_metabuf {
	MDB_page	mb_page;
	struct {
		char		mm_pad[PAGEHDRSZ];
		MDB_meta	mm_meta;
	} mb_metabuf;
} MDB_metabuf;

	/** Auxiliary DB info.
	 *	The information here is mostly static/read-only. There is
	 *	only a single copy of this record in the environment.
	 */
typedef struct MDB_dbx {
	MDB_val		md_name;		/**< name of the database */
	MDB_cmp_func	*md_cmp;	/**< function for comparing keys */
	MDB_cmp_func	*md_dcmp;	/**< function for comparing data items */
	MDB_rel_func	*md_rel;	/**< user relocate function */
	void		*md_relctx;		/**< user-provided context for md_rel */
} MDB_dbx;

	/** A database transaction.
	 *	Every operation requires a transaction handle.
	 */
struct MDB_txn {
	MDB_txn		*mt_parent;		/**< parent of a nested txn */
	/** Nested txn under this txn, set together with flag #MDB_TXN_HAS_CHILD */
	MDB_txn		*mt_child;
	pgno_t		mt_next_pgno;	/**< next unallocated page */
#if MDB_RPAGE_CACHE
	pgno_t		mt_last_pgno;	/**< last written page */
#endif
	/** The ID of this transaction. IDs are integers incrementing from 1.
	 *	Only committed write transactions increment the ID. If a transaction
	 *	aborts, the ID may be re-used by the next writer.
	 */
	txnid_t		mt_txnid;

	/** Written to mp_txnid of dirty pages, to be fixed by #mdb_page_flush().
	 *
	 *	Value >= WRITEMAP ? txnid : 1 + (parent ? parent.last_workid : txnid).
	 *	See #MDB_page.%mp_txnid.
	 *
	 *	An MDB_txn can write to a page when page.mp_txnid >= txn.mt_workid.
	 *	New children get bigger workid than pages dirty in their parent
	 *	(i.e. bigger than parent.mt_last_workid).	When children commit,
	 *	they copy #mt_last_workid to the parent to match their pages.
	 */
	txnid_t		mt_workid;
	/**	Current max mp_txnid of the MDB_txn's dirty pages: Starts as
	 *	#mt_workid, then grows as it is copied from children who commit.
	 */
	txnid_t		mt_last_workid;
	/** Callback function for when the transaction is visible */
	MDB_txn_visible* mt_callback;
	void* mt_ctx;

	MDB_env		*mt_env;		/**< the DB environment */
	/** The list of pages that became unused during this transaction.
	 */
	MDB_IDL		mt_free_pgs;
	/** The list of loose pages that became unused and may be reused
	 *	in this transaction, linked through #NEXT_LOOSE_PAGE(page).
	 */
	MDB_page	*mt_loose_pgs;
	/** Number of loose pages (#mt_loose_pgs) */
	int			mt_loose_count;
	/** The sorted list of dirty pages we temporarily wrote to disk
	 *	because the dirty list was full. page numbers in here are
	 *	shifted left by 1, deleted slots have the LSB set.
	 *	Unused with #MDB_WRITEMAP, which does not use a dirty list.
	 */
	MDB_IDL		mt_spill_pgs;
	union {
		/** For write txns: Modified pages, sorted. Unused when MDB_WRITEMAP. */
		MDB_ID2L	dirty_list;
		/** For read txns: This thread/txn's reader table slot, or NULL. */
		MDB_reader	*reader;
	} mt_u;
#if OVERFLOW_NOTYET
	/** The sorted list of dirty overflow pages. */
	MDB_ID2L	mt_dirty_ovs;
#endif
	/** Array of records for each DB known in the environment. */
	MDB_dbx		*mt_dbxs;
	/** Array of MDB_db records for each known DB */
	MDB_db		*mt_dbs;
	/** Array of sequence numbers for each DB handle */
	unsigned int	*mt_dbiseqs;
/** @defgroup mt_dbflag	Transaction DB Flags
 *	@ingroup internal
 * @{
 */
#define DB_DIRTY	0x01		/**< DB was written in this txn */
#define DB_STALE	0x02		/**< Named-DB record is older than txnID */
#define DB_NEW		0x04		/**< Named-DB handle opened in this txn */
#define DB_VALID	0x08		/**< DB handle is valid, see also #MDB_VALID */
#define DB_USRVALID	0x10		/**< As #DB_VALID, but not set for #FREE_DBI */
#define DB_DUPDATA	0x20		/**< DB is #MDB_DUPSORT data */
/** @} */
	/** In write txns, array of cursors for each DB */
	MDB_cursor	**mt_cursors;
	/** Array of flags for each DB */
	unsigned char	*mt_dbflags;
#if MDB_RPAGE_CACHE
	/** List of read-only pages (actually chunks) */
	MDB_ID3L	mt_rpages;
	/** We map chunks of 16 pages. Even though Windows uses 4KB pages, all
	 * mappings must begin on 64KB boundaries. So we round off all pgnos to
	 * a chunk boundary. We do the same on Linux for symmetry, and also to
	 * reduce the frequency of mmap/munmap calls.
	 */
#define MDB_RPAGE_CHUNK	16
#define MDB_TRPAGE_SIZE	4096	/**< size of #mt_rpages array of chunks */
#define MDB_TRPAGE_MAX	(MDB_TRPAGE_SIZE-1)	/**< maximum chunk index */
	unsigned int mt_rpcheck;	/**< threshold for reclaiming unref'd chunks */
#endif
	/**	Number of DB records in use, or 0 when the txn is finished.
	 *	This number only ever increments until the txn finishes; we
	 *	don't decrement it when individual DB handles are closed.
	 */
	MDB_dbi		mt_numdbs;

/** @defgroup mdb_txn	Transaction Flags
 *	@ingroup internal
 *	@{
 */
	/** #mdb_txn_begin() flags */
#define MDB_TXN_BEGIN_FLAGS	(MDB_NOMETASYNC|MDB_NOSYNC|MDB_RDONLY|MDB_OVERLAPPINGSYNC)
#define MDB_TXN_NOMETASYNC	MDB_NOMETASYNC	/**< don't sync meta for this txn on commit */
#define MDB_TXN_NOSYNC		MDB_NOSYNC	/**< don't sync this txn on commit */
#define MDB_TXN_RDONLY		MDB_RDONLY	/**< read-only transaction */
	/* internal txn flags */
#define MDB_TXN_WRITEMAP	MDB_WRITEMAP	/**< copy of #MDB_env flag in writers */
#define MDB_TXN_FINISHED	0x01		/**< txn is finished or never began */
#define MDB_TXN_ERROR		0x02		/**< txn is unusable after an error */
#define MDB_TXN_DIRTY		0x04		/**< must write, even if dirty list is empty */
#define MDB_TXN_SPILLS		0x08		/**< txn or a parent has spilled pages */
#define MDB_TXN_HAS_CHILD	0x10		/**< txn has an #MDB_txn.%mt_child */
#define MDB_TXN_DIRTYNUM	0x20		/**< dirty list uses nump list */
	/** most operations on the txn are currently illegal */
#define MDB_TXN_BLOCKED		(MDB_TXN_FINISHED|MDB_TXN_ERROR|MDB_TXN_HAS_CHILD)
/** @} */
	unsigned int	mt_flags;		/**< @ref mdb_txn */
	/** #dirty_list room: Array size - \#dirty pages visible to this txn.
	 *	Includes ancestor txns' dirty pages not hidden by other txns'
	 *	dirty/spilled pages. Thus commit(nested txn) has room to merge
	 *	dirty_list into mt_parent after freeing hidden mt_parent pages.
	 *	When #MDB_WRITEMAP, it is nonzero but otherwise irrelevant.
	 */
	unsigned int	mt_dirty_room;
};

/** Enough space for 2^32 nodes with minimum of 2 keys per node. I.e., plenty.
 * At 4 keys per node, enough for 2^64 nodes, so there's probably no need to
 * raise this on a 64 bit machine.
 */
#define CURSOR_STACK		 32

struct MDB_xcursor;

	/** Cursors are used for all DB operations.
	 *	A cursor holds a path of (page pointer, key index) from the DB
	 *	root to a position in the DB, plus other state. #MDB_DUPSORT
	 *	cursors include an xcursor to the current data item. Write txns
	 *	track their cursors and keep them up to date when data moves.
	 *	Exception: An xcursor's pointer to a #P_SUBP page can be stale.
	 *	(A node with #F_DUPDATA but no #F_SUBDATA contains a subpage).
	 */
struct MDB_cursor {
	/** Next cursor on this DB in this txn */
	MDB_cursor	*mc_next;
	/** Backup of the original cursor if this cursor is a shadow */
	MDB_cursor	*mc_backup;
	/** Context used for databases with #MDB_DUPSORT, otherwise NULL */
	struct MDB_xcursor	*mc_xcursor;
	/** The transaction that owns this cursor */
	MDB_txn		*mc_txn;
	/** The database handle this cursor operates on */
	MDB_dbi		mc_dbi;
	/** The database record for this cursor */
	MDB_db		*mc_db;
	/** The database auxiliary record for this cursor */
	MDB_dbx		*mc_dbx;
	/** The @ref mt_dbflag for this database */
	unsigned char	*mc_dbflag;
	unsigned short 	mc_snum;	/**< number of pushed pages */
	unsigned short	mc_top;		/**< index of top page, normally mc_snum-1 */
/** @defgroup mdb_cursor	Cursor Flags
 *	@ingroup internal
 *	Cursor state flags.
 *	@{
 */
#define C_INITIALIZED	0x01	/**< cursor has been initialized and is valid */
#define C_EOF	0x02			/**< No more data */
#define C_SUB	0x04			/**< Cursor is a sub-cursor */
#define C_DEL	0x08			/**< last op was a cursor_del */
#define C_UNTRACK	0x40		/**< Un-track cursor when closing */
#define C_WRITEMAP	MDB_TXN_WRITEMAP /**< Copy of txn flag */
/** Read-only cursor into the txn's original snapshot in the map.
 *	Set for read-only txns, and in #mdb_page_alloc() for #FREE_DBI when
 *	#MDB_DEVEL & 2. Only implements code which is necessary for this.
 */
#define C_ORIG_RDONLY	MDB_TXN_RDONLY
/** @} */
	unsigned int	mc_flags;	/**< @ref mdb_cursor */
	MDB_page	*mc_pg[CURSOR_STACK];	/**< stack of pushed pages */
	indx_t		mc_ki[CURSOR_STACK];	/**< stack of page indices */
#if MDB_RPAGE_CACHE
	MDB_page	*mc_ovpg;		/**< a referenced overflow page */
#	define MC_OVPG(mc)			((mc)->mc_ovpg)
#	define MC_SET_OVPG(mc, pg)	\
		((mc)->mc_ovpg = \
		 (MDB_REMAPPING((mc)->mc_txn->mt_env->me_flags) ? (pg) : NULL))
#else
#	define MC_OVPG(mc)			((MDB_page *)0)
#	define MC_SET_OVPG(mc, pg)	((void)0)
#endif
};

	/** Context for sorted-dup records.
	 *	We could have gone to a fully recursive design, with arbitrarily
	 *	deep nesting of sub-databases. But for now we only handle these
	 *	levels - main DB, optional sub-DB, sorted-duplicate DB.
	 */
typedef struct MDB_xcursor {
	/** A sub-cursor for traversing the Dup DB */
	MDB_cursor mx_cursor;
	/** The database record for this Dup DB */
	MDB_db	mx_db;
	/**	The auxiliary DB record for this Dup DB */
	MDB_dbx	mx_dbx;
	/** The @ref mt_dbflag for this Dup DB */
	unsigned char mx_dbflag;
} MDB_xcursor;

	/** Check if there is an inited xcursor */
#define XCURSOR_INITED(mc) \
	((mc)->mc_xcursor && ((mc)->mc_xcursor->mx_cursor.mc_flags & C_INITIALIZED))

	/** Update the xcursor's sub-page pointer, if any, in \b mc.	Needed
	 *	when the node which contains the sub-page may have moved.	Called
	 *	with leaf page \b mp = mc->mc_pg[\b top].
	 */
#define XCURSOR_REFRESH(mc, top, mp) do { \
	MDB_page *xr_pg = (mp); \
	MDB_node *xr_node; \
	if (!XCURSOR_INITED(mc) || (mc)->mc_ki[top] >= NUMKEYS(xr_pg)) break; \
	xr_node = NODEPTR(xr_pg, (mc)->mc_ki[top]); \
	if ((xr_node->mn_flags & (F_DUPDATA|F_SUBDATA)) == F_DUPDATA) \
		(mc)->mc_xcursor->mx_cursor.mc_pg[0] = NODEDATA(xr_node); \
} while (0)

	/** State of FreeDB old pages, stored in the MDB_env */
typedef struct MDB_pgstate {
	pgno_t		*mf_pghead;	/**< Reclaimed freeDB pages, or NULL before use */
	pgno_t		mf_freelist_start;	/**< Page number of beginning of active freelist (in-memory) */
	pgno_t		mf_freelist_end;	/**< Page number of end of active freelist (in-memory) */
	/** Position in the free page list, so that we can keep trying to write to the same block
	 * if possible. We use sign here, a negative number means we are in middle of contiguous
	 * (in a transaction) and we really want to use the next page, a positive number is
	 * just the next page number */
	int 		mf_position;
	int 		mf_written_start; /* start of modified entries in free-list */
	int 		mf_written_end; /* start of modified entries in free-list */
} MDB_pgstate;
/*<lmdb-js>*/
struct MDB_last_map {
	struct MDB_last_map	*last_map;
	char 			*map;
	mdb_size_t		mapsize;
};
typedef struct MDB_last_map MDB_last_map;
/*</lmdb-js>*/
	/** The database environment. */
struct MDB_env {
	HANDLE		me_fd;		/**< The main data file */
	HANDLE		me_lfd;		/**< The lock file */
	HANDLE		me_mfd;		/**< For writing and syncing the meta pages */
#ifdef _WIN32
#ifdef MDB_RPAGE_CACHE
	HANDLE		me_fmh;		/**< File Mapping handle */
#endif
	HANDLE		me_ovfd;	/**< Overlapped/async with write-through file handle */
#endif /* _WIN32 */
	/** Failed to update the meta page. Probably an I/O error. */
#define	MDB_FATAL_ERROR	0x80000000U
	/** using a raw block device */
#define	MDB_RAWPART		0x40000000U
	/** Some fields are initialized. */
#define	MDB_ENV_ACTIVE	0x20000000U
	/** me_txkey is set */
#define	MDB_ENV_TXKEY	0x10000000U
	/** fdatasync is unreliable */
#define	MDB_FSYNCONLY	0x08000000U
	uint32_t 	me_flags;		/**< @ref mdb_env */
	unsigned int	me_psize;	/**< DB page size, inited from me_os_psize */
	unsigned int	me_os_psize;	/**< OS page size, from #GET_PAGESIZE */
	unsigned int	me_maxreaders;	/**< size of the reader table */
	/** Max #MDB_txninfo.%mti_numreaders of interest to #mdb_env_close() */
	volatile int	me_close_readers;
	MDB_dbi		me_numdbs;		/**< number of DBs opened */
	MDB_dbi		me_maxdbs;		/**< size of the DB table */
	MDB_PID_T	me_pid;		/**< process ID of this env */
	char		*me_path;		/**< path to the DB files */
	char		*me_map;		/**< the memory map of the data file */
/*<lmdb-js>*/
	MDB_last_map	*me_last_map;	/**< the previous memory map of the data file after a resize */
/*</lmdb-js>*/
	MDB_txninfo	*me_txns;		/**< the memory map of the lock file or NULL */
	MDB_meta	*me_metas[NUM_METAS];	/**< pointers to the two meta pages */
	void		*me_pbuf;		/**< scratch area for DUPSORT put() */
	MDB_txn		*me_txn;		/**< current write transaction */
	MDB_txn		*me_txn0;		/**< prealloc'd write transaction */
	mdb_size_t	me_mapsize;		/**< size of the data memory map */
	MDB_OFF_T	me_size;		/**< current file size */
	pgno_t		me_maxpg;		/**< me_mapsize / me_psize */
	MDB_dbx		*me_dbxs;		/**< array of static DB info */
	uint16_t	*me_dbflags;	/**< array of flags from MDB_db.md_flags */
	unsigned int	*me_dbiseqs;	/**< array of dbi sequence numbers */
	pthread_key_t	me_txkey;	/**< thread-key for readers */
	txnid_t		me_pgoldest;	/**< ID of oldest reader last time we looked */
	MDB_pgstate	me_pgstate;		/**< state of old pages from freeDB */
#	define		me_freelist_start	me_pgstate.mf_freelist_start
#	define		me_freelist_end	me_pgstate.mf_freelist_end
#	define		me_pghead	me_pgstate.mf_pghead
#	define		me_freelist_position	me_pgstate.mf_position
#	define		me_freelist_written_start	me_pgstate.mf_written_start
#	define		me_freelist_written_end	me_pgstate.mf_written_end
	unsigned int me_maxfreepgs_to_load; /**< max freelist entries to load into memory */
	unsigned int me_maxfreepgs_to_retain; /**< max freelist entries to load into memory */
	MDB_page	*me_dpages;		/**< list of malloc'd blocks for re-use */
	/** IDL of pages that became unused in a write txn */
	MDB_IDL		me_free_pgs;
	/** ID2L of pages written during a write txn. Length MDB_IDL_UM_SIZE.
	 *	Unused except for a dummy element when #MDB_WRITEMAP.
	 */
	MDB_ID2L	me_dirty_list;
	int			*me_dirty_nump;
	/** Max number of freelist items that can fit in a single overflow page */
	int			me_maxfree_1pg;
	/** Max size of a node on a page */
	unsigned int	me_nodemax;
#if !(MDB_MAXKEYSIZE)
	unsigned int	me_maxkey;	/**< max size of a key */
#endif
	int		me_live_reader;		/**< have liveness lock in reader table */
#ifdef _WIN32
	int		me_pidquery;		/**< Used in OpenProcess */
	OVERLAPPED		*me_ov;			/**< Used for overlapping I/O requests */
	int		me_ovs;				/**< Count of MDB_overlaps */
#endif
#ifdef MDB_USE_POSIX_MUTEX	/* Posix mutexes reside in shared mem */
#	define		me_rmutex	me_txns->mti_rmutex /**< Shared reader lock */
#	define		me_wmutex	me_txns->mti_wmutex /**< Shared writer lock */
#	define		me_sync_mutex	me_txns->mti_sync_mutex /**< Shared sync lock */
#else
	mdb_mutex_t	me_rmutex;
	mdb_mutex_t	me_wmutex;
	mdb_mutex_t	me_sync_mutex;
# if defined(_WIN32) || defined(MDB_USE_POSIX_SEM)
	/** Half-initialized name of mutexes, to be completed by #MUTEXNAME() */
	char		me_mutexname[sizeof(MUTEXNAME_PREFIX) + 11];
# endif
#endif
	mdb_size_t me_synced_txn_id;
#if MDB_RPAGE_CACHE
	MDB_ID3L	me_rpages;	/**< like #mt_rpages, but global to env */
	pthread_mutex_t	me_rpmutex;	/**< control access to #me_rpages */
	MDB_sum_func *me_sumfunc;	/**< checksum env data */
	unsigned short me_sumsize;	/**< size of per-page checksums */
#define MDB_ERPAGE_SIZE	16384
#define MDB_ERPAGE_MAX	(MDB_ERPAGE_SIZE-1)
	unsigned short me_esumsize;	/**< size of per-page authentication data */
	unsigned int me_rpcheck;

	MDB_enc_func *me_encfunc;	/**< encrypt env data */
	MDB_val		me_enckey;	/**< key for env encryption */
#endif
	void		*me_userctx;	 /**< User-settable context */
	MDB_metrics		me_metrics;	 /**< Metrics tracking */
	MDB_assert_func *me_assert_func; /**< Callback for assertion failures */
	void *me_callback; /**< General callback */
	int64_t 	boot_id;
};

	/** Nested transaction */
typedef struct MDB_ntxn {
	MDB_txn		mnt_txn;		/**< the transaction */
	MDB_pgstate	mnt_pgstate;	/**< parent transaction's saved freestate */
} MDB_ntxn;

	/** max number of pages to commit in one writev() call */
#define MDB_COMMIT_PAGES	 64
#if defined(IOV_MAX) && IOV_MAX < MDB_COMMIT_PAGES
#undef MDB_COMMIT_PAGES
#define MDB_COMMIT_PAGES	IOV_MAX
#endif

	/** max bytes to write in one call */
#define MAX_WRITE		(0x40000000U >> (sizeof(ssize_t) == 4))

	/** Check \b txn and \b dbi arguments to a function */
#define TXN_DBI_EXIST(txn, dbi, validity) \
	((txn) && (dbi)<(txn)->mt_numdbs && ((txn)->mt_dbflags[dbi] & (validity)))

	/** Check for misused \b dbi handles */
#define TXN_DBI_CHANGED(txn, dbi) \
	((txn)->mt_dbiseqs[dbi] != (txn)->mt_env->me_dbiseqs[dbi])

#define MDB_FREELIST_DELETING	1
static int	mdb_page_alloc(MDB_cursor *mc, int num, MDB_page **mp);
static int	mdb_page_new(MDB_cursor *mc, uint32_t flags, int num, MDB_page **mp);
static int	mdb_page_touch(MDB_cursor *mc);

#define MDB_END_NAMES {"committed", "empty-commit", "abort", "reset", \
	"reset-tmp", "fail-begin", "fail-beginchild"}
enum {
	/* mdb_txn_end operation number, for logging */
	MDB_END_COMMITTED, MDB_END_EMPTY_COMMIT, MDB_END_ABORT, MDB_END_RESET,
	MDB_END_RESET_TMP, MDB_END_FAIL_BEGIN, MDB_END_FAIL_BEGINCHILD
};
#define MDB_END_OPMASK	0x0F	/**< mask for #mdb_txn_end() operation number */
#define MDB_END_UPDATE	0x10	/**< update env state (DBIs) */
#define MDB_END_FREE	0x20	/**< free txn unless it is #MDB_env.%me_txn0 */
#define MDB_END_SLOT MDB_NOTLS	/**< release any reader slot if #MDB_NOTLS */
static void mdb_txn_end(MDB_txn *txn, unsigned mode);

#if MDB_RPAGE_CACHE
#define MDB_PAGE_GET(mc, pg, numpgs, mp)	mdb_page_get(mc, pg, numpgs, mp)
static void mdb_rpage_dispose(MDB_env *env, MDB_ID3 *id3);
#else
/* Drop unused numpgs argument when !MDB_RPAGE_CACHE */
#define MDB_PAGE_GET(mc, pg, numpgs, mp)	mdb_page_get(mc, pg, mp)
#endif
static int	MDB_PAGE_GET(MDB_cursor *mc, pgno_t pgno, int numpgs, MDB_page **mp);

static int	mdb_page_search_root(MDB_cursor *mc,
					MDB_val *key, int modify);
#define MDB_PS_MODIFY	1
#define MDB_PS_ROOTONLY	2
#define MDB_PS_FIRST	4
#define MDB_PS_LAST		8
static int	mdb_page_search(MDB_cursor *mc,
					MDB_val *key, int flags);
static int	mdb_page_merge(MDB_cursor *csrc, MDB_cursor *cdst);

#define MDB_SPLIT_REPLACE	MDB_APPENDDUP	/**< newkey is not new */
static int	mdb_page_split(MDB_cursor *mc, MDB_val *newkey, MDB_val *newdata,
				pgno_t newpgno, unsigned int nflags);

static int	mdb_env_read_header(MDB_env *env, int prev, MDB_meta *meta);
static MDB_meta *mdb_env_pick_meta(const MDB_env *env);
static int	mdb_env_write_meta(MDB_txn *txn);
#if defined(MDB_USE_POSIX_MUTEX) && !defined(MDB_ROBUST_SUPPORTED) /* Drop unused excl arg */
# define mdb_env_close_active(env, excl) mdb_env_close1(env)
#endif
static void mdb_env_close_active(MDB_env *env, int excl);

static MDB_node *mdb_node_search(MDB_cursor *mc, MDB_val *key, int *exactp);
static int	mdb_node_add(MDB_cursor *mc, indx_t indx,
					MDB_val *key, MDB_val *data, pgno_t pgno, unsigned int flags);
static void mdb_node_del(MDB_cursor *mc, int ksize);
static void mdb_node_shrink(MDB_page *mp, indx_t indx);
static int	mdb_node_move(MDB_cursor *csrc, MDB_cursor *cdst, int fromleft);
static int	mdb_node_read(MDB_cursor *mc, MDB_node *leaf, MDB_val *data);
static size_t	mdb_leaf_size(MDB_env *env, MDB_val *key, MDB_val *data);
static size_t	mdb_branch_size(MDB_env *env, MDB_val *key);

static int	mdb_rebalance(MDB_cursor *mc);
static int	mdb_update_key(MDB_cursor *mc, MDB_val *key);

static void	mdb_cursor_pop(MDB_cursor *mc);
static int	mdb_cursor_push(MDB_cursor *mc, MDB_page *mp);

static int	mdb_cursor_del0(MDB_cursor *mc);
static int	mdb_del0(MDB_txn *txn, MDB_dbi dbi, MDB_val *key, MDB_val *data, unsigned flags);
static int	mdb_cursor_sibling(MDB_cursor *mc, int move_right);
static int	mdb_cursor_next(MDB_cursor *mc, MDB_val *key, MDB_val *data, MDB_cursor_op op);
static int	mdb_cursor_prev(MDB_cursor *mc, MDB_val *key, MDB_val *data, MDB_cursor_op op);
static int	mdb_cursor_set(MDB_cursor *mc, MDB_val *key, MDB_val *data, MDB_cursor_op op,
				int *exactp);
static int	mdb_cursor_first(MDB_cursor *mc, MDB_val *key, MDB_val *data);
static int	mdb_cursor_last(MDB_cursor *mc, MDB_val *key, MDB_val *data);

static void	mdb_cursor_init(MDB_cursor *mc, MDB_txn *txn, MDB_dbi dbi, MDB_xcursor *mx);
static void	mdb_xcursor_init0(MDB_cursor *mc);
static void	mdb_xcursor_init1(MDB_cursor *mc, MDB_node *node);
static void	mdb_xcursor_init2(MDB_cursor *mc, MDB_xcursor *src_mx, int force);

static int	mdb_drop0(MDB_cursor *mc, int subs);
static void mdb_default_cmp(MDB_txn *txn, MDB_dbi dbi);
static int mdb_reader_check0(MDB_env *env, int rlocked, int *dead);

/** @cond */
static MDB_cmp_func	mdb_cmp_memn, mdb_cmp_memnr, mdb_cmp_int, mdb_cmp_cint, mdb_cmp_long;
/** @endcond */

/** Compare two items pointing at '#mdb_size_t's of unknown alignment. */
#ifdef MISALIGNED_OK
# define mdb_cmp_clong mdb_cmp_long
#else
# define mdb_cmp_clong mdb_cmp_cint
#endif

/** True if we need #mdb_cmp_clong() instead of \b cmp for #MDB_INTEGERDUP */
#define NEED_CMP_CLONG(cmp, ksize) \
	(UINT_MAX < MDB_SIZE_MAX && \
	 (cmp) == mdb_cmp_int && (ksize) == sizeof(mdb_size_t))

#ifdef _WIN32
static SECURITY_DESCRIPTOR mdb_null_sd;
static SECURITY_ATTRIBUTES mdb_all_sa;
static int mdb_sec_inited;

struct MDB_name;
static int utf8_to_utf16(const char *src, struct MDB_name *dst, int xtra);
#endif

/** Return the library version info. */
char * ESECT
mdb_version(int *major, int *minor, int *patch)
{
	if (major) *major = MDB_VERSION_MAJOR;
	if (minor) *minor = MDB_VERSION_MINOR;
	if (patch) *patch = MDB_VERSION_PATCH;
	return MDB_VERSION_STRING;
}

/** Table of descriptions for LMDB @ref errors */
static char *const mdb_errstr[] = {
	"MDB_KEYEXIST: Key/data pair already exists",
	"MDB_NOTFOUND: No matching key/data pair found",
	"MDB_PAGE_NOTFOUND: Requested page not found",
	"MDB_CORRUPTED: Located page was wrong type",
	"MDB_PANIC: Update of meta page failed or environment had fatal error",
	"MDB_VERSION_MISMATCH: Database environment version mismatch",
	"MDB_INVALID: File is not an LMDB file",
	"MDB_MAP_FULL: Environment mapsize limit reached",
	"MDB_DBS_FULL: Environment maxdbs limit reached",
	"MDB_READERS_FULL: Environment maxreaders limit reached",
	"MDB_TLS_FULL: Thread-local storage keys full - too many environments open",
	"MDB_TXN_FULL: Transaction has too many dirty pages - transaction too big",
	"MDB_CURSOR_FULL: Internal error - cursor stack limit reached",
	"MDB_PAGE_FULL: Internal error - page has no more space",
	"MDB_MAP_RESIZED: Database contents grew beyond environment mapsize",
	"MDB_INCOMPATIBLE: Operation and DB incompatible, or DB flags changed",
	"MDB_BAD_RSLOT: Invalid reuse of reader locktable slot",
	"MDB_BAD_TXN: Transaction must abort, has a child, or is invalid",
	"MDB_BAD_VALSIZE: Unsupported size of key/DB name/data, or wrong DUPFIXED size",
	"MDB_BAD_DBI: The specified DBI handle was closed/changed unexpectedly",
	"MDB_PROBLEM: Unexpected problem - txn should abort",
	"MDB_BAD_CHECKSUM: Page checksum mismatch",
	"MDB_CRYPTO_FAIL: Page encryption or decryption failed",
	"MDB_ENV_ENCRYPTION: Environment encryption mismatch",
	"MDB_LOCK_FAILURE: Failed to get robust semaphore, increase max semaphore count by running 'sudo sysctl kern.sysv.semume=200' or build with robust mutexes disabled (see docs)",
	"MDB_EMPTY_TXN: Transaction was empty",
};
//<lmdb-js>
static char* last_error = NULL;
//</lmdb-js>

char *
mdb_strerror(int err)
{
#ifdef _WIN32
	/** HACK: pad 4KB on stack over the buf. Return system msgs in buf.
	 *	This works as long as no function between the call to mdb_strerror
	 *	and the actual use of the message uses more than 4K of stack.
	 */
#define MSGSIZE	1024
#define PADSIZE	4096
	char buf[MSGSIZE+PADSIZE], *ptr = buf;
#endif
	int i;
	if (!err)
		return ("Successful return: 0");

	if (err >= MDB_KEYEXIST && err <= MDB_LAST_ERRCODE) {
		i = err - MDB_KEYEXIST;
		if (last_error) {
			char* error = malloc(300);
			strcpy(error, mdb_errstr[i]);
			strcat(error, ": ");
			strcat(error, last_error);
			last_error = NULL;
			return error;
		}
		return mdb_errstr[i];
	}

#ifdef _WIN32
	/* These are the C-runtime error codes we use. The comment indicates
	 * their numeric value, and the Win32 error they would correspond to
	 * if the error actually came from a Win32 API. A major mess, we should
	 * have used LMDB-specific error codes for everything.
	 */
	switch(err) {
	case ENOENT:	/* 2, FILE_NOT_FOUND */
	case EIO:		/* 5, ACCESS_DENIED */
	case ENOMEM:	/* 12, INVALID_ACCESS */
	case EACCES:	/* 13, INVALID_DATA */
	case EBUSY:		/* 16, CURRENT_DIRECTORY */
	case EINVAL:	/* 22, BAD_COMMAND */
	case ENOSPC:	/* 28, OUT_OF_PAPER */
		if (last_error) {
			char* error = malloc(300);
			strcpy(error, strerror(err));
			strcat(error, ": ");
			strcat(error, last_error);
			last_error = NULL;
			return error;
		}
		return strerror(err);
	default:
		;
	}
	buf[0] = 0;
	FormatMessageA(FORMAT_MESSAGE_FROM_SYSTEM |
		FORMAT_MESSAGE_IGNORE_INSERTS,
		NULL, err, 0, ptr, MSGSIZE, (va_list *)NULL);
	return ptr;
#else
	if (last_error) {
		char* error = malloc(300);
		strcpy(error, strerror(err));
		strcat(error, ": ");
		strcat(error, last_error);
		last_error = NULL;
		return error;
	}
	return strerror(err);
#endif
}

/** assert(3) variant in cursor context */
#define mdb_cassert(mc, expr)	mdb_assert0((mc)->mc_txn->mt_env, expr, #expr)
/** assert(3) variant in transaction context */
#define mdb_tassert(txn, expr)	mdb_assert0((txn)->mt_env, expr, #expr)
/** assert(3) variant in environment context */
#define mdb_eassert(env, expr)	mdb_assert0(env, expr, #expr)

#ifndef NDEBUG
# define mdb_assert0(env, expr, expr_txt) ((expr) ? (void)0 : \
		mdb_assert_fail(env, expr_txt, mdb_func_, __FILE__, __LINE__))

static void ESECT
mdb_assert_fail(MDB_env *env, const char *expr_txt,
	const char *func, const char *file, int line)
{
	char buf[400];
	sprintf(buf, "%.100s:%d: Assertion '%.200s' failed in %.40s()",
		file, line, expr_txt, func);
	if (env->me_assert_func)
		env->me_assert_func(env, buf);
	fprintf(stderr, "%s\n", buf);
	abort();
}
#else
# define mdb_assert0(env, expr, expr_txt) ((void) 0)
#endif /* NDEBUG */

#if MDB_DEBUG
/** Return the page number of \b mp which may be sub-page, for debug output */
static pgno_t
mdb_dbg_pgno(MDB_page *mp)
{
	pgno_t ret;
	COPY_PGNO(ret, mp->mp_pgno);
	return ret;
}

/** Display a key in hexadecimal and return the address of the result.
 * @param[in] key the key to display
 * @param[in] buf the buffer to write into. Should always be #DKBUF.
 * @return The key in hexadecimal form.
 */
char *
mdb_dkey(MDB_val *key, char *buf)
{
	char *ptr = buf;
	unsigned char *c = key->mv_data;
	unsigned int i;

	if (!key)
		return "";

	if (key->mv_size > DKBUF_MAXKEYSIZE)
		return "MDB_MAXKEYSIZE";
	/* may want to make this a dynamic check: if the key is mostly
	 * printable characters, print it as-is instead of converting to hex.
	 */
#if 1
	buf[0] = '\0';
	for (i=0; i<key->mv_size; i++)
		ptr += sprintf(ptr, "%02x", *c++);
#else
	sprintf(buf, "%.*s", key->mv_size, key->mv_data);
#endif
	return buf;
}

static const char *
mdb_leafnode_type(MDB_node *n)
{
	static char *const tp[2][2] = {{"", ": DB"}, {": sub-page", ": sub-DB"}};
	return F_ISSET(n->mn_flags, F_BIGDATA) ? ": overflow page" :
		tp[F_ISSET(n->mn_flags, F_DUPDATA)][F_ISSET(n->mn_flags, F_SUBDATA)];
}

/** Display all the keys in the page. */
void
mdb_page_list(MDB_page *mp)
{
	pgno_t pgno = mdb_dbg_pgno(mp);
	const char *type;
	MDB_node *node;
	unsigned int i, nkeys, nsize, total = 0;
	MDB_val key;
	DKBUF;

	switch (mp->mp_flags & (P_BRANCH|P_LEAF|P_LEAF2|P_META|P_OVERFLOW|P_SUBP)) {
	case P_BRANCH:							type = "Branch page";		break;
	case P_LEAF:								type = "Leaf page";			break;
	case P_LEAF|P_SUBP:				 type = "Sub-page";			break;
	case P_LEAF|P_LEAF2:				type = "LEAF2 page";		break;
	case P_LEAF|P_LEAF2|P_SUBP: type = "LEAF2 sub-page";	break;
	case P_OVERFLOW:
		fprintf(stderr, "Overflow page %"Yu" pages %u\n", pgno, mp->mp_pages);
		return;
	case P_META:
		fprintf(stderr, "Meta-page %"Yu" txnid %"Yu"\n",
			pgno, ((MDB_meta *)METADATA(mp))->mm_txnid);
		return;
	default:
		fprintf(stderr, "Bad page %"Yu" flags 0x%X\n", pgno, mp->mp_flags);
		return;
	}

	nkeys = NUMKEYS(mp);
	fprintf(stderr, "%s %"Yu" numkeys %d\n", type, pgno, nkeys);

	for (i=0; i<nkeys; i++) {
		if (IS_LEAF2(mp)) {	/* LEAF2 pages have no mp_ptrs[] or node headers */
			key.mv_size = nsize = mp->mp_pad;
			key.mv_data = LEAF2KEY(mp, i, nsize);
			total += nsize;
			fprintf(stderr, "key %d: nsize %d, %s\n", i, nsize, DKEY(&key));
			continue;
		}
		node = NODEPTR(mp, i);
		key.mv_size = node->mn_ksize;
		key.mv_data = node->mn_data;
		nsize = NODESIZE + key.mv_size;
		if (IS_BRANCH(mp)) {
			fprintf(stderr, "key %d: page %"Yu", %s\n", i, NODEPGNO(node),
				DKEY(&key));
			total += nsize;
		} else {
			if (F_ISSET(node->mn_flags, F_BIGDATA))
				nsize += sizeof(MDB_ovpage);
			else
				nsize += NODEDSZ(node);
			total += nsize;
			nsize += sizeof(indx_t);
			fprintf(stderr, "key %d: nsize %d, %s%s\n",
				i, nsize, DKEY(&key), mdb_leafnode_type(node));
		}
		total = EVEN(total);
	}
	fprintf(stderr, "Total: header %d + contents %d + unused %d\n",
		IS_LEAF2(mp) ? PAGEHDRSZ : PAGEBASE + mp->mp_lower, total, SIZELEFT(mp));
}

void
mdb_cursor_chk(MDB_cursor *mc)
{
	unsigned int i;
	MDB_node *node;
	MDB_page *mp;

	if (!mc->mc_snum || !(mc->mc_flags & C_INITIALIZED)) return;
	for (i=0; i<mc->mc_top; i++) {
		mp = mc->mc_pg[i];
		node = NODEPTR(mp, mc->mc_ki[i]);
		if (NODEPGNO(node) != mc->mc_pg[i+1]->mp_pgno)
			printf("oops!\n");
	}
	if (mc->mc_ki[i] >= NUMKEYS(mc->mc_pg[i]))
		printf("ack!\n");
	if (XCURSOR_INITED(mc)) {
		node = NODEPTR(mc->mc_pg[mc->mc_top], mc->mc_ki[mc->mc_top]);
		if (((node->mn_flags & (F_DUPDATA|F_SUBDATA)) == F_DUPDATA) &&
			mc->mc_xcursor->mx_cursor.mc_pg[0] != NODEDATA(node)) {
			printf("blah!\n");
		}
	}
}
#endif

#if (MDB_DEBUG) > 2
/** Count all the pages in each DB and in the freelist
 *	and make sure it matches the actual number of pages
 *	being used.
 *	All named DBs must be open for a correct count.
 */
static void mdb_audit(MDB_txn *txn)
{
	MDB_cursor mc;
	MDB_val key, data;
	MDB_ID freecount, count;
	MDB_dbi i;
	int rc;

	freecount = 0;
	mdb_cursor_init(&mc, txn, FREE_DBI, NULL);
	while ((rc = mdb_cursor_get(&mc, &key, &data, MDB_NEXT)) == 0)
		freecount += *(MDB_ID *)data.mv_data;
	mdb_tassert(txn, rc == MDB_NOTFOUND);

	count = 0;
	for (i = 0; i<txn->mt_numdbs; i++) {
		MDB_xcursor mx;
		if (!(txn->mt_dbflags[i] & DB_VALID))
			continue;
		mdb_cursor_init(&mc, txn, i, &mx);
		if (txn->mt_dbs[i].md_root == P_INVALID)
			continue;
		count += txn->mt_dbs[i].md_branch_pages +
			txn->mt_dbs[i].md_leaf_pages +
			txn->mt_dbs[i].md_overflow_pages;
		if (txn->mt_dbs[i].md_flags & MDB_DUPSORT) {
			rc = mdb_page_search(&mc, NULL, MDB_PS_FIRST);
			for (; rc == MDB_SUCCESS; rc = mdb_cursor_sibling(&mc, 1)) {
				unsigned j;
				MDB_page *mp;
				mp = mc.mc_pg[mc.mc_top];
				for (j=0; j<NUMKEYS(mp); j++) {
					MDB_node *leaf = NODEPTR(mp, j);
					if (leaf->mn_flags & F_SUBDATA) {
						MDB_db db;
						memcpy(&db, NODEDATA(leaf), sizeof(db));
						count += db.md_branch_pages + db.md_leaf_pages +
							db.md_overflow_pages;
					}
				}
			}
			mdb_tassert(txn, rc == MDB_NOTFOUND);
		}
	}
	if (freecount + count + NUM_METAS != txn->mt_next_pgno) {
		fprintf(stderr, "audit: %"Yu" freecount: %"Yu" count: %"Yu" total: %"Yu" next_pgno: %"Yu"\n",
			txn->mt_txnid, freecount, count+NUM_METAS,
			freecount+count+NUM_METAS, txn->mt_next_pgno);
	}
}
#endif

int
mdb_cmp(MDB_txn *txn, MDB_dbi dbi, const MDB_val *a, const MDB_val *b)
{
	return txn->mt_dbxs[dbi].md_cmp(a, b);
}

int
mdb_dcmp(MDB_txn *txn, MDB_dbi dbi, const MDB_val *a, const MDB_val *b)
{
	MDB_cmp_func *dcmp = txn->mt_dbxs[dbi].md_dcmp;
	if (NEED_CMP_CLONG(dcmp, a->mv_size))
		dcmp = mdb_cmp_clong;
	return dcmp(a, b);
}

/** Allocate memory for a page.
 * Re-use old malloc'd pages first for singletons, otherwise just malloc.
 * Set #MDB_TXN_ERROR on failure.
 */
static MDB_page *
mdb_page_malloc(MDB_txn *txn, unsigned num, int init)
{
	MDB_env *env = txn->mt_env;
	MDB_page *ret = env->me_dpages;
	size_t psize = env->me_psize, sz = psize, off;
	/* For ! #MDB_NOMEMINIT, psize counts how much to init.
	 * For a single page alloc, we init everything after the page header.
	 * For multi-page, we init the final page; if the caller needed that
	 * many pages they will be filling in at least up to the last page.
	 */
	if (num == 1) {
		psize -= off = PAGEHDRSZ;
		if (ret) {
			VGMEMP_ALLOC(env, ret, sz);
			VGMEMP_DEFINED(ret, sizeof(ret->mp_next));
			env->me_dpages = ret->mp_next;
			goto init;
		}
	} else {
		sz *= num;
		off = sz - psize;
	}
	if ((ret = malloc(sz)) != NULL) {
		VGMEMP_ALLOC(env, ret, sz);
init:
		if (init && !(env->me_flags & MDB_NOMEMINIT)) {
			memset((char *)ret + off, 0, psize);
			ret->mp_pad = 0;
		}
	} else {
		fprintf(stderr, "malloc error\n");
		txn->mt_flags |= MDB_TXN_ERROR;
	}
	ret->mp_flags = 0;
	return ret;
}
/** Free a single page.
 * Saves single pages to a list, for future reuse.
 * (This is not used for multi-page overflow pages.)
 */
static void
mdb_page_free(MDB_env *env, MDB_page *mp)
{
	mp->mp_next = env->me_dpages;
	VGMEMP_FREE(env, mp);
	env->me_dpages = mp;
}

/** Free a dirty page */
static void
mdb_dpage_free(MDB_env *env, MDB_page *dp)
{
	if (!IS_OVERFLOW(dp) || dp->mp_pages == 1) {
		mdb_page_free(env, dp);
	} else {
		/* large pages just get freed directly */
		VGMEMP_FREE(env, dp);
		free(dp);
	}
}

#if MDB_RPAGE_CACHE
/** Free an encrypted dirty page
 * We can't check if it's an overflow page,
 * caller must tell us how many are being freed.
 */
static void
mdb_dpage_free_n(MDB_env *env, MDB_page *dp, int num)
{
	if (num == 1)
		mdb_page_free(env, dp);
	else {
		/* large pages just get freed directly */
		VGMEMP_FREE(env, dp);
		free(dp);
	}
}
#endif

/**	Return all dirty pages to dpage list */
static void
mdb_dlist_free(MDB_txn *txn)
{
	MDB_env *env = txn->mt_env;
	MDB_ID2L dl = txn->mt_u.dirty_list;
	unsigned i, n = dl[0].mid;

	if (txn->mt_flags & MDB_TXN_DIRTYNUM) {
		int *dl_nump = env->me_dirty_nump;
		for (i = 1; i <= n; i++) {
			mdb_dpage_free_n(env, dl[i].mptr, dl_nump[i]);
		}
		txn->mt_flags ^= MDB_TXN_DIRTYNUM;
	} else {
		for (i = 1; i <= n; i++) {
			mdb_dpage_free(env, dl[i].mptr);
		}
	}
	dl[0].mid = 0;
}

#if MDB_RPAGE_CACHE
#if defined(MDB_VL32) || ((MDB_RPAGE_CACHE) & 2) /* Always remap */
#define MDB_REMAPPING(flags)	1
#else
#define MDB_REMAPPING(flags)	((flags) & MDB_REMAP_CHUNKS)
#endif

static void
mdb_page_unref(MDB_txn *txn, MDB_page *mp)
{
	pgno_t pgno;
	MDB_ID3L tl = txn->mt_rpages;
	unsigned x, rem;
	if (IS_SUBP(mp) || IS_DIRTY_NW(txn, mp))
		return;
	rem = mp->mp_pgno & (MDB_RPAGE_CHUNK-1);
	pgno = mp->mp_pgno ^ rem;
	x = mdb_mid3l_search(tl, pgno);
	if (x != tl[0].mid && tl[x+1].mid == mp->mp_pgno)
		x++;
	if (tl[x].mref)
		tl[x].mref--;
}
#define MDB_PAGE_UNREF(txn, mp) \
	(MDB_REMAPPING(txn->mt_env->me_flags) ? mdb_page_unref(txn, mp) : (void)0)

static void
mdb_cursor_unref(MDB_cursor *mc)
{
	int i;
	if (mc->mc_txn->mt_rpages[0].mid) {
		if (!mc->mc_snum || !mc->mc_pg[0] || IS_SUBP(mc->mc_pg[0]))
			return;
		for (i=0; i<mc->mc_snum; i++)
			mdb_page_unref(mc->mc_txn, mc->mc_pg[i]);
		if (mc->mc_ovpg) {
			mdb_page_unref(mc->mc_txn, mc->mc_ovpg);
			mc->mc_ovpg = 0;
		}
	}
	mc->mc_snum = mc->mc_top = 0;
	mc->mc_pg[0] = NULL;
	mc->mc_flags &= ~C_INITIALIZED;
}
#define MDB_CURSOR_UNREF(mc, force) \
	((MDB_REMAPPING((mc)->mc_txn->mt_env->me_flags) && \
	 ((force) || ((mc)->mc_flags & C_INITIALIZED))) \
	 ? mdb_cursor_unref(mc) \
	 : (void)0)

/* Unref ovpage \b omp in \b mc and tracked cursors */
static void
mdb_ovpage_unref_all(MDB_cursor *mc, MDB_page *omp)
{
	MDB_txn *txn = mc->mc_txn;
	MDB_cursor *next = txn->mt_cursors[mc->mc_dbi];
	for (;; mc = next, next = mc->mc_next) {
		if (MC_OVPG(mc) == omp) {
			mdb_page_unref(mc->mc_txn, omp);
			MC_SET_OVPG(mc, NULL);
		}
		if (next == NULL)
			break;
	}
}

#else
#define MDB_REMAPPING(flags)	0
#define MDB_PAGE_UNREF(txn, mp)
#define MDB_CURSOR_UNREF(mc, force) ((void)0)
#define mdb_ovpage_unref_all(mc, omp, pgno) ((void)0)
#endif /* MDB_RPAGE_CACHE */

/** Loosen or free a single page.
 * Saves single pages to a list for future reuse
 * in this same txn. It has been pulled from the freeDB
 * and already resides on the dirty list, but has been
 * deleted. Use these pages first before pulling again
 * from the freeDB.
 *
 * If the page wasn't dirtied in this txn, just add it
 * to this txn's free list.
 */
static int
mdb_page_loose(MDB_cursor *mc, MDB_page *mp)
{
	int rc = MDB_SUCCESS;
	MDB_txn *txn = mc->mc_txn;

	if (IS_WRITABLE(txn, mp) && mc->mc_dbi != FREE_DBI) {
		/* Page is dirty in this txn, and is not in freeDB */
		DPRINTF(("loosen db %d page %"Yu, DDBI(mc), mp->mp_pgno));
		NEXT_LOOSE_PAGE(mp) = txn->mt_loose_pgs;
		txn->mt_loose_pgs = mp;
		txn->mt_loose_count++;
		mp->mp_flags |= P_LOOSE;
	} else {
		rc = mdb_midl_append(&txn->mt_free_pgs, mp->mp_pgno);
	}

	return rc;
}

/** Set or clear P_KEEP in dirty, non-overflow, non-sub pages watched by txn.
 * @param[in] mc A cursor handle for the current operation.
 * @param[in] pflags Flags of the pages to update:
 * 0 to set P_KEEP, P_KEEP to clear it.
 * @param[in] all No shortcuts. Needed except after a full #mdb_page_flush().
 * @return 0 on success, non-zero on failure.
 */
static int
mdb_pages_xkeep(MDB_cursor *mc, unsigned pflags, int all)
{
	enum { Mask = P_SUBP|P_LOOSE|P_KEEP };
	MDB_txn *txn = mc->mc_txn;
	MDB_cursor *m3, *m0 = mc;
	MDB_xcursor *mx;
	MDB_page *dp, *mp;
	MDB_node *leaf;
	unsigned i, j, x;
	int rc = MDB_SUCCESS;

	/* Mark pages seen by cursors: First m0, then tracked cursors */
	for (i = txn->mt_numdbs;; ) {
		if (mc->mc_flags & C_INITIALIZED) {
			for (m3 = mc;; m3 = &mx->mx_cursor) {
				mp = NULL;
				for (j=0; j<m3->mc_snum; j++) {
					mp = m3->mc_pg[j];
					if ((mp->mp_flags & Mask) == pflags)
						if (IS_WRITABLE(txn, mp))
						mp->mp_flags ^= P_KEEP;
				}
				if (MC_OVPG(m3) && ((MC_OVPG(m3)->mp_flags & Mask) == pflags) &&
					IS_WRITABLE(txn, MC_OVPG(m3)))
					MC_OVPG(m3)->mp_flags ^= P_KEEP;
				mx = m3->mc_xcursor;
				/* Proceed to mx if it is at a sub-database */
				if (! (mx && (mx->mx_cursor.mc_flags & C_INITIALIZED)))
					break;
				if (! (mp && (mp->mp_flags & P_LEAF)))
					break;
				leaf = NODEPTR(mp, m3->mc_ki[j-1]);
				if (!(leaf->mn_flags & F_SUBDATA))
					break;
			}
		}
		mc = mc->mc_next;
		for (; !mc || mc == m0; mc = txn->mt_cursors[--i])
			if (i == 0)
				goto mark_done;
	}

mark_done:
	if (all) {
		/* Mark dirty root pages */
		MDB_ID2L dl = txn->mt_u.dirty_list;
		for (i=0; i<txn->mt_numdbs; i++) {
			if (txn->mt_dbflags[i] & DB_DIRTY) {
				pgno_t pgno = txn->mt_dbs[i].md_root;
				if (pgno == P_INVALID)
					continue;
				x = mdb_mid2l_search(dl, pgno);
				if (! (x <= dl[0].mid && dl[x].mid == pgno))
					continue;
				dp = dl[x].mptr;
				if ((dp->mp_flags & Mask) == pflags)
					dp->mp_flags ^= P_KEEP;
			}
		}
	}

	return rc;
}

static int mdb_page_flush(MDB_txn *txn, int keep);

/**	Spill pages from the dirty list back to disk.
 * This is intended to prevent running into #MDB_TXN_FULL situations,
 * but note that they may still occur in a few cases:
 *	1) our estimate of the txn size could be too small. Currently this
 *	 seems unlikely, except with a large number of #MDB_MULTIPLE items.
 *	2) child txns may run out of space if their parents dirtied a
 *	 lot of pages and never spilled them. TODO: we probably should do
 *	 a preemptive spill during #mdb_txn_begin() of a child txn, if
 *	 the parent's dirty_room is below a given threshold.
 *
 * Otherwise, if not using nested txns, it is expected that apps will
 * not run into #MDB_TXN_FULL any more. The pages are flushed to disk
 * the same way as for a txn commit.
 * If the txn never references them again, they can be left alone.
 * If the txn only reads them, they can be used without any fuss.
 * If the txn writes them again, they can be dirtied immediately without
 * going thru all of the work of #mdb_page_touch(). Such references are
 * handled by #mdb_page_unspill().
 *
 * Also note, we never spill DB root pages, nor pages of active cursors,
 * because we'll need these back again soon anyway. And in nested txns,
 * we can't spill a page in a child txn if it was already spilled in a
 * parent txn. That would alter the parent txns' data even though
 * the child hasn't committed yet, and we'd have no way to undo it if
 * the child aborted.
 *
 * @param[in] m0 cursor A cursor handle identifying the transaction and
 *	database for which we are checking space.
 * @param[in] key For a put operation, the key being stored.
 * @param[in] data For a put operation, the data being stored.
 * @return 0 on success, non-zero on failure.
 */
static int
mdb_page_spill(MDB_cursor *m0, MDB_val *key, MDB_val *data)
{
	MDB_txn *txn = m0->mc_txn;
	MDB_page *dp;
	MDB_ID2L dl = txn->mt_u.dirty_list;
	unsigned int i, j, need;
	int rc;

	if (m0->mc_flags & (C_SUB|C_WRITEMAP))
		return MDB_SUCCESS;

	/* Estimate how much space this op will take */
	i = m0->mc_db->md_depth;
	/* Named DBs also dirty the main DB */
	if (m0->mc_dbi >= CORE_DBS)
		i += txn->mt_dbs[MAIN_DBI].md_depth;
	/* For puts, roughly factor in the key+data size */
	if (key)
		i += (LEAFSIZE(key, data) + txn->mt_env->me_psize) / txn->mt_env->me_psize;
	i += i;	/* double it for good measure */
	need = i;

	if (txn->mt_dirty_room > i)
		return MDB_SUCCESS;

	if (!txn->mt_spill_pgs) {
		txn->mt_spill_pgs = mdb_midl_alloc(MDB_IDL_UM_MAX);
		if (!txn->mt_spill_pgs)
			return ENOMEM;
	} else {
		/* purge deleted slots */
		MDB_IDL sl = txn->mt_spill_pgs;
		unsigned int num = sl[0];
		j=0;
		for (i=1; i<=num; i++) {
			if (!(sl[i] & 1))
				sl[++j] = sl[i];
		}
		sl[0] = j;
	}

	/* Preserve pages which may soon be dirtied again */
	if ((rc = mdb_pages_xkeep(m0, 0, 1)) != MDB_SUCCESS)
		goto done;

	/* Less aggressive spill - we originally spilled the entire dirty list,
	 * with a few exceptions for cursor pages and DB root pages. But this
	 * turns out to be a lot of wasted effort because in a large txn many
	 * of those pages will need to be used again. So now we spill only 1/8th
	 * of the dirty pages. Testing revealed this to be a good tradeoff,
	 * better than 1/2, 1/4, or 1/10.
	 */
	if (need < MDB_IDL_UM_MAX / 8)
		need = MDB_IDL_UM_MAX / 8;

	/* Save the page IDs of all the pages we're flushing */
	/* flush from the tail forward, this saves a lot of shifting later on. */
	for (i=dl[0].mid; i && need; i--) {
		MDB_ID pn = dl[i].mid << 1;
		dp = dl[i].mptr;
		if (dp->mp_flags & (P_LOOSE|P_KEEP))
			continue;
		/* Can't spill twice, make sure it's not already in a parent's
		 * spill list.
		 */
		if (txn->mt_parent) {
			MDB_txn *tx2;
			for (tx2 = txn->mt_parent; tx2; tx2 = tx2->mt_parent) {
				if (tx2->mt_spill_pgs) {
					j = mdb_midl_search(tx2->mt_spill_pgs, pn);
					if (j <= tx2->mt_spill_pgs[0] && tx2->mt_spill_pgs[j] == pn) {
						dp->mp_flags |= P_KEEP;
						break;
					}
				}
			}
			if (tx2)
				continue;
		}
		if ((rc = mdb_midl_append(&txn->mt_spill_pgs, pn)))
			goto done;
		need--;
	}
	mdb_midl_sort(txn->mt_spill_pgs);

	/* Flush the spilled part of dirty list */
	if ((rc = mdb_page_flush(txn, i)) != MDB_SUCCESS) {
		fprintf(stderr, "mdb_page_spill flushing error %i\n", rc);
		goto done;
	}

	/* Reset any dirty pages we kept that page_flush didn't see */
	rc = mdb_pages_xkeep(m0, P_KEEP, i);

done:
if (rc) {
	fprintf(stderr, "mdb_page_spill error %i\n", rc);
}
	txn->mt_flags |= rc ? MDB_TXN_ERROR : MDB_TXN_SPILLS;
	return rc;
}

/** Find oldest txnid still referenced. Expects txn->mt_txnid > 0. */
static txnid_t
mdb_find_oldest(MDB_txn *txn)
{
	int i;
	/* <lmdb-js> */
	txnid_t mr, oldest = (txn->mt_env->me_flags & MDB_OVERLAPPINGSYNC) ?
		txn->mt_env->me_synced_txn_id :
		(txn->mt_txnid - 1);
	/* </lmdb-js> */
	if (txn->mt_env->me_txns) {
		MDB_reader *r = txn->mt_env->me_txns->mti_readers;
		for (i = txn->mt_env->me_txns->mti_numreaders; --i >= 0; ) {
			if (r[i].mr_pid) {
				mr = r[i].mr_txnid;
				if (oldest > mr)
					oldest = mr;
			}
		}
	}
	return oldest;
}

/** Add a page to the txn's dirty list, if there is one */
static void
mdb_page_dirty(MDB_txn *txn, MDB_page *mp)
{
	MDB_ID2 mid;
	int rc;

	if (txn->mt_flags & MDB_TXN_WRITEMAP) {
		txn->mt_flags |= MDB_TXN_DIRTY;
		return;
	}
	mid.mid = mp->mp_pgno;
	mid.mptr = mp;
	rc = mdb_mid2l_insert(txn->mt_u.dirty_list, &mid);
	mdb_tassert(txn, rc == 0);
	txn->mt_dirty_room--;
}

const static int MAX_SCAN_SEGMENT = 50;

/** Allocate page numbers and memory for writing.	Maintain me_freelist_start,
 * me_pghead and mt_next_pgno.	Set #MDB_TXN_ERROR on failure.
 *
 * If there are free pages available from older transactions, they
 * are re-used first. Otherwise allocate a new page at mt_next_pgno.
 * Do not modify the freedB, just merge freeDB records into me_pghead[]
 * and move me_freelist_start to say which records were consumed.	Only this
 * function can create me_pghead and move me_freelist_start/mt_next_pgno.
 * When #MDB_DEVEL & 2, it is not affected by #mdb_freelist_save(): it
 * then uses the transaction's original snapshot of the freeDB.
 * @param[in] mc cursor A cursor handle identifying the transaction and
 *	database for which we are allocating.
 * @param[in] num the number of pages to allocate.
 * @param[out] mp Address of the allocated page(s). Requests for multiple pages
 *	will always be satisfied by a single contiguous chunk of memory.
 * @return 0 on success, non-zero on failure.
 */
static int
mdb_page_alloc(MDB_cursor *mc, int num, MDB_page **mp)
{
#ifdef MDB_PARANOID	/* Seems like we can ignore this now */
	/* Get at most <Max_retries> more freeDB records once me_pghead
	 * has enough pages.	If not enough, use new pages from the map.
	 * If <Paranoid> and mc is updating the freeDB, only get new
	 * records if me_pghead is empty. Then the freelist cannot play
	 * catch-up with itself by growing while trying to save it.
	 */
	enum { Paranoid = 1, Max_retries = 500 };
#else
	enum { Paranoid = 0, Max_retries = INT_MAX /*infinite*/ };
#endif
	int rc, retry = num * 60;
	MDB_txn *txn = mc->mc_txn;
	MDB_env *env = txn->mt_env;
	pgno_t pgno, *mop = env->me_pghead;
	unsigned i, j, mop_len = mop ? mop[0] : 0, n2 = num-1;
	MDB_page *np;
	txnid_t oldest = 0, last = 0;
	MDB_cursor_op op;
	MDB_cursor m2;
	int found_old = 0;

#if OVERFLOW_NOTYET
	MDB_dovpage *dph = NULL;

	if (ov) {
		if (!txn->mt_dirty_ovs) {
			txn->mt_dirty_ovs = mdb_mid2l_alloc(16);
			if (!txn->mt_dirty_ovs)
				return ENOMEM;
		} else if (mdb_mid2l_need(&txn->mt_dirty_ovs, txn->mt_dirty_ovs[0].mid + 1))
			return ENOMEM;
		dph = malloc(sizeof(MDB_dovpage));
	}
#endif

	/* If there are any loose pages, just use them */
	if (num == 1 && txn->mt_loose_pgs) {
		np = txn->mt_loose_pgs;
		txn->mt_loose_pgs = NEXT_LOOSE_PAGE(np);
		txn->mt_loose_count--;
		DPRINTF(("db %d use loose page %"Yu, DDBI(mc), np->mp_pgno));
#if OVERFLOW_NOTYET
		if (ov) {
			dph->mp_hdr = np->mp_hdr;
			dph->mp_ptr = np;
			np = (MDB_page *)dph;
		}
#endif
		*mp = np;
		np->mp_flags &= P_ADM_FLAGS;
		return MDB_SUCCESS;
	}

	*mp = NULL;

	/* If our dirty list is already full, we can't do anything */
	if (txn->mt_dirty_room == 0) {
		rc = MDB_TXN_FULL;
		goto fail;
	}
	unsigned empty_entries = 0;
	unsigned best_fit_start; // this is a block we will use if we don't find an exact fit
	pgno_t best_fit_size;
/*	if (c++ % 100000 == 0) {
		mdb_midl_print(stderr, mop);
	}*/
	for (op = MDB_SET_RANGE;;) {
		MDB_val key, data;
		MDB_node *leaf;
		pgno_t *idl;
restart_search:
		best_fit_start = 0;
		best_fit_size = -1;
		pgno_t block_start;
		/* Seek a big enough contiguous page range. Prefer blocks that fit the remaining space to reduce defragmentation
		 */
		block_start = 0;
		unsigned block_size = 0;
		ssize_t entry;
		empty_entries = 0;
		int start = env->me_freelist_position != 0 ? env->me_freelist_position : 1;
		if (start < 0) start = -start;
		if (start > mop_len) {
			start = 1;
		}
		if (mop_len && (ssize_t) mop[start - 1] < 0) start++; // don't start in the middle of a length block
		unsigned end = start + mop_len;
		unsigned check_point = start + MAX_SCAN_SEGMENT;
		if (check_point > mop_len) check_point = mop_len;
		unsigned wrapped = 0;
		// TODO: Don't scan if the list is too small
		//fprintf(stderr, "loop from %u to %u over %u\n", start, end, mop_len);
		for (i = start; 1; i++) {
			//fprintf(stderr, "l %u ", i);
			if (i > check_point) {
				unsigned counting_i = i + (wrapped ? mop_len : 0);
				if (best_fit_start > 0 && best_fit_size - num < (counting_i - start) >> 6) {
					goto continue_best_fit;
				}
				if (counting_i - start >= mop_len) {
					break; // searched everything in current list
				}
				if (i > mop_len) {
					// loop back and continue
					i = 1;
					wrapped = 1;
					check_point = start - 1;
					if (check_point > MAX_SCAN_SEGMENT) check_point = MAX_SCAN_SEGMENT;
				} else {
					check_point = i + MAX_SCAN_SEGMENT;
					if (wrapped) {
						if (check_point > start - 1) check_point = start - 1;
					} else if (check_point > mop_len) check_point = mop_len;
				}
			}

			entry = mop[i];
			//fprintf(stderr, "pgno %u next would be %u\n", entry, block_start + block_size);
			if (entry == 0) {
				empty_entries++;
				continue;
			}
			if (entry > 0) {
				pgno = entry;
				block_size = 1;
			} else {
				block_size = -entry;
				pgno = mop[++i];
			}

			if (block_size >= num) {
				if (block_size == num) {
					// we found a block of the right size
					env->me_freelist_position = i + 1;
					mop[i] = 0;
					if (env->me_freelist_written_end < i) env->me_freelist_written_end = i;
					if (entry < 1)
						mop[--i] = 0;
					if (env->me_freelist_written_start > i || !env->me_freelist_written_start)
						env->me_freelist_written_start = i;
					if (pgno == 0) {
						fprintf(stderr, "found freespace entry pointing to root block of size %u at %u\n", block_size, i);
						mdb_midl_print(stderr, mop);
					} else
						goto search_done;
				} else if (block_size < best_fit_size || best_fit_size == 0) {
					best_fit_start = i - 1;
					best_fit_size = block_size;
					if (i == 1 - env->me_freelist_position) {
						// if we just wrote to this block and we are continuing on this block,
						// skip ahead to using this block
						goto continue_best_fit;
					}
				}
			}
		}
		// see if there is too many empty entries; after a respread it should be between one third and one quarter full of empty entries
		if (empty_entries > ((i + (wrapped ? mop_len : 0) - start) / 3) + 10) {
			unsigned old_length = env->me_pghead[0];
			mdb_midl_respread(&env->me_pghead);
			mop = env->me_pghead;
			mop_len = mop[0];
			// consider the whole free-list to be updated now
			env->me_freelist_written_end = 0x7fffffff;
			env->me_freelist_written_start = 1;
			//fprintf(stderr, "resized from %u to %u\n", old_length, mop_len);
			goto restart_search;
		}
		env->me_freelist_position = i;
		i = 0;

		if (mop_len > env->me_maxfreepgs_to_load) {
			//fprintf(stderr, "Too many entries %u, looking for %u, best fit %u, not loading anymore\n", mop_len, num, best_fit_size);
			//mdb_midl_print(stderr, mop);
			goto continue_best_fit;
		}
		load_more:
		if (op == MDB_SET_RANGE) {	/* 1st iteration */
			/* Prepare to fetch more and coalesce */
			oldest = env->me_pgoldest;
			mdb_cursor_init(&m2, txn, FREE_DBI, NULL);
#if (MDB_DEVEL) & 2	/* "& 2" so MDB_DEVEL=1 won't hide bugs breaking freeDB */
			/* Use original snapshot. TODO: Should need less care in code
			 * which modifies the database. Maybe we can delete some code?
			 */
			m2.mc_flags |= C_ORIG_RDONLY;
			m2.mc_db = &env->me_metas[(txn->mt_txnid-1) & 1]->mm_dbs[FREE_DBI];
			m2.mc_dbflag = (unsigned char *)""; /* probably unnecessary */
#endif
//			fprintf(stderr,"loading more freespace, oldest %u: ", oldest);
			if (!oldest) {
				// if we need to load more, and we haven't already loaded the latest transactions, we need to load them now
				// and we load from the end of the freelist, to right before the last referenced transaction, trying load
				// new transactions before going to older transactions
				// TODO: There might be some heuristic we may want to apply if there is a very large number of read transactions to avoid
				// scanning the whole reader list of every transaction (that needs to load more data)
				oldest = mdb_find_oldest(txn);
				env->me_pgoldest = oldest;
			}
			if (env->me_freelist_end && env->me_freelist_end < oldest) {
				last = env->me_freelist_end;
				key.mv_data = &last; // start at the end of the freelist and read newer transactions free pages
				key.mv_size = sizeof(last);
				rc = mdb_cursor_get(&m2, &key, NULL, op);
				op = MDB_NEXT; // now iterate forwards through the txns of free list
				last = *(txnid_t *) key.mv_data;
			} else {
				// no more transactions to read going forward through newest, we are now going
				// to switch to reading older transactions. However, first we set the op
				// to forward to ensure that we fall into the switch into previous iterations below
				op = MDB_NEXT;
				rc = MDB_NOTFOUND; // fall into switch to previous iterations below
				if (!env->me_freelist_start) {
					env->me_freelist_end = oldest;
					env->me_freelist_start = oldest;
				}
			}
		} else {
			// we are now iterating through the free list entries
			// now iterate
			rc = mdb_cursor_get(&m2, &key, NULL, op);
			if (rc && rc != MDB_NOTFOUND)
				goto fail;
			else {
				mdb_cassert(&m2, key.mv_size > 0);
				last = *(txnid_t *) key.mv_data;
			}
		}
		if (op == MDB_NEXT) {
			// iterating forward from the freelist range to find newer transactions
			if (last >= oldest || rc == MDB_NOTFOUND) {
				env->me_freelist_end = oldest;
				// no more newer transactions, go to the beginning of the range and look for older txns
				op = MDB_SET_RANGE;
				if (env->me_freelist_start <= 1) break; // should be no zero entry, break out
				last = env->me_freelist_start - 1;
				key.mv_data = &last; // start at the start of the freelist and read older transactions free pages
				key.mv_size = sizeof(last);
				mdb_cursor_init(&m2, txn, FREE_DBI, NULL);
				rc = mdb_cursor_get(&m2, &key, NULL, op);
				op = MDB_PREV;
				if (rc) {
					if (rc == MDB_NOTFOUND) {
						rc = mdb_cursor_last(&m2, &key, NULL);
						if (rc) {
							if (rc == MDB_NOTFOUND) break;
							else goto fail;
						}
					} else goto fail;
				}
				mdb_cassert(&m2, key.mv_size > 0);
				last = *(txnid_t*)key.mv_data;
				if (!last) {
					fprintf(stderr, "Invalid null txn id\n");
					rc = MDB_NOTFOUND;
					goto fail;
				}
			} else
				env->me_freelist_end = last + 1;
		} else {
			if (rc == MDB_NOTFOUND) {
				// reached the very start, mark it as 1
				env->me_freelist_start = 1;
				break;
			}
		}
		if (op == MDB_PREV) {
			// move to previous entry until we are before the last start time.
			// note that occasionally this might take multiple iterations if we are in the middle of a
			// rebalance and a node being moved
			while (last >= env->me_freelist_start) {
				// go to previous entry, through prev iteration
				rc = mdb_cursor_get(&m2, &key, NULL, MDB_PREV);
				if (rc) {
					if (rc == MDB_NOTFOUND) {
						// reached the very start, mark it as 1
						env->me_freelist_start = 1;
						break;
					}
					goto fail;
				} else {
			if (key.mv_size == 0) {
			fprintf(stderr, "Invalid zero size key\n");
			rc = MDB_BAD_VALSIZE;
			goto fail;
			}
		}
				last = *(txnid_t *) key.mv_data;
			}
			if (rc == MDB_NOTFOUND) break;
			env->me_freelist_start = last;
		}
		if (!last) {
			fprintf(stderr, "Invalid null txn id\n");
			rc = MDB_NOTFOUND;
			goto fail;
		}

		np = m2.mc_pg[m2.mc_top];
		leaf = NODEPTR(np, m2.mc_ki[m2.mc_top]);
		if ((rc = mdb_node_read(&m2, leaf, &data)) != MDB_SUCCESS)
			goto fail;

		idl = (MDB_ID *) data.mv_data;
		i = idl[0];
		if (!mop) {
			if (!(env->me_pghead = mop = mdb_midl_alloc(i))) {
				rc = ENOMEM;
				goto fail;
			}
		} else {
			if ((rc = mdb_midl_need(&env->me_pghead, i)) != 0)
				goto fail;
			mop = env->me_pghead;
		}
		// consider the whole free-list to be updated now
		env->me_freelist_written_end = 0x7fffffff;
		env->me_freelist_written_start = 1;
#if (MDB_DEBUG) > 1
		DPRINTF(("IDL read txn %"Yu" root %"Yu" num %u",
			last, txn->mt_dbs[FREE_DBI].md_root, i));
		for (j = i; j; j--)
			DPRINTF(("IDL %"Yu, idl[j]));
#endif
		/* Merge in descending sorted order */
		//fprintf(stderr, "Merging %u: ", last);
		//mdb_midl_print(stderr, idl);
		if ((rc = mdb_midl_xmerge(&mop, idl)) != 0) {
			if (rc == -1) rc = 0; // ignore duplicate value errors and just ignore the duplicates
			else goto fail;
		}
		if (mop != env->me_pghead) env->me_pghead = mop;
		mop_len = mop[0];
	}
	continue_best_fit:
	if (best_fit_start > 0) {
		mop[best_fit_start] += num; // block length is a negative, so we add to it in order to subtract the amount we are using
		if (mop[best_fit_start] == -1) mop[best_fit_start] = 0;
		if (env->me_freelist_written_start > best_fit_start || !env->me_freelist_written_start) env->me_freelist_written_start = best_fit_start;
		// we use a negative value as an indicator that this position is in the middle of a block, and we can continue it
		// for sequential writing (faster)
		env->me_freelist_position = -best_fit_start;
		pgno = mop[++best_fit_start];
		if (pgno == 0) {
			fprintf(stderr, "found best-fit freespace entry pointing to root block of size %u at %u\n", best_fit_size, best_fit_start);
			mdb_midl_print(stderr, mop);
		} else {
			mop[best_fit_start] += num; // update position
			if (env->me_freelist_written_end < best_fit_start) env->me_freelist_written_end = best_fit_start;
			//fprintf(stderr, "\nusing best fit size %u of %u\n", num, best_fit_size);
			//env->me_block_size_cache[best_fit_size] = 0; // clear this out of the cache (TODO: could move it)

			i = 1; // indicate that we found something
			goto search_done;
		}
	}
	/* Use new pages from the map when nothing suitable in the freeDB */
	i = 0;
	pgno = txn->mt_next_pgno;
	if (pgno + num >= env->me_maxpg) {
	/* <lmdb-js addition> */
		size_t new_size = ((size_t) (2 * (pgno + num) * env->me_psize / 0x40000 + 1)) * 0x40000;
//		fprintf(stderr, "resizing from %u to %u", env->me_mapsize, new_size);
		rc = mdb_env_set_mapsize(env, new_size);
	/* </lmdb-js addition> */
	}
#if defined(_WIN32) && !defined(MDB_VL32)
	if (env->me_flags & MDB_WRITEMAP) {
		void *p;
		p = (MDB_page *)(env->me_map + env->me_psize * pgno);
		p = VirtualAlloc(p, env->me_psize * num, MEM_COMMIT, PAGE_READWRITE);
		if (!p) {
			DPUTS("VirtualAlloc failed");
			rc = ErrCode();
			goto fail;
		}
	}
#endif

search_done:
	if (env->me_flags & MDB_WRITEMAP) {
		np = (MDB_page *)(env->me_map + env->me_psize * pgno);
	} else {
		if (!(np = mdb_page_malloc(txn, num, 1))) {
			rc = ENOMEM;
			goto fail;
		}
	}
	if (i) {
		// found
		//fprintf(stderr,"f %u %u %u	", num, env->me_freelist_position < 0 ? -env->me_freelist_position : env->me_freelist_position, pgno);
	} else {
		//fprintf(stderr,"a %u %u %u	", num, env->me_freelist_position < 0 ? -env->me_freelist_position : env->me_freelist_position, pgno);
		txn->mt_next_pgno = pgno + num;
	}
#if OVERFLOW_NOTYET
	if (ov) {
		dph->mp_hdr = np->mp_hdr;
		dph->mp_ptr = np;
		np = (MDB_page *)dph;
	}
#endif
	np->mp_pgno = pgno;
	SET_PGTXNID(txn, np);
	np->mp_flags = 0;
#if OVERFLOW_NOTYET
	mdb_page_dirty(txn, np, ov);
#else
	mdb_page_dirty(txn, np);
#endif
	*mp = np;

	return MDB_SUCCESS;

fail:
#if OVERFLOW_NOTYET
	if (dph)
		free(dph);
#endif
fprintf(stderr, "mdb_page_alloc error\n");
	txn->mt_flags |= MDB_TXN_ERROR;
	return rc;
}

/** Copy the used portions of a non-overflow page.
 * @param[in] dst page to copy into
 * @param[in] src page to copy from
 * @param[in] psize size of a page
 */
static void
mdb_page_copy(MDB_page *dst, MDB_page *src, unsigned int psize)
{
	enum { Align = sizeof(pgno_t) };
	indx_t upper = src->mp_upper, lower = src->mp_lower, unused = upper-lower;

	/* If page isn't full, just copy the used portion. Adjust
	 * alignment so memcpy may copy words instead of bytes.
	 */
	if ((unused &= -Align) && !IS_LEAF2(src)) {
		upper = (upper + PAGEBASE) & -Align;
		memcpy(dst, src, (lower + PAGEBASE + (Align-1)) & -Align);
		memcpy((pgno_t *)((char *)dst+upper), (pgno_t *)((char *)src+upper),
			psize - upper);
	} else {
		memcpy(dst, src, psize - unused);
	}
}

/** Bring back a page which this txn spilled to disk; make it writable again.
 * @param[in] txn the transaction handle.
 * @param[in] mp the spilled page.
 * @param[out] ret the writable page.
 */
static int
mdb_page_unspill(MDB_txn *txn, MDB_page *mp, MDB_page **ret)
{
	MDB_env *env = txn->mt_env;
	unsigned x;
	pgno_t pgno = mp->mp_pgno, pn = pgno << 1;

	if (txn->mt_dirty_room == 0)
		return MDB_TXN_FULL;

	/* x = position in current spill list, or 0 */
	x = 0;
	if (txn->mt_spill_pgs) {
		x = mdb_midl_search(txn->mt_spill_pgs, pn);
		if (! (x <= txn->mt_spill_pgs[0] && txn->mt_spill_pgs[x] == pn))
			x = 0;
	}
	if (x == 0 && !txn->mt_parent) {
		/* should be a spilled page */
		fprintf(stderr, "Page %u was unspilled, but was not found in spilled page list (%p)\n", mp->mp_pgno, txn->mt_spill_pgs);
		/*last_error = "mdb_page_unspill no parent";
		return MDB_PROBLEM;		*/
	}

	{
			MDB_page *np;
			int num;
			if (IS_OVERFLOW(mp))
				num = mp->mp_pages;
			else
				num = 1;
			{
				np = mdb_page_malloc(txn, num, 1);
				if (!np)
					return ENOMEM;
				if (num > 1)
					memcpy(np, mp, num * env->me_psize);
				else
					mdb_page_copy(np, mp, env->me_psize);
			}
			if (x) {
				/* If in current txn, this page is no longer spilled.
				 * If it happens to be the last page, truncate the spill list.
				 * Otherwise mark it as deleted by setting the LSB.
				 */
				if (x == txn->mt_spill_pgs[0])
					txn->mt_spill_pgs[0]--;
				else
					txn->mt_spill_pgs[x] |= 1;
			}	/* otherwise, if belonging to a parent txn, the
				 * page remains spilled until child commits
				 */

			mdb_page_dirty(txn, np);
			SET_PGTXNID(txn, np);
			*ret = np;
			return MDB_SUCCESS;
	}
}

/** Touch a page: make it dirty and re-insert into tree with updated pgno.
 * Set #MDB_TXN_ERROR on failure.
 * @param[in] mc cursor pointing to the page to be touched
 * @return 0 on success, non-zero on failure.
 */
static int
mdb_page_touch(MDB_cursor *mc)
{
	MDB_page *mp = mc->mc_pg[mc->mc_top], *np;
	MDB_txn *txn = mc->mc_txn;
	MDB_cursor *m2, *m3;
	unsigned np_flags;
	pgno_t	pgno;
	int rc;

	if (IS_SUBP(mp) || IS_WRITABLE(txn, mp))
		return MDB_SUCCESS;

	if (!IS_MUTABLE(txn, mp)) {
		/* Page from an older snapshot */
		if ((rc = mdb_midl_need(&txn->mt_free_pgs, 1)) ||
			(rc = mdb_page_alloc(mc, 1, &np)))
			goto fail;
		pgno = np->mp_pgno;
		DPRINTF(("touched db %d page %"Yu" -> %"Yu, DDBI(mc),
			mp->mp_pgno, pgno));
		mdb_cassert(mc, mp->mp_pgno != pgno);
		mdb_midl_xappend(txn->mt_free_pgs, mp->mp_pgno);
		/* Update the parent page, if any, to point to the new page */
		if (mc->mc_top) {
			MDB_page *parent = mc->mc_pg[mc->mc_top-1];
			MDB_node *node = NODEPTR(parent, mc->mc_ki[mc->mc_top-1]);
			SETPGNO(node, pgno);
		} else {
			mc->mc_db->md_root = pgno;
		}
	} else if (!IS_DIRTY_NW(txn, mp)) {
		rc = mdb_page_unspill(txn, mp, &np);
		if (rc)
			goto fail;
		goto done;
	} else {
		/* Writable in an ancestor txn */
		MDB_ID2 mid, *dl = txn->mt_u.dirty_list;
		pgno = mp->mp_pgno;
		if (!txn->mt_parent) {
			last_error = "mdb_page_touch no parent";
			rc = MDB_PROBLEM;
			goto fail;
		}
		mdb_cassert(mc, dl[0].mid < MDB_IDL_UM_MAX);
		np = mdb_page_malloc(txn, 1, 1);
		if (!np) {
			rc = ENOMEM;
			goto fail;
		}
		mid.mid = pgno;
		mid.mptr = np;
		rc = mdb_mid2l_insert(dl, &mid);
		mdb_cassert(mc, rc == 0);
	}

	np_flags = np->mp_flags;	/* P_ADM_FLAGS */
	mdb_page_copy(np, mp, txn->mt_env->me_psize);
	np->mp_flags |= np_flags;
	np->mp_pgno = pgno;
	SET_PGTXNID(txn, np);

done:
	/* Adjust cursors pointing to mp */
	mc->mc_pg[mc->mc_top] = np;
	m2 = txn->mt_cursors[mc->mc_dbi];
	if (mc->mc_flags & C_SUB) {
		for (; m2; m2=m2->mc_next) {
			m3 = &m2->mc_xcursor->mx_cursor;
			if (m3->mc_snum < mc->mc_snum) continue;
			if (m3->mc_pg[mc->mc_top] == mp)
				m3->mc_pg[mc->mc_top] = np;
		}
	} else {
		for (; m2; m2=m2->mc_next) {
			if (m2->mc_snum < mc->mc_snum) continue;
			if (m2 == mc) continue;
			if (m2->mc_pg[mc->mc_top] == mp) {
				m2->mc_pg[mc->mc_top] = np;
				if (IS_LEAF(np))
					XCURSOR_REFRESH(m2, mc->mc_top, np);
			}
		}
	}
	MDB_PAGE_UNREF(mc->mc_txn, mp);
	return 0;

fail:
fprintf(stderr, "mdb_page_touch error\n");
	txn->mt_flags |= MDB_TXN_ERROR;
	return rc;
}

#ifdef _WIN32
uint64_t get_time64() {
	return GetTickCount64();
}
#else
uint64_t get_time64() {
	struct timespec time;
	clock_gettime(CLOCK_MONOTONIC, &time);
	return time.tv_sec * 1000000000ll + time.tv_nsec;
}
#endif

int
mdb_env_sync0(MDB_env *env, int force, pgno_t numpgs)
{
	int rc = 0;
	if (env->me_flags & MDB_RDONLY)
		return EACCES;

	if (force || !(env->me_flags & MDB_NOSYNC)
#ifdef _WIN32	/* Sync is normally achieved in Windows by doing WRITE_THROUGH writes */
	 && (env->me_flags & MDB_WRITEMAP)
#endif
		) {
		if (env->me_flags & MDB_WRITEMAP) {
			int flags = ((env->me_flags & MDB_MAPASYNC) && !force)
				? MS_ASYNC : MS_SYNC;
			if (MDB_MSYNC(env->me_map, env->me_psize * numpgs, flags))
				rc = ErrCode();
#ifdef _WIN32
			else if (flags == MS_SYNC && MDB_FDATASYNC(env->me_fd))
				rc = ErrCode();
#endif
		} else {
#ifdef BROKEN_FDATASYNC
			if (env->me_flags & MDB_FSYNCONLY) {
				if (fsync(env->me_fd))
					rc = ErrCode();
			} else
#endif
			if (MDB_FDATASYNC(env->me_fd))
				rc = ErrCode();
		}
	}
	return rc;
}

int
mdb_env_sync(MDB_env *env, int force)
{
	MDB_meta *m = mdb_env_pick_meta(env);
	// <lmdb-js>
	if (env->me_flags & MDB_OVERLAPPINGSYNC) {
		MDB_txninfo *ti = env->me_txns;
		size_t last_txn_id = ti->mti_txnid;
		//fprintf(stderr,"syncing txn %u, ", last_txn_id);
		int rc;
		if (LOCK_MUTEX(rc, env, env->me_sync_mutex))
			return rc;
		if (env->me_synced_txn_id >= last_txn_id) {
			UNLOCK_MUTEX(env->me_sync_mutex);
			return 0;
		}
		uint64_t start = 0;
		if (env->me_flags & MDB_TRACK_METRICS) {
			start = get_time64();
		}
		MDB_txn sync_txn;
		MDB_db dbs[2];
		do {
			m = mdb_env_pick_meta(env);
			sync_txn.mt_env = env;
			sync_txn.mt_flags = 2;
			sync_txn.mt_dbs = dbs;
			sync_txn.mt_dbs[FREE_DBI] = m->mm_dbs[FREE_DBI];
			sync_txn.mt_dbs[MAIN_DBI] = m->mm_dbs[MAIN_DBI];
			sync_txn.mt_dbs[FREE_DBI].md_flags &= ~MDB_OVERLAPPINGSYNC; // clear this to indicate it is flushed txn
			sync_txn.mt_txnid = last_txn_id = m->mm_txnid;
			sync_txn.mt_next_pgno = m->mm_last_pg + 1;
		} while(ti->mti_txnid != last_txn_id); // avoid race condition in copying data by verifying that this is updated
		rc = mdb_env_sync0(env, force, sync_txn.mt_next_pgno);

		if (rc) {
			if (env->me_flags & MDB_TRACK_METRICS) {
				env->me_metrics.time_sync += get_time64() - start;
			}
			UNLOCK_MUTEX(env->me_sync_mutex);
			return rc;
		}
		rc = mdb_env_write_meta(&sync_txn);
		if (rc == 0)
			env->me_synced_txn_id = last_txn_id;
		//fprintf(stderr,"finished syncing txn %u, ", last_txn_id);
		if (env->me_flags & MDB_TRACK_METRICS) {
			env->me_metrics.time_sync += get_time64() - start;
		}
		UNLOCK_MUTEX(env->me_sync_mutex);
		return rc;
	} else {
		uint64_t start = 0;
		if (env->me_flags & MDB_TRACK_METRICS) {
			start = get_time64();
		}
		int rc = mdb_env_sync0(env, force, m->mm_last_pg+1);
		if (env->me_flags & MDB_TRACK_METRICS) {
			env->me_metrics.time_sync += get_time64() - start;
		}
		return rc;
	}
	// </lmdb-js>
}

/** Back up parent txn's cursors, then grab the originals for tracking */
static int
mdb_cursor_shadow(MDB_txn *src, MDB_txn *dst)
{
	MDB_cursor *mc, *bk;
	MDB_xcursor *mx;
	size_t size;
	int i;

	for (i = src->mt_numdbs; --i >= 0; ) {
		if ((mc = src->mt_cursors[i]) != NULL) {
			size = sizeof(MDB_cursor);
			if (mc->mc_xcursor)
				size += sizeof(MDB_xcursor);
			for (; mc; mc = bk->mc_next) {
				bk = malloc(size);
				if (!bk)
					return ENOMEM;
				*bk = *mc;
				mc->mc_backup = bk;
				mc->mc_db = &dst->mt_dbs[i];
				/* Kill pointers into src to reduce abuse: The
				 * user may not use mc until dst ends. But we need a valid
				 * txn pointer here for cursor fixups to keep working.
				 */
				mc->mc_txn		= dst;
				mc->mc_dbflag = &dst->mt_dbflags[i];
				if ((mx = mc->mc_xcursor) != NULL) {
					*(MDB_xcursor *)(bk+1) = *mx;
					mx->mx_cursor.mc_txn = dst;
				}
				mc->mc_next = dst->mt_cursors[i];
				dst->mt_cursors[i] = mc;
			}
		}
	}
	return MDB_SUCCESS;
}

/** Close this write txn's cursors, give parent txn's cursors back to parent.
 * @param[in] txn the transaction handle.
 * @param[in] merge true to keep changes to parent cursors, false to revert.
 * @return 0 on success, non-zero on failure.
 */
static void
mdb_cursors_close(MDB_txn *txn, unsigned merge)
{
	MDB_cursor **cursors = txn->mt_cursors, *mc, *next, *bk;
	MDB_xcursor *mx;
	int i;

	for (i = txn->mt_numdbs; --i >= 0; ) {
		for (mc = cursors[i]; mc; mc = next) {
			next = mc->mc_next;
			if ((bk = mc->mc_backup) != NULL) {
				if (merge) {
					/* Commit changes to parent txn */
					mc->mc_next = bk->mc_next;
					mc->mc_backup = bk->mc_backup;
					mc->mc_txn = bk->mc_txn;
					mc->mc_db = bk->mc_db;
					mc->mc_dbflag = bk->mc_dbflag;
					if ((mx = mc->mc_xcursor) != NULL)
						mx->mx_cursor.mc_txn = bk->mc_txn;
				} else {
					/* Abort nested txn */
					*mc = *bk;
					if ((mx = mc->mc_xcursor) != NULL)
						*mx = *(MDB_xcursor *)(bk+1);
				}
				mc = bk;
			}
			/* Only malloced cursors are permanently tracked. */
			free(mc);
		}
		cursors[i] = NULL;
	}
}

#if !(MDB_PIDLOCK)		/* Currently the same as defined(_WIN32) */
enum Pidlock_op {
	Pidset, Pidcheck
};
#else
enum Pidlock_op {
	Pidset = F_SETLK, Pidcheck = F_GETLK
};
#endif

/** Set or check a pid lock. Set returns 0 on success.
 * Check returns 0 if the process is certainly dead, nonzero if it may
 * be alive (the lock exists or an error happened so we do not know).
 *
 * On Windows Pidset is a no-op, we merely check for the existence
 * of the process with the given pid. On POSIX we use a single byte
 * lock on the lockfile, set at an offset equal to the pid.
 */
static int
mdb_reader_pid(MDB_env *env, enum Pidlock_op op, MDB_PID_T pid)
{
#if !(MDB_PIDLOCK)		/* Currently the same as defined(_WIN32) */
	int ret = 0;
	HANDLE h;
	if (op == Pidcheck) {
		h = OpenProcess(env->me_pidquery, FALSE, pid);
		/* No documented "no such process" code, but other program use this: */
		if (!h)
			return ErrCode() != ERROR_INVALID_PARAMETER;
		/* A process exists until all handles to it close. Has it exited? */
		ret = WaitForSingleObject(h, 0) != 0;
		CloseHandle(h);
	}
	return ret;
#else
	for (;;) {
		int rc;
		struct flock lock_info;
		memset(&lock_info, 0, sizeof(lock_info));
		lock_info.l_type = F_WRLCK;
		lock_info.l_whence = SEEK_SET;
		lock_info.l_start = pid;
		lock_info.l_len = 1;
		if ((rc = fcntl(env->me_lfd, op, &lock_info)) == 0) {
			if (op == F_GETLK && lock_info.l_type != F_UNLCK)
				rc = -1;
		} else if ((rc = ErrCode()) == EINTR) {
			continue;
		}
		return rc;
	}
#endif
}

/** Common code for #mdb_txn_begin() and #mdb_txn_renew().
 * @param[in] txn the transaction handle to initialize
 * @return 0 on success, non-zero on failure.
 */
static int
mdb_txn_renew0(MDB_txn *txn)
{
	MDB_env *env = txn->mt_env;
	MDB_txninfo *ti = env->me_txns;
	MDB_meta *meta;
	unsigned int i, nr, flags = txn->mt_flags;
	uint16_t x;
	int rc, new_notls = 0;

	if ((flags &= MDB_TXN_RDONLY) != 0) {
		if (!ti) {
			meta = mdb_env_pick_meta(env);
			txn->mt_txnid = meta->mm_txnid;
			txn->mt_u.reader = NULL;
		} else {
			MDB_reader *r = (env->me_flags & MDB_NOTLS) ? txn->mt_u.reader :
				pthread_getspecific(env->me_txkey);
			if (r) {
				if (r->mr_pid != env->me_pid || r->mr_txnid != (txnid_t)-1) {
					last_error = malloc(100);
					if (r->mr_pid != env->me_pid)
						sprintf(last_error, "The reader lock pid %u, txn %i, doesn't match env pid %u", r->mr_pid, r->mr_txnid, env->me_pid);
					else
						sprintf(last_error, "The reader lock has a txn id %i", r->mr_txnid);
					return MDB_BAD_RSLOT;
				}
			} else {
				MDB_PID_T pid = env->me_pid;
				MDB_THR_T tid = pthread_self();
				mdb_mutexref_t rmutex = env->me_rmutex;

				if (!env->me_live_reader) {
					rc = mdb_reader_pid(env, Pidset, pid);
					if (rc)
						return rc;
					env->me_live_reader = 1;
				}

				if (LOCK_MUTEX(rc, env, rmutex))
					return rc;
				nr = ti->mti_numreaders;
				for (i=0; i<nr; i++)
					if (ti->mti_readers[i].mr_pid == 0)
						break;
				if (i == env->me_maxreaders) {
					UNLOCK_MUTEX(rmutex);
					return MDB_READERS_FULL;
				}
				r = &ti->mti_readers[i];
				/* Claim the reader slot, carefully since other code
				 * uses the reader table un-mutexed: First reset the
				 * slot, next publish it in mti_numreaders.	After
				 * that, it is safe for mdb_env_close() to touch it.
				 * When it will be closed, we can finally claim it.
				 */
				r->mr_pid = 0;
				r->mr_txnid = (txnid_t)-1;
				r->mr_tid = tid;
				if (i == nr)
					ti->mti_numreaders = ++nr;
				env->me_close_readers = nr;
				r->mr_pid = pid;
				UNLOCK_MUTEX(rmutex);

				new_notls = (env->me_flags & MDB_NOTLS);
				if (!new_notls && (rc=pthread_setspecific(env->me_txkey, r))) {
					r->mr_pid = 0;
					return rc;
				}
			}
			do /* LY: Retry on a race, ITS#7970. */
				r->mr_txnid = ti->mti_txnid;
			while(r->mr_txnid != ti->mti_txnid);
			txn->mt_txnid = r->mr_txnid;
			txn->mt_u.reader = r;
			meta = env->me_metas[txn->mt_txnid & 1];
		}

	} else {
		/* Not yet touching txn == env->me_txn0, it may be active */
		uint64_t start_lock_time;
		if (env->me_flags & MDB_TRACK_METRICS) {
			start_lock_time = get_time64();
		}
		if (ti) {
			if (LOCK_MUTEX(rc, env, env->me_wmutex))
				return rc;
			if (txn->mt_txnid != ti->mti_txnid) {
				//fprintf(stderr, "freelist reset, expected txn id %u, db txn id %u\n", txn->mt_txnid, ti->mti_txnid);
				if (env->me_pghead) mdb_midl_free(env->me_pghead);
				env->me_pghead = NULL;
				env->me_freelist_start = 0;
				env->me_freelist_end = 0;
				env->me_freelist_position = 0;
				// TODO: Free it first
				//env->me_block_size_cache = NULL;
			}
			env->me_pgoldest = 0; // we use this as an indicator that we need to load the newest transactions on the next transaction
			if (env->me_freelist_position < 0)
				env->me_freelist_position = -env->me_freelist_position;
			txn->mt_txnid = ti->mti_txnid;
			meta = env->me_metas[txn->mt_txnid & 1];
		} else {
			meta = mdb_env_pick_meta(env);
			txn->mt_txnid = meta->mm_txnid;
		}
		if (env->me_flags & MDB_TRACK_METRICS) {
			uint64_t now = get_time64();
			env->me_metrics.time_start_txns += now - start_lock_time;
			env->me_metrics.clock_txn = now;
		}
		txn->mt_txnid++;
#if MDB_DEBUG
		if (txn->mt_txnid == mdb_debug_start)
			mdb_debug = 1;
#endif
		txn->mt_child = NULL;
		txn->mt_loose_pgs = NULL;
		txn->mt_loose_count = 0;
		if (env->me_flags & MDB_WRITEMAP) {
			txn->mt_workid = txn->mt_txnid;
			txn->mt_dirty_room = 1;
		} else {
			txn->mt_workid = (txn->mt_txnid | MDB_PGTXNID_FLAGMASK) + 1;
			txn->mt_dirty_room = MDB_IDL_UM_MAX;
		}
		txn->mt_last_workid = txn->mt_workid;
		txn->mt_u.dirty_list = env->me_dirty_list;
		txn->mt_u.dirty_list[0].mid = 0;
		txn->mt_free_pgs = env->me_free_pgs;
		txn->mt_free_pgs[0] = 0;
		txn->mt_spill_pgs = NULL;
#if OVERFLOW_NOTYET
		txn->mt_dirty_ovs = NULL;
#endif
		env->me_txn = txn;
		memcpy(txn->mt_dbiseqs, env->me_dbiseqs, env->me_maxdbs * sizeof(unsigned int));
	}

	/* Copy the DB info and flags */
	memcpy(txn->mt_dbs, meta->mm_dbs, CORE_DBS * sizeof(MDB_db));

	/* Moved to here to avoid a data race in read TXNs */
	txn->mt_next_pgno = meta->mm_last_pg+1;
#if MDB_RPAGE_CACHE
	if (MDB_REMAPPING(env->me_flags))
		txn->mt_last_pgno = txn->mt_next_pgno - 1;
#endif

	txn->mt_flags = flags;

	/* Setup db info */
	txn->mt_numdbs = env->me_numdbs;
	for (i=CORE_DBS; i<txn->mt_numdbs; i++) {
		x = env->me_dbflags[i];
		txn->mt_dbs[i].md_flags = x & PERSISTENT_FLAGS;
		txn->mt_dbflags[i] = (x & MDB_VALID) ? DB_VALID|DB_USRVALID|DB_STALE : 0;
	}
	txn->mt_dbflags[MAIN_DBI] = DB_VALID|DB_USRVALID;
	txn->mt_dbflags[FREE_DBI] = DB_VALID;

	if (env->me_flags & MDB_FATAL_ERROR) {
		DPUTS("environment had fatal error, must shutdown!");
		rc = MDB_PANIC;
	} else {
		/* <lmdb-js change> */
		if (env->me_maxpg < txn->mt_next_pgno) {
			// need to resize map
			size_t new_size = ((size_t) (2 * (txn->mt_next_pgno) * env->me_psize / 0x40000 + 1)) * 0x40000;
			mdb_env_set_mapsize(env, new_size);
		}
		/* </lmdb-js change> */
		return MDB_SUCCESS;
	}
	mdb_txn_end(txn, new_notls /*0 or MDB_END_SLOT*/ | MDB_END_FAIL_BEGIN);
	return rc;
}

int
mdb_txn_renew(MDB_txn *txn)
{
	int rc;

	if (!txn || !F_ISSET(txn->mt_flags, MDB_TXN_RDONLY|MDB_TXN_FINISHED)) {
		if (!txn)
			last_error = "No transaction to renew";
		else if (F_ISSET(txn->mt_flags, MDB_TXN_RDONLY)) {
			// Txn is already renewed, consider this as invalid for compatibility with v1
			return EINVAL;
		} else {
			last_error = malloc(100);
			sprintf(last_error, "Transaction flag was invalid for renew: %u", txn->mt_flags);
		}
		return MDB_BAD_TXN; // if the transaction is not read-only, communicate this with a separate error code
	}

	rc = mdb_txn_renew0(txn);
	if (rc == MDB_SUCCESS) {
		DPRINTF(("renew txn %"Yu"%c %p on mdbenv %p, root page %"Yu,
			txn->mt_txnid, (txn->mt_flags & MDB_TXN_RDONLY) ? 'r' : 'w',
			(void *)txn, (void *)txn->mt_env, txn->mt_dbs[MAIN_DBI].md_root));
	} else {
		last_error = mdb_strerror(rc);
	}
	return rc;
}

/** Used up all workids.	Rewind it and update dirty pages to match. */
static txnid_t ESECT
mdb_workid_rewind(MDB_txn *txn)
{
	txnid_t workid, diff;

	while (txn->mt_parent)
		txn = txn->mt_parent;
	workid = txn->mt_txnid & ~MDB_PGTXNID_FLAGMASK;
	do {
		workid += MDB_PGTXNID_STEP;
		diff = txn->mt_last_workid - workid;
		if (diff) {
			MDB_ID2L dl = txn->mt_u.dirty_list;
			int i;
			for (i = dl[0].mid; i; i--) {
				if (MDB_PGTXNID_FLAGBITS)
					((MDB_page *)dl[i].mptr)->mp_txnid -= diff;
				else
					((MDB_page *)dl[i].mptr)->mp_txnid = workid;
			}
			txn->mt_workid = txn->mt_last_workid = workid;
		}
	} while ((txn = txn->mt_child) != NULL);
	return workid;
}

int ESECT
mdb_txn_set_callback(MDB_txn *txn, MDB_txn_visible *func, void* ctx)
{
	if (!txn)
		return EINVAL;
	txn->mt_callback = func;
	txn->mt_ctx = ctx;
	return MDB_SUCCESS;
}

int
mdb_txn_begin(MDB_env *env, MDB_txn *parent, unsigned int flags, MDB_txn **ret)
{
	MDB_txn *txn;
	MDB_ntxn *ntxn;
	int rc, size, tsize;

	flags &= MDB_TXN_BEGIN_FLAGS;
	flags |= env->me_flags & MDB_WRITEMAP;

	if (env->me_flags & MDB_RDONLY & ~flags) /* write txn in RDONLY env */
		return EACCES;

	if (parent) {
		/* Nested transactions: Max 1 child, write txns only, no writemap */
		flags |= parent->mt_flags;
		if (flags & (MDB_RDONLY|MDB_WRITEMAP|MDB_TXN_BLOCKED)) {
			return (parent->mt_flags & MDB_TXN_RDONLY) ? EINVAL : MDB_BAD_TXN;
		}
		/* Child txns save MDB_pgstate and use own copy of cursors */
		size = env->me_maxdbs * (sizeof(MDB_db)+sizeof(MDB_cursor *)+1);
		size += tsize = sizeof(MDB_ntxn);
	} else if (flags & MDB_RDONLY) {
		size = env->me_maxdbs * (sizeof(MDB_db)+1);
		size += tsize = sizeof(MDB_txn);
	} else {
		/* Reuse preallocated write txn. However, do not touch it until
		 * mdb_txn_renew0() succeeds, since it currently may be active.
		 */
		txn = env->me_txn0;
		goto renew;
	}
	if ((txn = calloc(1, size)) == NULL) {
		DPRINTF(("calloc: %s", strerror(errno)));
		return ENOMEM;
	}
#if MDB_RPAGE_CACHE
	if (MDB_REMAPPING(env->me_flags) && !parent) {
		txn->mt_rpages = malloc(MDB_TRPAGE_SIZE * sizeof(MDB_ID3));
		if (!txn->mt_rpages) {
			free(txn);
			return ENOMEM;
		}
		txn->mt_rpages[0].mid = 0;
		txn->mt_rpcheck = MDB_TRPAGE_SIZE/2;
	}
#endif
	txn->mt_dbxs = env->me_dbxs;	/* static */
	txn->mt_dbs = (MDB_db *) ((char *)txn + tsize);
	txn->mt_dbflags = (unsigned char *)txn + size - env->me_maxdbs;
	txn->mt_flags = flags;
	txn->mt_env = env;

	if (parent) {
		unsigned int i;
		txnid_t workid = parent->mt_last_workid + MDB_PGTXNID_STEP;
		if (!workid) /* wraparound after lots of previous children */
			workid = mdb_workid_rewind(parent) + MDB_PGTXNID_STEP;
		txn->mt_workid = txn->mt_last_workid = workid;
		txn->mt_cursors = (MDB_cursor **)(txn->mt_dbs + env->me_maxdbs);
		txn->mt_dbiseqs = parent->mt_dbiseqs;
		txn->mt_u.dirty_list = malloc(sizeof(MDB_ID2)*MDB_IDL_UM_SIZE);
		if (!txn->mt_u.dirty_list ||
			!(txn->mt_free_pgs = mdb_midl_alloc(MDB_IDL_UM_MAX)))
		{
			free(txn->mt_u.dirty_list);
			free(txn);
			return ENOMEM;
		}
		txn->mt_txnid = parent->mt_txnid;
		txn->mt_dirty_room = parent->mt_dirty_room;
		txn->mt_u.dirty_list[0].mid = 0;
		txn->mt_spill_pgs = NULL;
#if OVERFLOW_NOTYET
		txn->mt_dirty_ovs = NULL;
#endif
		txn->mt_next_pgno = parent->mt_next_pgno;
		parent->mt_flags |= MDB_TXN_HAS_CHILD;
		parent->mt_child = txn;
		txn->mt_parent = parent;
		txn->mt_numdbs = parent->mt_numdbs;
#if MDB_RPAGE_CACHE
		if (MDB_REMAPPING(env->me_flags))
			txn->mt_rpages = parent->mt_rpages;
#endif
		memcpy(txn->mt_dbs, parent->mt_dbs, txn->mt_numdbs * sizeof(MDB_db));
		/* Copy parent's mt_dbflags, but clear DB_NEW */
		for (i=0; i<txn->mt_numdbs; i++)
			txn->mt_dbflags[i] = parent->mt_dbflags[i] & ~DB_NEW;
		rc = 0;
		ntxn = (MDB_ntxn *)txn;
		ntxn->mnt_pgstate = env->me_pgstate; /* save parent me_pghead & co */
		if (env->me_pghead) {
			size = MDB_IDL_SIZEOF(env->me_pghead);
			env->me_pghead = mdb_midl_alloc(env->me_pghead[0]);
			if (env->me_pghead)
				memcpy(env->me_pghead, ntxn->mnt_pgstate.mf_pghead, size);
			else
				rc = ENOMEM;
		}
		if (!rc)
			rc = mdb_cursor_shadow(parent, txn);
		if (rc)
			mdb_txn_end(txn, MDB_END_FAIL_BEGINCHILD);
	} else { /* MDB_RDONLY */
		txn->mt_dbiseqs = env->me_dbiseqs;
renew:
		rc = mdb_txn_renew0(txn);
	}
	txn->mt_callback = NULL;
	if (rc) {
		if (txn != env->me_txn0) {
#if MDB_RPAGE_CACHE
			if (MDB_REMAPPING(env->me_flags))
				free(txn->mt_rpages);
#endif
			free(txn);
		}
	} else {
		txn->mt_flags |= flags;	/* could not change txn=me_txn0 earlier */
		*ret = txn;
		DPRINTF(("begin txn %"Yu"%c %p on mdbenv %p, root page %"Yu,
			txn->mt_txnid, (flags & MDB_RDONLY) ? 'r' : 'w',
			(void *) txn, (void *) env, txn->mt_dbs[MAIN_DBI].md_root));
	}

	return rc;
}

MDB_env *
mdb_txn_env(MDB_txn *txn)
{
	if(!txn) return NULL;
	return txn->mt_env;
}

mdb_size_t
mdb_txn_id(MDB_txn *txn)
{
		if(!txn) return 0;
		return txn->mt_txnid;
}

/** Export or close DBI handles opened in this txn. */
static void
mdb_dbis_update(MDB_txn *txn, int keep)
{
	int i;
	MDB_dbi n = txn->mt_numdbs;
	MDB_env *env = txn->mt_env;
	unsigned char *tdbflags = txn->mt_dbflags;

	for (i = n; --i >= CORE_DBS;) {
		if (tdbflags[i] & DB_NEW) {
			if (keep) {
				env->me_dbflags[i] = txn->mt_dbs[i].md_flags | MDB_VALID;
			} else {
				char *ptr = env->me_dbxs[i].md_name.mv_data;
				if (ptr) {
					env->me_dbxs[i].md_name.mv_data = NULL;
					env->me_dbxs[i].md_name.mv_size = 0;
					env->me_dbflags[i] = 0;
					env->me_dbiseqs[i]++;
					free(ptr);
				}
			}
		}
	}
	if (keep && env->me_numdbs < n)
		env->me_numdbs = n;
}

/** End a transaction, except successful commit of a nested transaction.
 * May be called twice for readonly txns: First reset it, then abort.
 * @param[in] txn the transaction handle to end
 * @param[in] mode why and how to end the transaction
 */
static void
mdb_txn_end(MDB_txn *txn, unsigned mode)
{
	MDB_env	*env = txn->mt_env;
#if MDB_DEBUG
	static const char *const names[] = MDB_END_NAMES;
#endif

	/* Export or close DBI handles opened in this txn */
	mdb_dbis_update(txn, mode & MDB_END_UPDATE);

	DPRINTF(("%s txn %"Yu"%c %p on mdbenv %p, root page %"Yu,
		names[mode & MDB_END_OPMASK],
		txn->mt_txnid, (txn->mt_flags & MDB_TXN_RDONLY) ? 'r' : 'w',
		(void *) txn, (void *)env, txn->mt_dbs[MAIN_DBI].md_root));

	if (F_ISSET(txn->mt_flags, MDB_TXN_RDONLY)) {
		if (txn->mt_u.reader) {
			txn->mt_u.reader->mr_txnid = (txnid_t)-1;
			if (!(env->me_flags & MDB_NOTLS)) {
				txn->mt_u.reader = NULL; /* txn does not own reader */
			} else if (mode & MDB_END_SLOT) {
				txn->mt_u.reader->mr_pid = 0;
				txn->mt_u.reader = NULL;
			} /* else txn owns the slot until it does MDB_END_SLOT */
		}
		txn->mt_numdbs = 0;		/* prevent further DBI activity */
		txn->mt_flags |= MDB_TXN_FINISHED;

	} else if (!F_ISSET(txn->mt_flags, MDB_TXN_FINISHED)) {
		pgno_t *pghead = env->me_pghead;

		if (!(mode & MDB_END_UPDATE)) /* !(already closed cursors) */
			mdb_cursors_close(txn, 0);
		if (!(env->me_flags & MDB_WRITEMAP)) {
			mdb_dlist_free(txn);
		}

		txn->mt_numdbs = 0;
		txn->mt_flags = MDB_TXN_FINISHED;

		mdb_midl_free(txn->mt_spill_pgs);
#if OVERFLOW_NOTYET
		mdb_mid2l_free(txn->mt_dirty_ovs);
#endif
		if (!txn->mt_parent) {
			mdb_midl_shrink(&txn->mt_free_pgs);
			env->me_free_pgs = txn->mt_free_pgs;
			/* me_pgstate: */
			if (env->me_pghead && (env->me_pghead[0] > env->me_maxfreepgs_to_retain || env->me_freelist_end + 300 < txn->mt_txnid || (mode & MDB_END_ABORT))) {
				//fprintf(stderr, "Free list too large %u or out of date (%u / %u) or aborted, dumping from memory\n", env->me_pghead[0], env->me_freelist_end, txn->mt_txnid);
				// if it is too large, reset it
				mdb_midl_free(env->me_pghead);
				env->me_pghead = NULL;
				env->me_freelist_start = 0;
				env->me_freelist_end = 0;
			}

			env->me_txn = NULL;
			mode = 0;	/* txn == env->me_txn0, do not free() it */

			/* The writer mutex was locked in mdb_txn_begin. */
			if (env->me_txns)
				UNLOCK_MUTEX(env->me_wmutex);
		} else {
			txn->mt_parent->mt_child = NULL;
			txn->mt_parent->mt_flags &= ~MDB_TXN_HAS_CHILD;
			env->me_pgstate = ((MDB_ntxn *)txn)->mnt_pgstate;
			mdb_midl_free(txn->mt_free_pgs);
			free(txn->mt_u.dirty_list);
		}
	}
#if MDB_RPAGE_CACHE
	if (MDB_REMAPPING(env->me_flags) && !txn->mt_parent) {
		MDB_ID3L el = env->me_rpages, tl = txn->mt_rpages;
		unsigned i, x, n = tl[0].mid;
		pthread_mutex_lock(&env->me_rpmutex);
		for (i = 1; i <= n; i++) {
			if (tl[i].mid & (MDB_RPAGE_CHUNK-1)) {
				/* tmp overflow pages that we didn't share in env */
				munmap(tl[i].mptr, tl[i].mcnt * env->me_psize);
				if (tl[i].menc) {
					mdb_rpage_dispose(env, &tl[i]);
					tl[i].menc = NULL;
				}
			} else {
				x = mdb_mid3l_search(el, tl[i].mid);
				if (tl[i].mptr == el[x].mptr) {
					el[x].mref--;
					if (!el[x].mref)
						el[x].muse = 0;
				} else {
					/* another tmp overflow page */
					munmap(tl[i].mptr, tl[i].mcnt * env->me_psize);
					if (tl[i].menc) {
						mdb_rpage_dispose(env, &tl[i]);
						tl[i].menc = NULL;
					}
				}
			}
		}
		pthread_mutex_unlock(&env->me_rpmutex);
		tl[0].mid = 0;
		if (mode & MDB_END_FREE)
			free(tl);
	}
#endif
	if (mode & MDB_END_FREE)
		free(txn);
}

void
mdb_txn_reset(MDB_txn *txn)
{
	if (txn == NULL)
		return;

	/* This call is only valid for read-only txns */
	if (!(txn->mt_flags & MDB_TXN_RDONLY))
		return;

	mdb_txn_end(txn, MDB_END_RESET);
}

void
mdb_txn_abort(MDB_txn *txn)
{
	if (txn == NULL)
		return;

	if (txn->mt_child)
		mdb_txn_abort(txn->mt_child);
	mdb_txn_end(txn, MDB_END_ABORT|MDB_END_SLOT|MDB_END_FREE);
}

/** Save the freelist as of this transaction to the freeDB.
 * This changes the freelist. Keep trying until it stabilizes.
 *
 * When (MDB_DEVEL) & 2, the changes do not affect #mdb_page_alloc(),
 * it then uses the transaction's original snapshot of the freeDB.
 */
static int
mdb_freelist_save(MDB_txn *txn)
{
	/* env->me_pghead[] can grow and shrink during this call.
	 * env->me_freelist_start and txn->mt_free_pgs[] can only grow.
	 * Page numbers cannot disappear from txn->mt_free_pgs[].
	 */
	char fl_writes[100000];
	MDB_cursor mc;
	MDB_env	*env = txn->mt_env;
	int rc, maxfree_1pg = env->me_maxfree_1pg, more = 1, lost_loose = 0;
	txnid_t	pglast = 0;
	txnid_t head_id = 0;
	pgno_t	freecnt = 0, *free_pgs, *mop;
	ssize_t	head_room = 0, total_room = 0, mop_len, clean_limit;
	unsigned entry_size;

	mdb_cursor_init(&mc, txn, FREE_DBI, NULL);

	if (!env->me_pghead && txn->mt_loose_pgs) {
		/* Put loose page numbers in mt_free_pgs, since
		 * we may be unable to return them to me_pghead.
		 */
		MDB_page *mp = txn->mt_loose_pgs;
		MDB_ID2 *dl = txn->mt_u.dirty_list;
		unsigned x;
		if ((rc = mdb_midl_need(&txn->mt_free_pgs, txn->mt_loose_count)) != 0)
			return rc;
		for (; mp; mp = NEXT_LOOSE_PAGE(mp)) {
			mdb_midl_xappend(txn->mt_free_pgs, mp->mp_pgno);
			/* must also remove from dirty list */
			if (txn->mt_flags & MDB_TXN_WRITEMAP) {
				for (x=1; x<=dl[0].mid; x++)
					if (dl[x].mid == mp->mp_pgno)
						break;
				mdb_tassert(txn, x <= dl[0].mid);
			} else {
				x = mdb_mid2l_search(dl, mp->mp_pgno);
				mdb_tassert(txn, dl[x].mid == mp->mp_pgno);
				mdb_dpage_free(env, mp);
			}
			dl[x].mptr = NULL;
		}
		{
			/* squash freed slots out of the dirty list */
			unsigned y;
			for (y=1; dl[y].mptr && y <= dl[0].mid; y++);
			if (y <= dl[0].mid) {
				for(x=y, y++;;) {
					while (!dl[y].mptr && y <= dl[0].mid) y++;
					if (y > dl[0].mid) break;
					dl[x++] = dl[y++];
				}
				dl[0].mid = x-1;
			} else {
				/* all slots freed */
				dl[0].mid = 0;
			}
		}
		txn->mt_loose_pgs = NULL;
		txn->mt_loose_count = 0;
	}
	ssize_t reserved_space;
	ssize_t start_written;
	int freelist_written_start;
	int freelist_written_end;
	ssize_t end_written;
	/* MDB_RESERVE cancels meminit in ovpage malloc (when no WRITEMAP) */
	clean_limit = (env->me_flags & (MDB_NOMEMINIT|MDB_WRITEMAP))
		? SSIZE_MAX : maxfree_1pg;
//fprintf(stderr, "fl position: %u start: %u, end: %u, txn_id: %u\n", env->me_freelist_position, env->me_freelist_start, env->me_freelist_end, txn->mt_txnid);
	mop = env->me_pghead;
	mop_len = (mdb_midl_is_empty(mop) ? 0 : mop[0]) + txn->mt_loose_count;
	do {
		/* Come back here after each Put() in case freelist changed */
		/*if (start_written >= 0) {
			fprintf(stderr, "Rerunning freelist save, start was %u is %u, length was %u is %u\n",start_written, env->me_freelist_start, reserved_space, mop_len);
		}*/
		MDB_val key, data;
		pgno_t *pgs;
		ssize_t j;

		/* Save the IDL of pages freed by this txn, to a single record
		 * We do this first because sometimes this will end up exhausting the freelist
		 * so we don't have to write anything after this*/
		if (freecnt < txn->mt_free_pgs[0]) {
			if (!freecnt) {
				/* Make sure last page of freeDB is touched and on freelist */
				rc = mdb_page_search(&mc, NULL, MDB_PS_LAST|MDB_PS_MODIFY);
				if (rc && rc != MDB_NOTFOUND)
					return rc;
			}
			free_pgs = txn->mt_free_pgs;
			/* Write to last page of freeDB */
			key.mv_size = sizeof(txn->mt_txnid);
			key.mv_data = &txn->mt_txnid;
			do {
				freecnt = free_pgs[0];
				data.mv_size = MDB_IDL_SIZEOF(free_pgs);
				//fprintf(stderr, "write new freed pages, id: %u ", txn->mt_txnid);
				//mdb_midl_print(stderr, free_pgs);
				rc = mdb_cursor_put(&mc, &key, &data, MDB_RESERVE);
				if (rc)
					return rc;
				/* Retry if mt_free_pgs[] grew during the Put() */
				free_pgs = txn->mt_free_pgs;
			} while (freecnt < free_pgs[0]);
			//mdb_midl_sort(free_pgs);
			memcpy(data.mv_data, free_pgs, data.mv_size);
#if (MDB_DEBUG) > 1
			{
				unsigned int i = free_pgs[0];
				DPRINTF(("IDL write txn %"Yu" root %"Yu" num %u",
					txn->mt_txnid, txn->mt_dbs[FREE_DBI].md_root, i));
				for (; i; i--)
					DPRINTF(("IDL %"Yu, free_pgs[i]));
			}
#endif
//			continue;
		}

		/* Reserve records for me_pghead[]. Split it if multi-page,
		 * to avoid searching freeDB for a page range. Use keys in
		 * range [me_freelist_start me_freelist_end]: Smaller than txnid of oldest reader.
		 */
		head_id = start_written = env->me_freelist_start;
		// TODO: We may want to the smallest id we can here (previous entry + 1)
		reserved_space = mop_len;
		if (!head_id) {
			head_id = 1;
			start_written = 1;
		}
		freelist_written_start = env->me_freelist_written_start;
		freelist_written_end = env->me_freelist_written_end;
		if (mop_len) {
			// determine the size of the blocks we need
			entry_size = ((mop_len / ((env->me_freelist_end - start_written) * maxfree_1pg)) + 1) * maxfree_1pg;
			pglast = (mop_len - 1) / entry_size + head_id;
			//fprintf(stderr, "Reserving in-memory freelist, length: %u, ids: ", mop_len);
			int i = 0;
			while (head_id <= pglast) {
				key.mv_size = sizeof(head_id);
				key.mv_data = &head_id;
				int len = mop_len > entry_size ? entry_size : mop_len;
				unsigned end = mop_len;
				mop_len -= len;
				unsigned start = mop_len > 0 ? mop_len - 1 : 0;
				char do_write = env->me_freelist_written_start <= end && env->me_freelist_written_end >= start;
				fl_writes[i++] = do_write;
				// determine if it is in the range of actively written pages
				if (freelist_written_start <= end && freelist_written_end >= start) {
					// we will reserve one entry for size and potentially one extra entry in case we are splitting an entry
					data.mv_size = (len + (head_id < pglast ? 2 : 1)) * sizeof(pgno_t);
					//fprintf(stderr, "id: %u size: %u", head_id, data.mv_size);
					rc = mdb_cursor_put(&mc, &key, &data, MDB_RESERVE);
					*(pgno_t*)data.mv_data = 0; // set the size of the entry to zero in case it doesn't get overwritten
					head_id++;
					if (rc)
						return rc;
				} else {
					//fprintf(stderr, "Skipping reservation of freelist %u from %u-%u", head_id, mop_len, end);
					head_id++;
				}
			}
		} else pglast = head_id - 1;

		/* If using records from freeDB which we have not yet
		 * deleted, delete them and any we reserved for me_pghead.
		 */
		if (env->me_pghead) {
			//fprintf(stderr, "Free list start %u, end %u, pglast %u\n", env->me_freelist_start, env->me_freelist_end, pglast);
			do {
				key.mv_size = sizeof(head_id);
				key.mv_data = &head_id;
				rc = mdb_cursor_get(&mc, &key, NULL, MDB_SET_RANGE);
				if (rc) break;
				head_id = *(txnid_t *) key.mv_data;
				if (head_id >= env->me_freelist_end) // finished
					break;
				total_room = head_room = 0;
				mdb_tassert(txn, head_id >= env->me_freelist_start);
				//fprintf(stderr, "Deleting free list record %u\n", head_id);
				rc = mdb_cursor_del(&mc, 0);
				if (rc) {
					last_error = "Attempting to delete free-space record";
					return rc;
				}
			} while(1);
		}
		mop = env->me_pghead;
		if (mdb_midl_is_empty(mop) && mop) {
			mop[0] = 0;
		}
		mop_len = (mop ? mop[0] : 0) + txn->mt_loose_count;
		//fprintf(stderr, "mop_len %u = mop[0] %u + txn->mt_loose_count %u, reserved_sape,", mop_len, (mop ? mop[0] : 0), txn->mt_loose_count, reserved_space);
		if (freelist_written_start != env->me_freelist_written_start)
		freelist_written_end = env->me_freelist_written_end;
	} while(reserved_space != mop_len ||
		 (start_written > env->me_freelist_start && env->me_freelist_start > 0) ||
		 freecnt < txn->mt_free_pgs[0] ||
		 freelist_written_start != env->me_freelist_written_start ||
		 freelist_written_end != env->me_freelist_written_end);

	/* Return loose page numbers to me_pghead, though usually none are
	 * left at this point.	The pages themselves remain in dirty_list.
	 */
	if (txn->mt_loose_pgs) {
		MDB_page *mp = txn->mt_loose_pgs;
		if (!env->me_pghead) env->me_pghead = mdb_midl_alloc(1);
		unsigned count = txn->mt_loose_count;
		lost_loose += count;
		for (; mp; mp = NEXT_LOOSE_PAGE(mp))
			mdb_midl_append(&env->me_pghead, mp->mp_pgno);
		txn->mt_loose_pgs = NULL;
		txn->mt_loose_count = 0;
		mop = env->me_pghead;
		if (mop_len != mop[0]) {
			fprintf(stderr, "New free-space list does not match added number of loose pages, truncating additional loose pages, original length: %u, loose page count: %u, new total free-space count: %u\n", mop_len, count, mop[0]);
			mop[0] = mop_len;
		}
	}

	/* Fill in the reserved me_pghead records.	Everything is finally
	 * in place, so this will not allocate or free any DB pages.
	 */
	rc = MDB_SUCCESS;
	MDB_val key, data;

	/* Protect DB env from any (buggy) freelist use when saving mop */
	MDB_IDL pghead = env->me_pghead;
	env->me_pghead = NULL;
	txn->mt_dirty_room = 0;

//	fprintf(stderr, "Writing in-memory freelist, length: %u ids: ", mop_len);
	int i = 0;
	for (txnid_t id = start_written; id <= pglast; id++) {
		key.mv_size = sizeof(id);
		key.mv_data = &id;
		rc = mdb_cursor_get(&mc, &key, &data, MDB_SET_KEY);
		if (rc == MDB_NOTFOUND) {
			if (freelist_written_start < mop_len) {
				fprintf(stderr, "Freelist record not found %u %u %u %u %u %u %i %i %i %i %i %u %u\n", id, mop_len, start_written,
						pglast, env->me_freelist_start, env->me_freelist_end,
						freelist_written_start, env->me_freelist_written_start, freelist_written_end, env->me_freelist_written_end,
						i, fl_writes[0], fl_writes[1]);
				mdb_tassert(txn, freelist_written_start >= mop_len);
			}
			rc = 0; // this is acceptable as long as there are no entries to write
			break;
		}


		//fprintf(stderr, "freelist put: %u, entry size %u, reserved_space: %u, mop_len %u, ", id, data.mv_size, reserved_space, mop_len);

		int reserved_len = (data.mv_size / sizeof(id)) - 1;
		ssize_t reserved_end = reserved_space;
		ssize_t end = mop_len;
		ssize_t save_end = mop[end];
		if (save_end < 0) {
			// ends with a block length, zero it out, was already handled by previous iteration
			mop[end] = 0;
		}
		ssize_t start = reserved_space - reserved_len;
		int len = end - start;
		if (len < 0) len = 0;

		ssize_t save2 = 0;
		ssize_t save = mop[start];
		if (reserved_len > entry_size) {
			// these are full blocks, so not expecting to reach the end of our iteration and the start of the mop
			reserved_space = start + 1;
		} else {
			if (reserved_space > entry_size) {
				fprintf(stderr, "reserved_space too large %u %u %u %u %u %u", reserved_space, mop_len, entry_size, start_written, id, pglast);
			}
			// mdb_tassert(txn, reserved_space <= entry_size); // I believe this may result in lost free space, but should be recoverable
			reserved_space -= reserved_len;
			if (reserved_space < 0) reserved_space = 0;
			if (reserved_space > 0) {
				// this shouldn't happen, it means that somehow something was written to the freelist that was not accounted for
				fprintf(stderr, "reserved_space larger than allocated entry %u %u %u %u %u %u", reserved_space, mop_len, entry_size, mop[reserved_space - 1], id, pglast);
			}
		}
		if (id < pglast) {
			// we have an extra overlapping byte to handle block length prefix. But don't use it if it is a page number
			// because it could be preceded by a block length prefix, instead zero it out (only use that byte if it we are on the last block)
			save2 = mop[start + 1];
			if (save2 > 0) {
				mop[start + 1] = 0;
			}
		}

		if (reserved_space < mop_len) {
			mop_len = reserved_space;
		}
		char do_write = env->me_freelist_written_start <= reserved_end && env->me_freelist_written_end >= start;
		if (fl_writes[i++] != do_write) {
			fprintf(stderr, "Do write of page does not match");
		}
		// if it is in the written range
		if (do_write) {
			// we are in part of the range of the freelist that was written, so we actually need to save it
			// if end == 0 it means that the length was probably truncated and we need to rewrite the length byte
			if (save_end <= 0 && len > 0) len--; // leave off the last byte if we are ignoring it
			mop[start] = len;
			data.mv_data = &mop[start];
			//mdb_midl_print(stderr, &mop[start]);
			rc = mdb_cursor_put(&mc, &key, &data, MDB_CURRENT);
			mop[start] = save;
		} else {// else we are just skipping this write
			//fprintf(stderr, "Skipping write of freelist %u from %u-%u", id, start, end);
		}
		if (save_end < 0) mop[end] = save_end;
		if (save2 > 0) mop[start + 1] = save2;
		if (rc)
			break;
	}

	// now check to verify the integrity of the free-space entries
	// first check the new free space entry
	if (rc == 0) {
		key.mv_size = sizeof(txn->mt_txnid);
		key.mv_data = &txn->mt_txnid;
		rc = mdb_cursor_get(&mc, &key, NULL, MDB_SET);
		if (rc == 0 && key.mv_size != sizeof(txn->mt_txnid)) {
			fprintf(stderr, "new freelist entry key wrong size %u\n", txn->mt_txnid);
			last_error = malloc(100);
			sprintf(last_error, "new freelist entry key wrong size %u\n", txn->mt_txnid);
			rc = MDB_BAD_TXN;
		}
		if (rc == MDB_NOTFOUND) rc = 0;
	}
	// now check the updated freelist entries
	if (rc == 0) {
		MDB_IDL test_idl = mdb_midl_alloc(16);
		key.mv_size = sizeof(start_written);
		key.mv_data = &start_written;
		rc = mdb_cursor_get(&mc, &key, &data, MDB_SET);
		size_t last = 0;
		while (rc == 0) {
			if (key.mv_size != sizeof(start_written)) {
				fprintf(stderr, "updated freelist key wrong size between %u and %u, last %u\n", start_written, env->me_freelist_written_end, last);
				last_error = malloc(100);
				sprintf(last_error, "updated freelist key wrong size between %u and %u, last %u\n", start_written, env->me_freelist_written_end, last);
				rc = MDB_BAD_TXN;
				break;
			}
			if (rc = mdb_midl_xmerge(&test_idl, (MDB_ID *) data.mv_data)) {
				last = *(txnid_t *) key.mv_data;
				fprintf(stderr, "freelist duplicates/overlaps in free list %u %u %u %u %u %u %i %i %i %i %i %u %u\n", last, data.mv_size, start_written,
						pglast, env->me_freelist_start, env->me_freelist_end,
						freelist_written_start, env->me_freelist_written_start, freelist_written_end, env->me_freelist_written_end,
						i, fl_writes[0], fl_writes[1]);
				fprintf(stderr, "conflicting freelist to save:\n");
				mdb_midl_print(stderr, test_idl);
				fprintf(stderr, "from the original freelist to save:\n");
				mdb_midl_print(stderr, pghead);
				fprintf(stderr, "All the entries that we are saving to the freelist:\n");
				// print out all the entries that we are saving to the freelist
				test_idl = mdb_midl_alloc(16);
				key.mv_size = sizeof(start_written);
				key.mv_data = &start_written;
				rc = mdb_cursor_get(&mc, &key, &data, MDB_SET);
				size_t last = 0;
				while (rc == 0) {
					last = *(txnid_t *) key.mv_data;
					fprintf(stderr, "entry %u:\n", last);
					mdb_midl_print(stderr, (MDB_ID *) data.mv_data);
					if (last >= env->me_freelist_end) break;
					rc = mdb_cursor_get(&mc, &key, &data, MDB_NEXT);
				}
				last_error = malloc(100);
				sprintf(last_error, "freelist entry %u had bad/duplicate entries, error code %u\n", last, rc);
				rc = MDB_BAD_TXN;
				break;
			}
			last = *(txnid_t *) key.mv_data;
			if (last >= env->me_freelist_end) break;
			rc = mdb_cursor_get(&mc, &key, &data, MDB_NEXT);
		}
		mdb_midl_free(test_idl);
		if (rc == MDB_NOTFOUND) rc = 0;
	}

	// reset the start and end to indicate no writes have taken place
	env->me_freelist_written_start = 0x7fffffff;
	env->me_freelist_written_end = 0;
	env->me_pghead = pghead;
	/* Restore this so we can check vs. dirty_list after mdb_page_flush() */
	if (! (txn->mt_flags & MDB_TXN_WRITEMAP))
		txn->mt_loose_count += lost_loose;

	return rc;
}

#if MDB_RPAGE_CACHE
static int mdb_rpage_decrypt(MDB_env *env, MDB_ID3 *id3, int rem, int numpgs);
static int mdb_page_encrypt(MDB_env *env, MDB_page *in, MDB_page *out, size_t size);
static int mdb_page_chk_checksum(MDB_env *env, MDB_page *mp, size_t size);
static void mdb_page_set_checksum(MDB_env *env, MDB_page *mp, size_t size);
#endif

/** Flush (some) dirty pages to the map, after clearing their dirty flag.
 * @param[in] txn the transaction that's being committed
 * @param[in] keep number of initial pages in dirty_list to keep dirty.
 * @return 0 on success, non-zero on failure.
 */
static int
mdb_page_flush(MDB_txn *txn, int keep)
{
	MDB_env		*env = txn->mt_env;
	MDB_ID2L	dl = txn->mt_u.dirty_list;
	unsigned	psize = env->me_psize, j;
	int			i, pagecount = dl[0].mid, rc, *dl_nump, nump = 1;
	size_t		size = 0;
	MDB_OFF_T	pos = 0;
	pgno_t		pgno = 0;
	MDB_page	*dp = NULL;
	uint64_t		start = 0;
	int			write_i = 0;
	if (env->me_flags & MDB_TRACK_METRICS) {
		start = get_time64();
	}
#ifdef _WIN32
	OVERLAPPED	*ov, *this_ov;
	MDB_page	*wdp;
	HANDLE fd = (txn->mt_flags & MDB_NOSYNC) ? env->me_fd : env->me_ovfd;
#else
	struct iovec iov[MDB_COMMIT_PAGES];
	HANDLE fd = env->me_fd;
#endif
	ssize_t		wsize = 0, wres;
	MDB_OFF_T	wpos = 0, next_pos = 1; /* impossible pos, so pos != next_pos */
	int			n = 0;

	j = i = keep;

	if (env->me_flags & MDB_WRITEMAP) {
		goto done;
	}
	int pages_written = 0;
	/* setup nump list, flag that it's in use */
	dl_nump = env->me_dirty_nump;
	/* <lmdb-js addition> */
	for (n=1; n<=pagecount; n++) {
		dp = dl[n].mptr;
		dl_nump[n] = IS_OVERFLOW(dp) ? dp->mp_pages : 1;
		if (dl_nump[n] < 0) {
			fprintf(stderr, "Invalid page count %i at index %i for page no %i\n", dl_nump[n], n, dl[n].mid);
			return MDB_BAD_TXN;
		}
		pgno_t p = dl[n].mid + dl_nump[n];
		if (p > pgno)
			pgno = p;
		else {
			fprintf(stderr, "Page writes are out of order\n");
			return MDB_BAD_TXN;
		}
	}
	txn->mt_flags |= MDB_TXN_DIRTYNUM;
	/* <lmdb-js addition> */
	n = 0;
	
#ifdef _WIN32
	if (!(env->me_flags & MDB_WRITEMAP) && pagecount > 0) {
		DWORD file_high;
		size_t file_size = GetFileSize(fd, &file_high);
		file_size += (size_t) file_high << 32;
		if (pgno * psize >= file_size) {
			file_size = ((size_t) (pgno < 100 ? 2 : pgno < 1000 ? 1.5 : pgno < 10000 ? 1.25 : pgno < 100000 ? 1.125 : 1.0625) * pgno * psize / 0x40000 + 1) * 0x40000;
			LONG high_position = file_size >> 32;
			if (SetFilePointer(fd, file_size & 0xffffffff, &high_position, FILE_BEGIN) == INVALID_SET_FILE_POINTER) {
				fprintf(stderr, "SetFilePointer failed: %s\n", strerror(ErrCode()));
			} else {
				rc = SetEndOfFile(fd);
				if (!rc) {
					rc = ErrCode();
					if (rc != 1224) // occassionaly see a ERROR_USER_MAPPED_FILE with no apparent ill-effect
						fprintf(stderr, "SetEndOfFile error, extending to %u %i %s\n", file_size, rc, strerror(rc));
				}
			}
		}
		if (!MDB_REMAPPING(env->me_flags)) {
			MDB_meta *m = mdb_env_pick_meta(env);
			void *p;
			p = (MDB_page *)(env->me_map + env->me_psize * m->mm_last_pg);
			if (pgno > m->mm_last_pg) {
				p = VirtualAlloc(p, env->me_psize * (pgno - m->mm_last_pg), MEM_COMMIT, PAGE_READONLY);
				if (!p) {
					fprintf(stderr, "VirtualAlloc failed\n");
					DPUTS("VirtualAlloc failed");
					return ErrCode();
				}
			}
		}
	}
	/* </lmdb-js addition> */
	if (pagecount - keep >= env->me_ovs) {
		/* ran out of room in ov array, and re-malloc, copy handles and free previous */
		int ovs = (pagecount - keep) * 1.5; /* provide extra padding to reduce number of re-allocations */
		int new_size = ovs * sizeof(OVERLAPPED);
		ov = malloc(new_size);
		if (ov == NULL)
			return ENOMEM;
		int previous_size = env->me_ovs * sizeof(OVERLAPPED);
		memcpy(ov, env->me_ov, previous_size); /* Copy previous OVERLAPPED data to retain event handles */
		/* And clear rest of memory */
		memset(&ov[env->me_ovs], 0, new_size - previous_size);
		if (env->me_ovs > 0) {
			free(env->me_ov); /* release previous allocation */
		}

		env->me_ov = ov;
		env->me_ovs = ovs;
	}
	ov = env->me_ov;
#endif

	/* Write the pages */
	for (;;) {
		if (++i <= pagecount) {
			dp = dl[i].mptr;
			/* Don't flush this page yet */
			if (dp->mp_flags & (P_LOOSE|P_KEEP)) {
				dp->mp_flags &= ~P_KEEP;
				dl[i].mid = 0;
				continue;
			}
			pgno = dl[i].mid;
			/* Mark the page as clean */
			dp->mp_txnid = txn->mt_txnid;
			pos = pgno * psize;
			size = psize;
			nump = dl_nump[i];
			mdb_tassert(txn, nump > 0);
			pages_written += nump;
			size *= nump;
		}
		/* Write up to MDB_COMMIT_PAGES dirty pages at a time. */
		if (pos!=next_pos || n==MDB_COMMIT_PAGES || wsize+size>MAX_WRITE
#ifdef _WIN32
			/* Windows actually supports scatter/gather I/O, but only on
			 * unbuffered file handles. Since we're relying on the OS page
			 * cache for all our data, that's self-defeating. So we just
			 * write pages one at a time. We use the ov structure to set
			 * the write offset, to at least save the overhead of a Seek
			 * system call.
			 * If writemap is enabled, consecutive page positions infer
			 * contiguous (mapped) memory.
			 * Otherwise force write pages one at a time.
			 */
			|| !(env->me_flags & MDB_WRITEMAP)
#endif
			) {
			if (n) {
retry_write:
				rc = 0;
				/* Write previous page(s) */
#ifdef _WIN32
				this_ov = &ov[write_i];
				/* Clear status, and keep hEvent, we reuse that */
				this_ov->Internal = 0;
				this_ov->Offset = wpos & 0xffffffff;
				this_ov->OffsetHigh = wpos >> 16 >> 16;
				if (!F_ISSET(txn->mt_flags, MDB_NOSYNC) && !this_ov->hEvent) {
					HANDLE event = CreateEvent(NULL, FALSE, FALSE, NULL);
					if (!event) {
						rc = ErrCode();
						DPRINTF(("CreateEvent: %s", strerror(rc)));
						return rc;
					}
					this_ov->hEvent = event;
				}
				if (!WriteFile(fd, wdp, wsize, NULL, this_ov)) {
					rc = ErrCode();
					if (rc != ERROR_IO_PENDING) {
						DPRINTF(("WriteFile: %d", rc));
						return rc;
					}
					rc = 0;
				}
#else /* _WIN32 */
#ifdef MDB_USE_PWRITEV
				wres = pwritev(fd, iov, n, wpos);
#else
				if (n == 1) {
					wres = pwrite(fd, iov[0].iov_base, wsize, wpos);
				} else {
retry_seek:
					if (lseek(fd, wpos, SEEK_SET) == -1) {
						rc = ErrCode();
						if (rc == EINTR)
							goto retry_seek;
						DPRINTF(("lseek: %s", strerror(rc)));
						wres = wsize;
					} else {
						rc = 0;
						wres = writev(fd, iov, n);
					}
				}
#endif
				if (wres != wsize) {
					if (wres < 0) {
						rc = ErrCode();
						if (rc == EINTR)
							goto retry_write;
						fprintf(stderr, "Write error: %s position %u, size %u", strerror(rc), wpos, wsize);
						last_error = malloc(100);
						sprintf(last_error, "Attempting to write page at position %u, size %u, blocks %u, buffer sizes %i %i %i", wpos, wsize, n, iov[0].iov_len, iov[1].iov_len, iov[2].iov_len);
					} else {
						rc = EIO; /* TODO: Use which error code? */
						DPUTS("short write, filesystem full?");
					}
				}
#endif /* _WIN32 */
				write_i++;
				if (rc)
					return rc;
				n = 0;
			}
			if (i > pagecount)
				break;
			wpos = pos;
			wsize = 0;
		}
#if MDB_RPAGE_CACHE
		if (env->me_sumfunc) {
			mdb_page_set_checksum(env, dp, size);
		}
		if (env->me_encfunc) {
			MDB_page *encp = mdb_page_malloc(txn, nump, 0);
			if (!encp)
				return ENOMEM;
			if (mdb_page_encrypt(env, dp, encp, size)) {
				mdb_dpage_free_n(env, encp, nump);
				return MDB_CRYPTO_FAIL;
			}
			mdb_dpage_free_n(env, dp, nump);
			dp = encp;
			dl[i].mptr = dp;
		}
#endif
#ifdef _WIN32
		wdp = dp;
#else
		iov[n].iov_len = size;
		iov[n].iov_base = (char *)dp;
#endif
		DPRINTF(("committing page %"Yu, pgno));
		next_pos = pos + size;
		wsize += size;
		n++;
	}
#if MDB_RPAGE_CACHE
	if (MDB_REMAPPING(env->me_flags) && pgno > txn->mt_last_pgno)
		txn->mt_last_pgno = pgno;
#endif

	/* MIPS has cache coherency issues, this is a no-op everywhere else
	 * Note: for any size >= on-chip cache size, entire on-chip cache is
	 * flushed.
	 */
	CACHEFLUSH(env->me_map, txn->mt_next_pgno * env->me_psize, DCACHE);
	if (env->me_flags & MDB_TRACK_METRICS) {
		env->me_metrics.writes += write_i;
		env->me_metrics.page_flushes++;
		env->me_metrics.pages_written += pages_written;
	}

#ifdef _WIN32
	if (!(txn->mt_flags & MDB_NOSYNC)) {
		/* Now wait for all the asynchronous/overlapped sync/write-through writes to complete.
		* We start with the last one so that all the others should already be complete and
		* we reduce thread suspend/resuming (in practice, typically about 99.5% of writes are
		* done after the last write is done) */
		rc = 0;
		while (--write_i >= 0) {
			if (ov[write_i].hEvent) {
				if (!GetOverlappedResult(fd, &ov[write_i], &wres, TRUE)) {
					rc = ErrCode(); /* Continue on so that all the event signals are reset */
				}
			}
		}
		if (rc) { /* any error on GetOverlappedResult, exit now */
			return rc;
		}
	}
#endif	/* _WIN32 */

		if (!(env->me_flags & MDB_WRITEMAP)) {
				/* Don't free pages when using writemap (can only get here in NOSYNC mode in Windows)
				 */
		for (i = keep; ++i <= pagecount; ) {
			dp = dl[i].mptr;
			/* This is a page we skipped above */
			if (!dl[i].mid) {
				dl[++j] = dl[i];
				dl[j].mid = dp->mp_pgno;
				continue;
			}
			mdb_dpage_free_n(env, dp, dl_nump[i]);
		}
	}
	txn->mt_flags ^= MDB_TXN_DIRTYNUM;

done:
	i--;
	txn->mt_dirty_room += i - j;
	dl[0].mid = j;
	if (env->me_flags & MDB_TRACK_METRICS) {
		env->me_metrics.time_page_flushes += get_time64() - start;
	}
	return MDB_SUCCESS;
}

static int ESECT mdb_env_share_locks(MDB_env *env, int *excl);

int
mdb_txn_commit(MDB_txn *txn)
{
	int		rc;
	unsigned int i, end_mode;
	MDB_env	*env;

	if (txn == NULL)
		return EINVAL;

	/* mdb_txn_end() mode for a commit which writes nothing */
	end_mode = MDB_END_EMPTY_COMMIT|MDB_END_UPDATE|MDB_END_SLOT|MDB_END_FREE;

	if (txn->mt_child) {
		rc = mdb_txn_commit(txn->mt_child);
		if (rc)
			goto fail;
	}

	env = txn->mt_env;

	if (F_ISSET(txn->mt_flags, MDB_TXN_RDONLY)) {
		goto done;
	}

	if (txn->mt_flags & (MDB_TXN_FINISHED|MDB_TXN_ERROR)) {
		fprintf(stderr, "txn has failed/finished, can't commit");
		if (txn->mt_parent)
			txn->mt_parent->mt_flags |= MDB_TXN_ERROR;
		rc = MDB_BAD_TXN;
		goto fail;
	}

	if (txn->mt_parent) {
		MDB_txn *parent = txn->mt_parent;
		MDB_page **lp;
		MDB_ID2L dst, src;
		MDB_IDL pspill;
		unsigned x, y, len, ps_len;

		/* Append our free list to parent's */
		rc = mdb_midl_append_list(&parent->mt_free_pgs, txn->mt_free_pgs);
		if (rc)
			goto fail;
		mdb_midl_free(txn->mt_free_pgs);
		/* Failures after this must either undo the changes
		 * to the parent or set MDB_TXN_ERROR in the parent.
		 */

		parent->mt_next_pgno = txn->mt_next_pgno;
		parent->mt_flags = txn->mt_flags;

		/* Merge our cursors into parent's and close them */
		mdb_cursors_close(txn, 1);

		/* Update parent's DB table. */
		memcpy(parent->mt_dbs, txn->mt_dbs, txn->mt_numdbs * sizeof(MDB_db));
		parent->mt_numdbs = txn->mt_numdbs;
		parent->mt_dbflags[FREE_DBI] = txn->mt_dbflags[FREE_DBI];
		parent->mt_dbflags[MAIN_DBI] = txn->mt_dbflags[MAIN_DBI];
		for (i=CORE_DBS; i<txn->mt_numdbs; i++) {
			/* preserve parent's DB_NEW status */
			x = parent->mt_dbflags[i] & DB_NEW;
			parent->mt_dbflags[i] = txn->mt_dbflags[i] | x;
		}

		dst = parent->mt_u.dirty_list;
		src = txn->mt_u.dirty_list;
		/* Remove anything in our dirty list from parent's spill list */
		if ((pspill = parent->mt_spill_pgs) && (ps_len = pspill[0])) {
			x = y = ps_len;
			pspill[0] = (pgno_t)-1;
			/* Mark our dirty pages as deleted in parent spill list */
			for (i=0, len=src[0].mid; ++i <= len; ) {
				MDB_ID pn = src[i].mid << 1;
				while (pn > pspill[x])
					x--;
				if (pn == pspill[x]) {
					pspill[x] = 1;
					y = --x;
				}
			}
			/* Squash deleted pagenums if we deleted any */
			for (x=y; ++x <= ps_len; )
				if (!(pspill[x] & 1))
					pspill[++y] = pspill[x];
			pspill[0] = y;
		}

		/* Remove anything in our spill list from parent's dirty list */
		if (txn->mt_spill_pgs && txn->mt_spill_pgs[0]) {
			for (i=1; i<=txn->mt_spill_pgs[0]; i++) {
				MDB_ID pn = txn->mt_spill_pgs[i];
				if (pn & 1)
					continue;	/* deleted spillpg */
				pn >>= 1;
				y = mdb_mid2l_search(dst, pn);
				if (y <= dst[0].mid && dst[y].mid == pn) {
					free(dst[y].mptr);
					while (y < dst[0].mid) {
						dst[y] = dst[y+1];
						y++;
					}
					dst[0].mid--;
				}
			}
		}

		/* Find len = length of merging our dirty list with parent's */
		x = dst[0].mid;
		dst[0].mid = 0;		/* simplify loops */
		if (parent->mt_parent) {
			len = x + src[0].mid;
			y = mdb_mid2l_search(src, dst[x].mid + 1) - 1;
			for (i = x; y && i; y--) {
				pgno_t yp = src[y].mid;
				while (yp < dst[i].mid)
					i--;
				if (yp == dst[i].mid) {
					i--;
					len--;
				}
			}
		} else { /* Simplify the above for single-ancestor case */
			len = MDB_IDL_UM_MAX - txn->mt_dirty_room;
		}
		/* Merge our dirty list with parent's */
		y = src[0].mid;
		for (i = len; y; dst[i--] = src[y--]) {
			pgno_t yp = src[y].mid;
			while (yp < dst[x].mid)
				dst[i--] = dst[x--];
			if (yp == dst[x].mid)
				free(dst[x--].mptr);
		}
		mdb_tassert(txn, i == x);
		dst[0].mid = len;
		free(txn->mt_u.dirty_list);
		parent->mt_dirty_room = txn->mt_dirty_room;
		if (txn->mt_spill_pgs) {
			if (parent->mt_spill_pgs) {
				/* TODO: Prevent failure here, so parent does not fail */
				rc = mdb_midl_append_list(&parent->mt_spill_pgs, txn->mt_spill_pgs);
				if (rc) {
					fprintf(stderr, "failed to append spill list\n");
					parent->mt_flags |= MDB_TXN_ERROR;
				}
				mdb_midl_free(txn->mt_spill_pgs);
				mdb_midl_sort(parent->mt_spill_pgs);
			} else {
				parent->mt_spill_pgs = txn->mt_spill_pgs;
			}
		}

		/* Append our loose page list to parent's */
		for (lp = &parent->mt_loose_pgs; *lp; lp = &NEXT_LOOSE_PAGE(*lp))
			;
		*lp = txn->mt_loose_pgs;
		parent->mt_loose_count += txn->mt_loose_count;

		parent->mt_last_workid = txn->mt_last_workid;
		parent->mt_child = NULL;
		mdb_midl_free(((MDB_ntxn *)txn)->mnt_pgstate.mf_pghead);
		free(txn);
		return rc;
	}

	if (txn != env->me_txn) {
		DPUTS("attempt to commit unknown transaction");
		rc = EINVAL;
		goto fail;
	}

	mdb_cursors_close(txn, 0);

	if (!txn->mt_u.dirty_list[0].mid &&
		!(txn->mt_flags & (MDB_TXN_DIRTY | MDB_TXN_SPILLS))) {
		txn->mt_txnid--; // we can safely decrement this because should be no changes to freelist
		mdb_txn_end(txn, end_mode);
		return MDB_EMPTY_TXN;
	}

	DPRINTF(("committing txn %"Yu" %p on mdbenv %p, root page %"Yu,
			txn->mt_txnid, (void*)txn, (void*)env, txn->mt_dbs[MAIN_DBI].md_root));

	/* Update DB root pointers */
	if (txn->mt_numdbs > CORE_DBS) {
		MDB_cursor mc;
		MDB_dbi i;
		MDB_val data;
		data.mv_size = sizeof(MDB_db);

		mdb_cursor_init(&mc, txn, MAIN_DBI, NULL);
		for (i = CORE_DBS; i < txn->mt_numdbs; i++) {
			if (txn->mt_dbflags[i] & DB_DIRTY) {
				if (TXN_DBI_CHANGED(txn, i)) {
					rc = MDB_BAD_DBI;
					goto fail;
				}
				data.mv_data = &txn->mt_dbs[i];
				rc = mdb_cursor_put(&mc, &txn->mt_dbxs[i].md_name, &data,
					F_SUBDATA);
				if (rc)
					goto fail;
			}
		}
	}

	rc = mdb_freelist_save(txn);
	if (rc)
		goto fail;

	mdb_midl_shrink(&txn->mt_free_pgs);

#if (MDB_DEBUG) > 2
	mdb_audit(txn);
#endif
	int dirty_pages = txn->mt_u.dirty_list[0].mid;
	if ((rc = mdb_page_flush(txn, 0)))
		goto fail;
	if ((unsigned)txn->mt_loose_count < txn->mt_u.dirty_list[0].mid) {
		last_error = malloc(100);
		sprintf(last_error, "The loose count %i is less than the size of the dirty list %u", txn->mt_loose_count, txn->mt_u.dirty_list[0].mid);
		rc = MDB_PROBLEM; /* mt_loose_pgs does not match dirty_list */
		goto fail;
	}
	if (!F_ISSET(txn->mt_flags, MDB_TXN_NOSYNC) &&
		(rc = mdb_env_sync0(env, 0, txn->mt_next_pgno)))
		goto fail;

	//<lmdb-js>
	if ((txn->mt_flags & MDB_NOSYNC) && (env->me_flags & MDB_OVERLAPPINGSYNC))
		txn->mt_dbs[FREE_DBI].md_flags |= MDB_OVERLAPPINGSYNC;
	else
		txn->mt_dbs[FREE_DBI].md_flags &= ~MDB_OVERLAPPINGSYNC;
	//<//lmdb-js>

	if ((rc = mdb_env_write_meta(txn)))
		goto fail;

	//<lmdb-js>
	MDB_txn_visible* callback = txn->mt_callback;
	void* ctx = txn->mt_ctx;
	if (callback) {
		callback(ctx, 0);
		txn->mt_callback = NULL;
	}
	if (!F_ISSET(txn->mt_flags, MDB_TXN_NOSYNC))
		env->me_synced_txn_id = txn->mt_txnid;
	//</lmdb-js>
	end_mode = MDB_END_COMMITTED|MDB_END_UPDATE;
	if (env->me_flags & MDB_PREVSNAPSHOT) {
		if (!(env->me_flags & MDB_NOLOCK)) {
			int excl;
			rc = mdb_env_share_locks(env, &excl);
			if (rc)
				goto fail;
		}
		env->me_flags ^= MDB_PREVSNAPSHOT;
	}

done:
	//<lmdb-js>
	if (env->me_flags & MDB_TRACK_METRICS) {
		env->me_metrics.time_during_txns += get_time64() - env->me_metrics.clock_txn;
		env->me_metrics.txns++;
	}
	if ((txn->mt_flags & MDB_NOSYNC) && (env->me_flags & MDB_OVERLAPPINGSYNC)) {
		size_t txn_id = txn->mt_txnid;
		if (dirty_pages * (txn->mt_txnid - env->me_synced_txn_id) > 500) {
			// for bigger txns we wait for the flush before allowing next txn
			LOCK_MUTEX(rc, env, env->me_sync_mutex);
			mdb_txn_end(txn, end_mode);
		} else {
			mdb_txn_end(txn, end_mode);
			LOCK_MUTEX(rc, env, env->me_sync_mutex);
		}
		if (callback) {
			callback(ctx, 1);
		}
		if (rc)
			return rc;
		if (env->me_synced_txn_id < txn_id) { // check to see if we still need a sync
			uint64_t start = 0;
			if (env->me_flags & MDB_TRACK_METRICS) {
				start = get_time64();
			}
			MDB_meta* m;
			MDB_txninfo *ti = env->me_txns;
			MDB_txn sync_txn;
			MDB_db dbs[2];
			// if other transactions have completed by the time we get here, our sync txn
			// should include that
			do {
				m = mdb_env_pick_meta(env);
				sync_txn.mt_env = env;
				sync_txn.mt_flags = 2;
				sync_txn.mt_dbs = dbs;
				sync_txn.mt_dbs[FREE_DBI] = m->mm_dbs[FREE_DBI];
				sync_txn.mt_dbs[MAIN_DBI] = m->mm_dbs[MAIN_DBI];
				sync_txn.mt_dbs[FREE_DBI].md_flags &= ~MDB_OVERLAPPINGSYNC; // clear this to indicate it is flushed txn
				sync_txn.mt_txnid = txn_id = m->mm_txnid;
				sync_txn.mt_next_pgno = m->mm_last_pg + 1;
			} while(ti->mti_txnid != txn_id); // avoid race condition in copying data by verifying that this is updated

			rc = mdb_env_sync0(env, 0, m->mm_last_pg);
			if (rc) {
				UNLOCK_MUTEX(env->me_sync_mutex);
				return rc;
			}
			rc = mdb_env_write_meta(&sync_txn);
			if (rc == 0)
				env->me_synced_txn_id = sync_txn.mt_txnid;
			if (env->me_flags & MDB_TRACK_METRICS) {
				env->me_metrics.time_sync += get_time64() - start;
			}
			UNLOCK_MUTEX(env->me_sync_mutex);
		} else {
			UNLOCK_MUTEX(env->me_sync_mutex);
//			fprintf(stderr, "No sync needed after getting lock, committed %u, but already at %u\n", txn_id, env->me_synced_txn_id);
		}
	} else
		mdb_txn_end(txn, end_mode);
	return MDB_SUCCESS;

fail:
	mdb_txn_abort(txn);
	return rc;
}

MDB_meta* mdb_pick_meta(const MDB_env *env, MDB_meta* a, MDB_meta* b) {
	MDB_meta *latest = a->mm_txnid >= b->mm_txnid ? a : b;
	if (env->me_flags & MDB_PREVSNAPSHOT) {
		if (env->me_flags & MDB_OVERLAPPINGSYNC) {
			if (!b->mm_txnid)
				return a;
			if ((latest->boot_id && latest->boot_id == env->boot_id && !(env->me_flags & MDB_SAFE_RESTORE)) ||
					!(latest->mm_flags & MDB_OVERLAPPINGSYNC))
				return latest;
		}
		return a->mm_txnid > b->mm_txnid ? b : a; // previous snapshot, use oldest
	}
	return latest;
}

static int ESECT mdb_env_map(MDB_env *env, void *addr);

/** Read the environment parameters of a DB environment before
 * mapping it into memory.
 * @param[in] env the environment handle
 * @param[in] prev whether to read the backup meta page
 * @param[out] meta address of where to store the meta information
 * @return 0 on success, non-zero on failure.
 */
static int ESECT
mdb_env_read_header(MDB_env *env, int prev, MDB_meta *meta)
{
	MDB_metabuf	pbuf;
	MDB_page	*p;
	MDB_meta	*m;
	int			i, rc, off;
	enum { Size = sizeof(pbuf) };

	if (env->me_flags & MDB_RAWPART) {
#define VM_ALIGN	0x200000
		env->me_mapsize += VM_ALIGN-1;
		env->me_mapsize &= ~(VM_ALIGN-1);
		env->me_psize = env->me_os_psize;
		rc = mdb_env_map(env, NULL);
		if (rc)
			return rc;
		p = (MDB_page *)env->me_map;
		for (i=0; i<NUM_METAS; i++) {
			if (!F_ISSET(p->mp_flags, P_META))
				return ENOENT;
			if (env->me_metas[i]->mm_magic != MDB_MAGIC)
				return MDB_INVALID;
			if ((env->me_metas[i]->mm_version & 0xffff) != MDB_DATA_VERSION)
				return MDB_VERSION_MISMATCH;
			if (i == 0)
				*meta = *env->me_metas[i];
			else
				*meta = *mdb_pick_meta(env, meta, env->me_metas[i]);
			p = (MDB_page *)((char *)p + env->me_psize);
		}
		return 0;
	}

	/* We don't know the page size yet, so use a minimum value.
	 * Read both meta pages so we can use the latest one.
	 */
	int num_metas = env->me_flags & MDB_OVERLAPPINGSYNC ? 3 : NUM_METAS;

	for (i=off=0; i<num_metas; i++,
			off += env->me_flags & MDB_OVERLAPPINGSYNC ? meta->mm_psize >> 1 : meta->mm_psize) {
#ifdef _WIN32
		DWORD len;
		OVERLAPPED ov;
		memset(&ov, 0, sizeof(ov));
		ov.Offset = off;
		rc = ReadFile(env->me_fd, &pbuf, Size, &len, &ov) ? (int)len : -1;
		if (rc == -1 && ErrCode() == ERROR_HANDLE_EOF)
			rc = 0;
#else
		rc = pread(env->me_fd, &pbuf, Size, off);
#endif
		if (rc != Size) {
			if (rc == 0 && off == 0)
				return ENOENT;
			rc = rc < 0 ? (int) ErrCode() : MDB_INVALID;
			DPRINTF(("read: %s", mdb_strerror(rc)));
			return rc;
		}

		p = (MDB_page *)&pbuf;
		if (off == 0) {
			if (!F_ISSET(p->mp_flags, P_META)) {
				if (env->me_flags & MDB_RAWPART)
					return ENOENT;
				DPRINTF(("page %"Yu" not a meta page", p->mp_pgno));
				return MDB_INVALID;
			}

			m = METADATA(p);
			if (m->mm_magic != MDB_MAGIC) {
				DPUTS("meta has invalid magic");
				return MDB_INVALID;
			}

			if ((m->mm_version & 0xffff) != MDB_DATA_VERSION) {
				DPRINTF(("database is version %u, expected version %u",
					(m->mm_version & 0xffff), MDB_DATA_VERSION));
				return MDB_VERSION_MISMATCH;
			}
			*meta = *m;
		} else {
			m = METADATA(p);
			*meta = *mdb_pick_meta(env, meta, m);
		}
		// </lmdb-js>
	}
	return 0;
}

/** Fill in most of the zeroed #MDB_meta for an empty database environment */
static void ESECT
mdb_env_init_meta0(MDB_env *env, MDB_meta *meta)
{
	meta->mm_magic = MDB_MAGIC;
	meta->mm_version = MDB_DATA_VERSION;
	meta->mm_mapsize = env->me_mapsize;
	meta->mm_psize = env->me_psize;
	meta->mm_last_pg = NUM_METAS-1;
	meta->mm_flags = env->me_flags & 0xffff & ~MDB_OVERLAPPINGSYNC;
	meta->mm_flags |= MDB_INTEGERKEY; /* this is mm_dbs[FREE_DBI].md_flags */
	meta->mm_dbs[FREE_DBI].md_root = P_INVALID;
	meta->mm_dbs[MAIN_DBI].md_root = P_INVALID;
}

/** Write the environment parameters of a freshly created DB environment.
 * @param[in] env the environment handle
 * @param[in] meta the #MDB_meta to write
 * @return 0 on success, non-zero on failure.
 */
static int ESECT
mdb_env_init_meta(MDB_env *env, MDB_meta *meta)
{
	MDB_page *p, *q;
	int rc;
	unsigned int	 psize;
#ifdef _WIN32
	DWORD len;
	OVERLAPPED ov;
	memset(&ov, 0, sizeof(ov));
#define DO_PWRITE(rc, fd, ptr, size, len, pos)	do { \
	ov.Offset = pos;	\
	rc = WriteFile(fd, ptr, size, &len, &ov);	} while(0)
#else
	int len;
#define DO_PWRITE(rc, fd, ptr, size, len, pos)	do { \
	len = pwrite(fd, ptr, size, pos);	\
	if (len == -1 && ErrCode() == EINTR) continue; \
	rc = (len >= 0); break; } while(1)
#endif
	DPUTS("writing new meta page");

	psize = env->me_psize;

	if ((env->me_flags & (MDB_RAWPART|MDB_WRITEMAP)) == (MDB_RAWPART|MDB_WRITEMAP)) {
		p = (MDB_page *)env->me_map;
		p->mp_pgno = 0;
		p->mp_flags = P_META;
		*(MDB_meta *)METADATA(p) = *meta;
		q = (MDB_page *)((char *)p + psize);
		q->mp_pgno = 1;
		q->mp_flags = P_META;
		*(MDB_meta *)METADATA(q) = *meta;
		return 0;
	}

	p = calloc(NUM_METAS, psize);
	if (!p)
		return ENOMEM;
	p->mp_pgno = 0;
	p->mp_flags = P_META;
	*(MDB_meta *)METADATA(p) = *meta;

	q = (MDB_page *)((char *)p + psize);
	q->mp_pgno = 1;
	q->mp_flags = P_META;
	*(MDB_meta *)METADATA(q) = *meta;

#if MDB_RPAGE_CACHE
	if (env->me_sumsize) {
		/* save the checksum size in tail of page 0 */
		char *ptr = (char *)q;
		unsigned short *u = (unsigned short *)(ptr-2);
		*u = env->me_sumsize;
	}
#endif
	DO_PWRITE(rc, env->me_fd, p, psize * NUM_METAS, len, 0);
	if (!rc)
		rc = ErrCode();
	else if ((unsigned) len == psize * NUM_METAS)
		rc = MDB_SUCCESS;
	else
		rc = ENOSPC;
	free(p);
	return rc;
}

/** Update the environment info to commit a transaction.
 * @param[in] txn the transaction that's being committed
 * @return 0 on success, non-zero on failure.
 */
static int
mdb_env_write_meta(MDB_txn *txn)
{
	MDB_env *env;
	MDB_meta	meta, metab, *mp;
	unsigned flags;
	mdb_size_t mapsize;
	MDB_OFF_T off;
	int rc, len, toggle;
	char *ptr;
	HANDLE mfd;
#ifdef _WIN32
	OVERLAPPED ov;
#else
	int r2;
#endif

	toggle = txn->mt_txnid & 1;
	DPRINTF(("writing meta page %d for root page %"Yu,
		toggle, txn->mt_dbs[MAIN_DBI].md_root));

	env = txn->mt_env;
	flags = txn->mt_flags | env->me_flags;
	mp = env->me_metas[toggle];
	mapsize = env->me_metas[toggle ^ 1]->mm_mapsize;
	/* Persist any increases of mapsize config */
	if (mapsize < env->me_mapsize)
		mapsize = env->me_mapsize;

#ifndef _WIN32 /* We don't want to ever use MSYNC/FlushViewOfFile in Windows */
	if (flags & MDB_WRITEMAP) {
		mp->mm_mapsize = mapsize;
		mp->mm_dbs[FREE_DBI] = txn->mt_dbs[FREE_DBI];
		mp->mm_dbs[MAIN_DBI] = txn->mt_dbs[MAIN_DBI];
		mp->mm_last_pg = txn->mt_next_pgno - 1;
#if (__GNUC__ * 100 + __GNUC_MINOR__ >= 404) && /* TODO: portability */	\
	!(defined(__i386__) || defined(__x86_64__))
		/* LY: issue a memory barrier, if not x86. ITS#7969 */
		__sync_synchronize();
#endif
		mp->mm_txnid = txn->mt_txnid;
		mp->boot_id = env->boot_id;
		if (!(flags & (MDB_NOMETASYNC|MDB_NOSYNC))) {
			unsigned meta_size = env->me_psize;
			rc = (env->me_flags & MDB_MAPASYNC) ? MS_ASYNC : MS_SYNC;
			ptr = (char *)mp - PAGEHDRSZ;
			/* POSIX msync() requires ptr = start of OS page */
			r2 = (ptr - env->me_map) & (env->me_os_psize - 1);
			ptr -= r2;
			meta_size += r2;
			if (MDB_MSYNC(ptr, meta_size, rc)) {
				rc = ErrCode();
				goto fail;
			}
		}
		goto done;
	}
#endif
	metab.mm_txnid = mp->mm_txnid;
	metab.mm_last_pg = mp->mm_last_pg;

	meta.mm_mapsize = mapsize;
	meta.mm_dbs[FREE_DBI] = txn->mt_dbs[FREE_DBI];
	meta.mm_dbs[MAIN_DBI] = txn->mt_dbs[MAIN_DBI];
	meta.mm_last_pg = txn->mt_next_pgno - 1;
	meta.mm_txnid = txn->mt_txnid;
	meta.mm_version = MDB_DATA_VERSION;
	meta.boot_id = env->boot_id;

	off = offsetof(MDB_meta, mm_mapsize);
	ptr = (char *)&meta + off;
	len = sizeof(MDB_meta) - off;
	if (flags & 2) { // write to the sync meta, half page
		off += PAGEHDRSZ + (env->me_psize >> 1);
	} else
		off += (char *)mp - env->me_map;

	/* Write to the SYNC fd unless MDB_NOSYNC/MDB_NOMETASYNC.
	 * (me_mfd goes to the same file as me_fd, but writing to it
	 * also syncs to disk.	Avoids a separate fdatasync() call.)
	 */
	mfd = (flags & (MDB_NOSYNC|MDB_NOMETASYNC)) ? env->me_fd : env->me_mfd;
#ifdef _WIN32
	{
		memset(&ov, 0, sizeof(ov));
		ov.Offset = off;
		if (!WriteFile(mfd, ptr, len, (DWORD *)&rc, &ov))
			rc = -1;
	}
#else
retry_write:
	rc = pwrite(mfd, ptr, len, off);
#endif
	if (rc != len) {
		rc = rc < 0 ? ErrCode() : EIO;
#ifndef _WIN32
		if (rc == EINTR)
			goto retry_write;
#endif
		DPUTS("write failed, disk error?");
		/* On a failure, the pagecache still contains the new data.
		 * Write some old data back, to prevent it from being used.
		 * Use the non-SYNC fd; we know it will fail anyway.
		 */
		meta.mm_last_pg = metab.mm_last_pg;
		meta.mm_txnid = metab.mm_txnid;
#ifdef _WIN32
		memset(&ov, 0, sizeof(ov));
		ov.Offset = off;
		WriteFile(env->me_fd, ptr, len, NULL, &ov);
#else
		r2 = pwrite(env->me_fd, ptr, len, off);
		(void)r2;	/* Silence warnings. We don't care about pwrite's return value */
#endif
fail:
		env->me_flags |= MDB_FATAL_ERROR;
		return rc;
	}
	/* MIPS has cache coherency issues, this is a no-op everywhere else */
	CACHEFLUSH(env->me_map + off, len, DCACHE);
done:
	/* Memory ordering issues are irrelevant; since the entire writer
	 * is wrapped by wmutex, all of these changes will become visible
	 * after the wmutex is unlocked. Since the DB is multi-version,
	 * readers will get consistent data regardless of how fresh or
	 * how stale their view of these values is.
	 */
	if (env->me_txns && !(flags & 2))
		env->me_txns->mti_txnid = txn->mt_txnid;

	return MDB_SUCCESS;
}

/** Check both meta pages to see which one is newer.
 * @param[in] env the environment handle
 * @return newest #MDB_meta.
 */
static MDB_meta *
mdb_env_pick_meta(const MDB_env *env)
{
	MDB_meta *const *metas = env->me_metas;
	//<lmdb-js>
	MDB_meta *latest = mdb_pick_meta(env, metas[0], metas[1]);
	if (env->me_flags & MDB_PREVSNAPSHOT && env->me_flags & MDB_OVERLAPPINGSYNC) {
		int offset = env->me_psize >> 1;
		MDB_meta *flushed = ((MDB_meta*) (((char*)metas[0]) + offset));
		latest = mdb_pick_meta(env, latest, flushed);
	}
	//</lmdb-js>
	return latest;
}

int ESECT
mdb_env_create(MDB_env **env)
{
	MDB_env *e;

	e = calloc(1, sizeof(MDB_env));
	if (!e)
		return ENOMEM;

	e->me_maxreaders = DEFAULT_READERS;
	e->me_maxdbs = e->me_numdbs = CORE_DBS;
	e->me_maxfreepgs_to_load = 50000;
	e->me_maxfreepgs_to_retain = 75000;
	e->me_fd = INVALID_HANDLE_VALUE;
	e->me_lfd = INVALID_HANDLE_VALUE;
	e->me_mfd = INVALID_HANDLE_VALUE;
#ifdef MDB_USE_POSIX_SEM
	e->me_rmutex = SEM_FAILED;
	e->me_wmutex = SEM_FAILED;
	e->me_sync_mutex = SEM_FAILED;
#elif defined MDB_USE_SYSV_SEM
	e->me_rmutex->semid = -1;
	e->me_wmutex->semid = -1;
	e->me_sync_mutex->semid = -1;
#endif
	e->me_pid = getpid();
	GET_PAGESIZE(e->me_os_psize);
	VGMEMP_CREATE(e,0,0);
	*env = e;
	return MDB_SUCCESS;
}

#ifdef _WIN32
/** @brief Map a result from an NTAPI call to WIN32. */
static DWORD
mdb_nt2win32(NTSTATUS st)
{
	OVERLAPPED o = {0};
	DWORD br;
	o.Internal = st;
	GetOverlappedResult(NULL, &o, &br, FALSE);
	return GetLastError();
}
#endif

static int ESECT
mdb_env_map(MDB_env *env, void *addr)
{
	MDB_page *p;
	unsigned int flags = env->me_flags;
#ifdef _WIN32
	int rc;
	int access = SECTION_MAP_READ;
	HANDLE mh;
	void *map;
	SIZE_T msize;
	ULONG pageprot = PAGE_READONLY, secprot, alloctype;

	if (flags & MDB_WRITEMAP) {
		access |= SECTION_MAP_WRITE;
		pageprot = PAGE_READWRITE;
	}
	if (flags & MDB_RDONLY) {
		secprot = PAGE_READONLY;
		msize = 0;
		alloctype = 0;
	} else {
		secprot = PAGE_READWRITE;
		msize = env->me_mapsize;
		alloctype = MEM_RESERVE;
	}

	/** Some users are afraid of seeing their disk space getting used
	 * all at once, so the default is now to do incremental file growth.
	 * But that has a large performance impact, so give the option of
	 * allocating the file up front.
	 */
#ifdef MDB_FIXEDSIZE
	LARGE_INTEGER fsize;
	fsize.LowPart = msize & 0xffffffff;
	fsize.HighPart = msize >> 16 >> 16;
	rc = NtCreateSection(&mh, access, NULL, &fsize, secprot, SEC_RESERVE, env->me_fd);
#else
	rc = NtCreateSection(&mh, access, NULL, NULL, secprot, SEC_RESERVE, env->me_fd);
#endif
	if (rc)
		return mdb_nt2win32(rc);
	map = addr;
	if (MDB_REMAPPING(env->me_flags))
		msize = NUM_METAS * env->me_psize;
	rc = NtMapViewOfSection(mh, GetCurrentProcess(), &map, 0, 0, NULL, &msize, ViewUnmap, alloctype, pageprot);
#if MDB_RPAGE_CACHE
	if (MDB_REMAPPING(env->me_flags))
		env->me_fmh = mh;
	else
#endif
		NtClose(mh);
	if (rc)
		return mdb_nt2win32(rc);
	env->me_map = map;
#else	/* !_WIN32 */
	int mmap_flags = MAP_SHARED;
	int prot = PROT_READ;
	if (flags & MDB_WRITEMAP)
		prot |= PROT_WRITE;
#ifdef MAP_NOSYNC	/* Used on FreeBSD */
	if (flags & MDB_NOSYNC)
		mmap_flags |= MAP_NOSYNC;
#endif
	if (MDB_REMAPPING(env->me_flags)) {
		(void) flags;
		env->me_map = mmap(addr, NUM_METAS * env->me_psize, prot, mmap_flags,
			env->me_fd, 0);
		if (env->me_map == MAP_FAILED) {
			env->me_map = NULL;
			return ErrCode();
		}
	} else
	{
	if (flags & MDB_WRITEMAP) {
		if (!(flags & MDB_RAWPART) && ftruncate(env->me_fd, env->me_mapsize) < 0)
			return ErrCode();
	}
	env->me_map = mmap(addr, env->me_mapsize, prot, mmap_flags,
		env->me_fd, 0);
	if (env->me_map == MAP_FAILED) {
		env->me_map = NULL;
		return ErrCode();
	}

	if (flags & MDB_NORDAHEAD) {
		/* Turn off readahead. It's harmful when the DB is larger than RAM. */
#ifdef MADV_RANDOM
		madvise(env->me_map, env->me_mapsize, MADV_RANDOM);
#else
#ifdef POSIX_MADV_RANDOM
		posix_madvise(env->me_map, env->me_mapsize, POSIX_MADV_RANDOM);
#endif /* POSIX_MADV_RANDOM */
#endif /* MADV_RANDOM */
	}
	}
#endif /* _WIN32 */

	/* Can happen because the address argument to mmap() is just a
	 * hint.	mmap() can pick another, e.g. if the range is in use.
	 * The MAP_FIXED flag would prevent that, but then mmap could
	 * instead unmap existing pages to make room for the new map.
	 */
	if (addr && env->me_map != addr)
		return EBUSY;	/* TODO: Make a new MDB_* error code? */

	p = (MDB_page *)env->me_map;
	env->me_metas[0] = METADATA(p);
	env->me_metas[1] = (MDB_meta *)((char *)env->me_metas[0] + env->me_psize);

	return MDB_SUCCESS;
}

int ESECT
mdb_env_set_mapsize(MDB_env *env, mdb_size_t size)
{
	/* If env is already open, caller is responsible for making
	 * sure there are no active txns.
	 */
	if (env->me_map) {
		MDB_meta *meta;
		void *old;
		int rc;


	/* <lmdb-js removal> 
		if (env->me_txn) We are intentionally resizing during transactions now
			return EINVAL;
		</lmdb-js removal> */

		meta = mdb_env_pick_meta(env);
		if (!size)
			size = meta->mm_mapsize;
		{
			/* Silently round up to minimum if the size is too small */
			mdb_size_t minsize = (meta->mm_last_pg + 1) * env->me_psize;
			if (size < minsize)
				size = minsize;
		}
		if (!(MDB_REMAPPING(env->me_flags)))
		{
		/* For MDB_REMAP_CHUNKS this bit is a noop since we dynamically remap
		 * chunks of the DB anyway.
		 */
	/* <lmdb-js removal>	We don't unmap right now because we intentionally want to leave old maps around for lingering read transactions and other threads that haven't resized yet */
		MDB_last_map* last_map = malloc(sizeof(MDB_last_map));
		last_map->last_map = env->me_last_map;
		last_map->map = env->me_map;
		last_map->mapsize = env->me_mapsize;
		env->me_last_map = last_map;
		//munmap(env->me_map, env->me_mapsize);
	 	/*</lmdb-js removal> */
		env->me_mapsize = size;
		old = (env->me_flags & MDB_FIXEDMAP) ? env->me_map : NULL;
		rc = mdb_env_map(env, old);
		if (rc)
			return rc;
		}
	}
	env->me_mapsize = size;
	if (env->me_psize)
		env->me_maxpg = env->me_mapsize / env->me_psize;
	return MDB_SUCCESS;
}

int ESECT
mdb_env_set_maxdbs(MDB_env *env, MDB_dbi dbs)
{
	if (env->me_map)
		return EINVAL;
	env->me_maxdbs = dbs + CORE_DBS;
	return MDB_SUCCESS;
}

int ESECT
mdb_env_set_maxreaders(MDB_env *env, unsigned int readers)
{
	if (env->me_map || readers < 1)
		return EINVAL;
	env->me_maxreaders = readers;
	return MDB_SUCCESS;
}

int ESECT
mdb_env_get_maxreaders(MDB_env *env, unsigned int *readers)
{
	if (!env || !readers)
		return EINVAL;
	*readers = env->me_maxreaders;
	return MDB_SUCCESS;
}

static int ESECT
mdb_fsize(HANDLE fd, mdb_size_t *size)
{
#ifdef _WIN32
	LARGE_INTEGER fsize;

	if (!GetFileSizeEx(fd, &fsize))
		return ErrCode();

	*size = fsize.QuadPart;
#else
	struct stat st;

	if (fstat(fd, &st))
		return ErrCode();

	*size = st.st_size;
#endif
	return MDB_SUCCESS;
}


#ifdef _WIN32
typedef wchar_t	mdb_nchar_t;
# define MDB_NAME(str)	L##str
# define mdb_name_cpy	wcscpy
#else
/** Character type for file names: char on Unix, wchar_t on Windows */
typedef char	mdb_nchar_t;
# define MDB_NAME(str)	str		/**< #mdb_nchar_t[] string literal */
# define mdb_name_cpy	strcpy	/**< Copy name (#mdb_nchar_t string) */
#endif

/** Filename - string of #mdb_nchar_t[] */
typedef struct MDB_name {
	int mn_len;					/**< Length	*/
	int mn_alloced;				/**< True if #mn_val was malloced */
	mdb_nchar_t	*mn_val;		/**< Contents */
} MDB_name;

/** Filename suffixes [datafile,lockfile][without,with MDB_NOSUBDIR] */
static const mdb_nchar_t *const mdb_suffixes[2][2] = {
	{ MDB_NAME("/data.mdb"), MDB_NAME("")			},
	{ MDB_NAME("/lock.mdb"), MDB_NAME("-lock") }
};

#define MDB_SUFFLEN 9	/**< Max string length in #mdb_suffixes[] */

/** Set up filename + scratch area for filename suffix, for opening files.
 * It should be freed with #mdb_fname_destroy().
 * On Windows, paths are converted from char *UTF-8 to wchar_t *UTF-16.
 *
 * @param[in] path Pathname for #mdb_env_open().
 * @param[in] envflags Whether a subdir and/or lockfile will be used.
 * @param[out] fname Resulting filename, with room for a suffix if necessary.
 */
static int ESECT
mdb_fname_init(const char *path, unsigned envflags, MDB_name *fname)
{
	int no_suffix = F_ISSET(envflags, MDB_NOSUBDIR|MDB_NOLOCK);
	fname->mn_alloced = 0;
#ifdef _WIN32
	return utf8_to_utf16(path, fname, no_suffix ? 0 : MDB_SUFFLEN);
#else
	fname->mn_len = strlen(path);
	if (no_suffix)
		fname->mn_val = (char *) path;
	else if ((fname->mn_val = malloc(fname->mn_len + MDB_SUFFLEN+1)) != NULL) {
		fname->mn_alloced = 1;
		strcpy(fname->mn_val, path);
	}
	else
		return ENOMEM;
	return MDB_SUCCESS;
#endif
}

/** Destroy \b fname from #mdb_fname_init() */
#define mdb_fname_destroy(fname) \
	do { if ((fname).mn_alloced) free((fname).mn_val); } while (0)

#ifdef O_CLOEXEC /* POSIX.1-2008: Set FD_CLOEXEC atomically at open() */
# define MDB_CLOEXEC		O_CLOEXEC
#else
# define MDB_CLOEXEC		0
#endif

/** File type, access mode etc. for #mdb_fopen() */
enum mdb_fopen_type {
#ifdef _WIN32
	MDB_O_RDONLY, MDB_O_RDWR, MDB_O_OVERLAPPED, MDB_O_META, MDB_O_COPY, MDB_O_LOCKS
#else
	/* A comment in mdb_fopen() explains some O_* flag choices. */
	MDB_O_RDONLY= O_RDONLY,														/**< for RDONLY me_fd */
	MDB_O_RDWR	= O_RDWR	|O_CREAT,										/**< for me_fd */
	MDB_O_META	= O_WRONLY|MDB_DSYNC		 |MDB_CLOEXEC, /**< for me_mfd */
	MDB_O_COPY	= O_WRONLY|O_CREAT|O_EXCL|MDB_CLOEXEC, /**< for #mdb_env_copy() */
	/** Bitmask for open() flags in enum #mdb_fopen_type.	The other bits
	 * distinguish otherwise-equal MDB_O_* constants from each other.
	 */
	MDB_O_MASK	= MDB_O_RDWR|MDB_CLOEXEC | MDB_O_RDONLY|MDB_O_META|MDB_O_COPY,
	MDB_O_LOCKS = MDB_O_RDWR|MDB_CLOEXEC | ((MDB_O_MASK+1) & ~MDB_O_MASK) /**< for me_lfd */
#endif
};

/** Open an LMDB file.
 * @param[in] env	The LMDB environment.
 * @param[in,out] fname	Path from from #mdb_fname_init().	A suffix is
 * appended if necessary to create the filename, without changing mn_len.
 * @param[in] which	Determines file type, access mode, etc.
 * @param[in] mode	The Unix permissions for the file, if we create it.
 * @param[out] res	Resulting file handle.
 * @return 0 on success, non-zero on failure.
 */
static int ESECT
mdb_fopen(const MDB_env *env, MDB_name *fname,
	enum mdb_fopen_type which, mdb_mode_t mode,
	HANDLE *res)
{
	int rc = MDB_SUCCESS;
	HANDLE fd;
#ifdef _WIN32
	DWORD acc, share, disp, attrs;
#else
	int flags;
#endif

	if (fname->mn_alloced)		/* modifiable copy */
		mdb_name_cpy(fname->mn_val + fname->mn_len,
			mdb_suffixes[which==MDB_O_LOCKS][F_ISSET(env->me_flags, MDB_NOSUBDIR)]);

	/* The directory must already exist.	Usually the file need not.
	 * MDB_O_META requires the file because we already created it using
	 * MDB_O_RDWR.	MDB_O_COPY must not overwrite an existing file.
	 *
	 * With MDB_O_COPY we do not want the OS to cache the writes, since
	 * the source data is already in the OS cache.
	 *
	 * The lockfile needs FD_CLOEXEC (close file descriptor on exec*())
	 * to avoid the flock() issues noted under Caveats in lmdb.h.
	 * Also set it for other filehandles which the user cannot get at
	 * and close himself, which he may need after fork().	I.e. all but
	 * me_fd, which programs do use via mdb_env_get_fd().
	 */

#ifdef _WIN32
	acc = GENERIC_READ|GENERIC_WRITE;
	share = FILE_SHARE_READ|FILE_SHARE_WRITE;
	disp = OPEN_ALWAYS;
	attrs = FILE_ATTRIBUTE_NORMAL;
	switch (which) {
	case MDB_O_OVERLAPPED: 	/* for unbuffered asynchronous writes (write-through mode)*/
		acc = GENERIC_WRITE;
		disp = OPEN_EXISTING;
		attrs = FILE_FLAG_OVERLAPPED|FILE_FLAG_WRITE_THROUGH;
		break;
	case MDB_O_RDONLY:			/* read-only datafile */
		acc = GENERIC_READ;
		disp = OPEN_EXISTING;
		break;
	case MDB_O_META:			/* for writing metapages */
		acc = GENERIC_WRITE;
		disp = OPEN_EXISTING;
		attrs = FILE_ATTRIBUTE_NORMAL|FILE_FLAG_WRITE_THROUGH;
		break;
	case MDB_O_COPY:			/* mdb_env_copy() & co */
		acc = GENERIC_WRITE;
		share = 0;
		disp = CREATE_NEW;
		attrs = FILE_FLAG_NO_BUFFERING|FILE_FLAG_WRITE_THROUGH;
		break;
	default: break;	/* silence gcc -Wswitch (not all enum values handled) */
	}
	fd = CreateFileW(fname->mn_val, acc, share, NULL, disp, attrs, NULL);
#else
	fd = open(fname->mn_val, which & MDB_O_MASK, mode);
#endif

	if (fd == INVALID_HANDLE_VALUE)
		rc = ErrCode();
#ifndef _WIN32
	else {
		if (which != MDB_O_RDONLY && which != MDB_O_RDWR) {
			/* Set CLOEXEC if we could not pass it to open() */
			if (!MDB_CLOEXEC && (flags = fcntl(fd, F_GETFD)) != -1)
				(void) fcntl(fd, F_SETFD, flags | FD_CLOEXEC);
		}
		if (which == MDB_O_COPY && env->me_psize >= env->me_os_psize) {
			/* This may require buffer alignment.	There is no portable
			 * way to ask how much, so we require OS pagesize alignment.
			 */
# ifdef F_NOCACHE	/* __APPLE__ */
			(void) fcntl(fd, F_NOCACHE, 1);
# elif defined O_DIRECT
			/* open(...O_DIRECT...) would break on filesystems without
			 * O_DIRECT support (ITS#7682). Try to set it here instead.
			 */
			if ((flags = fcntl(fd, F_GETFL)) != -1)
				(void) fcntl(fd, F_SETFL, flags | O_DIRECT);
# endif
		}
	}
#endif	/* !_WIN32 */

	*res = fd;
	return rc;
}


#ifdef BROKEN_FDATASYNC
#include <sys/utsname.h>
#include <sys/vfs.h>
#endif

/** Further setup required for opening an LMDB environment
 */
static int ESECT
mdb_env_open2(MDB_env *env, int prev)
{
	unsigned int flags = env->me_flags;
	int i, newenv = 0, rc;
	MDB_meta meta;

#ifdef _WIN32
	/* See if we should use QueryLimited */
	rc = GetVersion();
	if ((rc & 0xff) > 5)
		env->me_pidquery = MDB_PROCESS_QUERY_LIMITED_INFORMATION;
	else
		env->me_pidquery = PROCESS_QUERY_INFORMATION;
	/* Grab functions we need from NTDLL */
	if (!NtCreateSection) {
		HMODULE h = GetModuleHandleW(L"NTDLL.DLL");
		if (!h)
			return MDB_PROBLEM;
		NtClose = (NtCloseFunc *)GetProcAddress(h, "NtClose");
		if (!NtClose)
			return MDB_PROBLEM;
		NtMapViewOfSection = (NtMapViewOfSectionFunc *)GetProcAddress(h, "NtMapViewOfSection");
		if (!NtMapViewOfSection)
			return MDB_PROBLEM;
		NtCreateSection = (NtCreateSectionFunc *)GetProcAddress(h, "NtCreateSection");
		if (!NtCreateSection)
			return MDB_PROBLEM;
	}
	env->me_ovs = 0;
#endif /* _WIN32 */

#ifdef BROKEN_FDATASYNC
	/* ext3/ext4 fdatasync is broken on some older Linux kernels.
	 * https://lkml.org/lkml/2012/9/3/83
	 * Kernels after 3.6-rc6 are known good.
	 * https://lkml.org/lkml/2012/9/10/556
	 * See if the DB is on ext3/ext4, then check for new enough kernel
	 * Kernels 2.6.32.60, 2.6.34.15, 3.2.30, and 3.5.4 are also known
	 * to be patched.
	 */
	{
		struct statfs st;
		fstatfs(env->me_fd, &st);
		while (st.f_type == 0xEF53) {
			struct utsname uts;
			int i;
			uname(&uts);
			if (uts.release[0] < '3') {
				if (!strncmp(uts.release, "2.6.32.", 7)) {
					i = atoi(uts.release+7);
					if (i >= 60)
						break;	/* 2.6.32.60 and newer is OK */
				} else if (!strncmp(uts.release, "2.6.34.", 7)) {
					i = atoi(uts.release+7);
					if (i >= 15)
						break;	/* 2.6.34.15 and newer is OK */
				}
			} else if (uts.release[0] == '3') {
				i = atoi(uts.release+2);
				if (i > 5)
					break;	/* 3.6 and newer is OK */
				if (i == 5) {
					i = atoi(uts.release+4);
					if (i >= 4)
						break;	/* 3.5.4 and newer is OK */
				} else if (i == 2) {
					i = atoi(uts.release+4);
					if (i >= 30)
						break;	/* 3.2.30 and newer is OK */
				}
			} else {	/* 4.x and newer is OK */
				break;
			}
			env->me_flags |= MDB_FSYNCONLY;
			break;
		}
	}
#endif

	if ((i = mdb_env_read_header(env, prev, &meta)) != 0) {
		if (i != ENOENT)
			return i;
		DPUTS("new mdbenv");
		newenv = 1;
		env->me_psize = env->me_os_psize;
		if (env->me_psize > MAX_PAGESIZE)
			env->me_psize = MAX_PAGESIZE;
		memset(&meta, 0, sizeof(meta));
		mdb_env_init_meta0(env, &meta);
		meta.mm_mapsize = DEFAULT_MAPSIZE;
	} else {
		env->me_psize = meta.mm_psize;
	}

	/* Was a mapsize configured? */
	if (!env->me_mapsize) {
		env->me_mapsize = meta.mm_mapsize;
	}
	{
		/* Make sure mapsize >= committed data size.	Even when using
		 * mm_mapsize, which could be broken in old files (ITS#7789).
		 */
		mdb_size_t minsize = (meta.mm_last_pg + 1) * meta.mm_psize;
		if (env->me_mapsize < minsize)
			env->me_mapsize = minsize;
	}
	meta.mm_mapsize = env->me_mapsize;

	if (newenv && !(flags & MDB_FIXEDMAP)) {
		/* mdb_env_map() may grow the datafile.	Write the metapages
		 * first, so the file will be valid if initialization fails.
		 * Except with FIXEDMAP, since we do not yet know mm_address.
		 * We could fill in mm_address later, but then a different
		 * program might end up doing that - one with a memory layout
		 * and map address which does not suit the main program.
		 */
		rc = mdb_env_init_meta(env, &meta);
		if (rc)
			return rc;
		newenv = 0;
	}
#ifdef _WIN32
	/* For FIXEDMAP, make sure the file is non-empty before we attempt to map it */
	if (newenv) {
		char dummy = 0;
		DWORD len;
		rc = WriteFile(env->me_fd, &dummy, 1, &len, NULL);
		if (!rc) {
			rc = ErrCode();
			return rc;
		}
	}
#endif

	rc = mdb_env_map(env, (flags & MDB_FIXEDMAP) ? meta.mm_address : NULL);
	if (rc)
		return rc;

	if (newenv) {
		if (flags & MDB_FIXEDMAP)
			meta.mm_address = env->me_map;
		i = mdb_env_init_meta(env, &meta);
		if (i != MDB_SUCCESS) {
			return i;
		}
	}
	if ((env->me_flags ^ env->me_metas[0]->mm_flags) & MDB_ENCRYPT)
		return MDB_ENV_ENCRYPTION;

#if MDB_RPAGE_CACHE
	if (!newenv && env->me_sumfunc) {
		/* for checksums, check sum size from tail of page 0 */
		char *ptr = env->me_map + env->me_psize;
		unsigned short *u = (unsigned short *)(ptr - 2);
		if (*u != env->me_sumsize)
			return MDB_BAD_CHECKSUM;
	}
#endif

	env->me_maxfree_1pg = (env->me_psize - PAGEHDRSZ) / sizeof(pgno_t) - 2;
	env->me_nodemax = (((env->me_psize - PAGEHDRSZ) / MDB_MINKEYS) & -2)
		- sizeof(indx_t);
#if !(MDB_MAXKEYSIZE)
	env->me_maxkey = env->me_nodemax - (NODESIZE + sizeof(MDB_db));
#endif
	env->me_maxpg = env->me_mapsize / env->me_psize;

	if (env->me_txns)
		env->me_txns->mti_txnid = meta.mm_txnid;

#if MDB_DEBUG
	{
		MDB_meta *meta = mdb_env_pick_meta(env);
		MDB_db *db = &meta->mm_dbs[MAIN_DBI];

		DPRINTF(("opened database version %u, pagesize %u",
			meta->mm_version, env->me_psize));
		DPRINTF(("using meta page %d",	(int) (meta->mm_txnid & 1)));
		DPRINTF(("depth: %u",					 db->md_depth));
		DPRINTF(("entries: %"Yu,				db->md_entries));
		DPRINTF(("branch pages: %"Yu,	 db->md_branch_pages));
		DPRINTF(("leaf pages: %"Yu,		 db->md_leaf_pages));
		DPRINTF(("overflow pages: %"Yu, db->md_overflow_pages));
		DPRINTF(("root: %"Yu,					 db->md_root));
	}
#endif

	return MDB_SUCCESS;
}


/** Release a reader thread's slot in the reader lock table.
 *	This function is called automatically when a thread exits.
 * @param[in] ptr This points to the slot in the reader lock table.
 */
static void
mdb_env_reader_dest(void *ptr)
{
	MDB_reader *reader = ptr;

#ifndef _WIN32
	if (reader->mr_pid == getpid()) /* catch pthread_exit() in child process */
#endif
		/* We omit the mutex, so do this atomically (i.e. skip mr_txnid) */
		reader->mr_pid = 0;
}

#ifdef _WIN32
/** Junk for arranging thread-specific callbacks on Windows. This is
 *	necessarily platform and compiler-specific. Windows supports up
 *	to 1088 keys. Let's assume nobody opens more than 64 environments
 *	in a single process, for now. They can override this if needed.
 */
#ifndef MAX_TLS_KEYS
#define MAX_TLS_KEYS	64
#endif
static pthread_key_t mdb_tls_keys[MAX_TLS_KEYS];
static int mdb_tls_nkeys;

static void NTAPI mdb_tls_callback(PVOID module, DWORD reason, PVOID ptr)
{
	int i;
	switch(reason) {
	case DLL_PROCESS_ATTACH: break;
	case DLL_THREAD_ATTACH: break;
	case DLL_THREAD_DETACH:
		for (i=0; i<mdb_tls_nkeys; i++) {
			MDB_reader *r = pthread_getspecific(mdb_tls_keys[i]);
			if (r) {
				mdb_env_reader_dest(r);
			}
		}
		break;
	case DLL_PROCESS_DETACH: break;
	}
}
#ifdef __GNUC__
#ifdef _WIN64
const PIMAGE_TLS_CALLBACK mdb_tls_cbp __attribute__((section (".CRT$XLB"))) = mdb_tls_callback;
#else
PIMAGE_TLS_CALLBACK mdb_tls_cbp __attribute__((section (".CRT$XLB"))) = mdb_tls_callback;
#endif
#else
#ifdef _WIN64
/* Force some symbol references.
 *	_tls_used forces the linker to create the TLS directory if not already done
 *	mdb_tls_cbp prevents whole-program-optimizer from dropping the symbol.
 */
#pragma comment(linker, "/INCLUDE:_tls_used")
#pragma comment(linker, "/INCLUDE:mdb_tls_cbp")
#pragma const_seg(".CRT$XLB")
extern const PIMAGE_TLS_CALLBACK mdb_tls_cbp;
const PIMAGE_TLS_CALLBACK mdb_tls_cbp = mdb_tls_callback;
#pragma const_seg()
#else	/* _WIN32 */
#pragma comment(linker, "/INCLUDE:__tls_used")
#pragma comment(linker, "/INCLUDE:_mdb_tls_cbp")
#pragma data_seg(".CRT$XLB")
PIMAGE_TLS_CALLBACK mdb_tls_cbp = mdb_tls_callback;
#pragma data_seg()
#endif	/* WIN 32/64 */
#endif	/* !__GNUC__ */
#endif

/** Downgrade the exclusive lock on the region back to shared */
static int ESECT
mdb_env_share_locks(MDB_env *env, int *excl)
{
	int rc = 0;
	MDB_meta *meta = mdb_env_pick_meta(env);

	env->me_txns->mti_txnid = meta->mm_txnid;

#ifdef _WIN32
	{
		OVERLAPPED ov;
		/* First acquire a shared lock. The Unlock will
		 * then release the existing exclusive lock.
		 */
		memset(&ov, 0, sizeof(ov));
		if (!LockFileEx(env->me_lfd, 0, 0, 1, 0, &ov)) {
			rc = ErrCode();
		} else {
			UnlockFile(env->me_lfd, 0, 0, 1, 0);
			*excl = 0;
		}
	}
#else
	{
		struct flock lock_info;
		/* The shared lock replaces the existing lock */
		memset((void *)&lock_info, 0, sizeof(lock_info));
		lock_info.l_type = F_RDLCK;
		lock_info.l_whence = SEEK_SET;
		lock_info.l_start = 0;
		lock_info.l_len = 1;
		while ((rc = fcntl(env->me_lfd, F_SETLK, &lock_info)) &&
				(rc = ErrCode()) == EINTR) ;
		*excl = rc ? -1 : 0;	/* error may mean we lost the lock */
	}
#endif

	return rc;
}

/** Try to get exclusive lock, otherwise shared.
 *	Maintain *excl = -1: no/unknown lock, 0: shared, 1: exclusive.
 */
static int ESECT
mdb_env_excl_lock(MDB_env *env, int *excl)
{
	int rc = 0;
#ifdef _WIN32
	if (LockFile(env->me_lfd, 0, 0, 1, 0)) {
		*excl = 1;
	} else {
		OVERLAPPED ov;
		memset(&ov, 0, sizeof(ov));
		if (LockFileEx(env->me_lfd, 0, 0, 1, 0, &ov)) {
			*excl = 0;
		} else {
			rc = ErrCode();
		}
	}
#else
	struct flock lock_info;
	memset((void *)&lock_info, 0, sizeof(lock_info));
	lock_info.l_type = F_WRLCK;
	lock_info.l_whence = SEEK_SET;
	lock_info.l_start = 0;
	lock_info.l_len = 1;
	while ((rc = fcntl(env->me_lfd, F_SETLK, &lock_info)) &&
			(rc = ErrCode()) == EINTR) ;
	if (!rc) {
		*excl = 1;
	} else
# ifndef MDB_USE_POSIX_MUTEX
	if (*excl < 0) /* always true when MDB_USE_POSIX_MUTEX */
# endif
	{
		lock_info.l_type = F_RDLCK;
		while ((rc = fcntl(env->me_lfd, F_SETLKW, &lock_info)) &&
				(rc = ErrCode()) == EINTR) ;
		if (rc == 0)
			*excl = 0;
	}
#endif
	return rc;
}

#ifdef MDB_USE_HASH
/*
 * hash_64 - 64 bit Fowler/Noll/Vo-0 FNV-1a hash code
 *
 * @(#) $Revision: 5.1 $
 * @(#) $Id: hash_64a.c,v 5.1 2009/06/30 09:01:38 chongo Exp $
 * @(#) $Source: /usr/local/src/cmd/fnv/RCS/hash_64a.c,v $
 *
 *		http://www.isthe.com/chongo/tech/comp/fnv/index.html
 *
 ***
 *
 * Please do not copyright this code.	This code is in the public domain.
 *
 * LANDON CURT NOLL DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE,
 * INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO
 * EVENT SHALL LANDON CURT NOLL BE LIABLE FOR ANY SPECIAL, INDIRECT OR
 * CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF
 * USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR
 * OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR
 * PERFORMANCE OF THIS SOFTWARE.
 *
 * By:
 *	chongo <Landon Curt Noll> /\oo/\
 *		http://www.isthe.com/chongo/
 *
 * Share and Enjoy!	:-)
 */

/** perform a 64 bit Fowler/Noll/Vo FNV-1a hash on a buffer
 * @param[in] val	value to hash
 * @param[in] len	length of value
 * @return 64 bit hash
 */
static mdb_hash_t
mdb_hash(const void *val, size_t len)
{
	const unsigned char *s = (const unsigned char *) val, *end = s + len;
	mdb_hash_t hval = 0xcbf29ce484222325ULL;
	/*
	 * FNV-1a hash each octet of the buffer
	 */
	while (s < end) {
		hval = (hval ^ *s++) * 0x100000001b3ULL;
	}
	/* return our new hash value */
	return hval;
}

/** Hash the string and output the encoded hash.
 * This uses modified RFC1924 Ascii85 encoding to accommodate systems with
 * very short name limits. We don't care about the encoding being reversible,
 * we just want to preserve as many bits of the input as possible in a
 * small printable string.
 * @param[in] str string to hash
 * @param[out] encbuf an array of 11 chars to hold the hash
 */
static const char mdb_a85[]= "0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz!#$%&()*+-;<=>?@^_`{|}~";

static void ESECT
mdb_pack85(unsigned long long l, char *out)
{
	int i;

	for (i=0; i<10 && l; i++) {
		*out++ = mdb_a85[l % 85];
		l /= 85;
	}
	*out = '\0';
}

/** Init #MDB_env.me_mutexname[] except the char which #MUTEXNAME() will set.
 *	Changes to this code must be reflected in #MDB_LOCK_FORMAT.
 */
static void ESECT
mdb_env_mname_init(MDB_env *env)
{
	char *nm = env->me_mutexname;
	strcpy(nm, MUTEXNAME_PREFIX);
	mdb_pack85(env->me_txns->mti_mutexid, nm + sizeof(MUTEXNAME_PREFIX));
}

/** Return env->me_mutexname after filling in ch ('r'/'w') for convenience */
#define MUTEXNAME(env, ch) ( \
		(void) ((env)->me_mutexname[sizeof(MUTEXNAME_PREFIX)-1] = (ch)), \
		(env)->me_mutexname)

#endif

/** Open and/or initialize the lock region for the environment.
 * @param[in] env The LMDB environment.
 * @param[in] fname Filename + scratch area, from #mdb_fname_init().
 * @param[in] mode The Unix permissions for the file, if we create it.
 * @param[in,out] excl In -1, out lock type: -1 none, 0 shared, 1 exclusive
 * @return 0 on success, non-zero on failure.
 */
static int ESECT
mdb_env_setup_locks(MDB_env *env, MDB_name *fname, int mode, int *excl)
{
#ifdef _WIN32
#	define MDB_ERRCODE_ROFS	ERROR_WRITE_PROTECT
#else
#	define MDB_ERRCODE_ROFS	EROFS
#endif
#ifdef MDB_USE_SYSV_SEM
	int semid;
	union semun semu;
#endif
	int rc;
	MDB_OFF_T size, rsize;
	if (env->me_callback) {
		rc = mdb_fopen(env, fname, MDB_O_RDWR, mode, &env->me_lfd);
		if (!rc) {
			rc = ((MDB_check_fd*) env->me_callback)(env->me_lfd, env);
			if (rc)
				return rc;
		}
		close(env->me_lfd);
	}
	rc = mdb_fopen(env, fname, MDB_O_LOCKS, mode, &env->me_lfd);
	if (rc) {
		/* Omit lockfile if read-only env on read-only filesystem or no write permission */
		if ((rc == MDB_ERRCODE_ROFS || rc == EACCES) && (env->me_flags & MDB_RDONLY)) {
			return MDB_SUCCESS;
		}
		goto fail;
	}

	if (!(env->me_flags & MDB_NOTLS)) {
		rc = pthread_key_create(&env->me_txkey, mdb_env_reader_dest);
		if (rc)
			goto fail;
		env->me_flags |= MDB_ENV_TXKEY;
#ifdef _WIN32
		/* Windows TLS callbacks need help finding their TLS info. */
		if (mdb_tls_nkeys >= MAX_TLS_KEYS) {
			rc = MDB_TLS_FULL;
			goto fail;
		}
		mdb_tls_keys[mdb_tls_nkeys++] = env->me_txkey;
#endif
	}

	/* Try to get exclusive lock. If we succeed, then
	 * nobody is using the lock region and we should initialize it.
	 */
	if ((rc = mdb_env_excl_lock(env, excl))) goto fail;

#ifdef _WIN32
	size = GetFileSize(env->me_lfd, NULL);
#else
	size = lseek(env->me_lfd, 0, SEEK_END);
	if (size == -1) goto fail_errno;
#endif
	rsize = (env->me_maxreaders-1) * sizeof(MDB_reader) + sizeof(MDB_txninfo);
	if (size < rsize && *excl > 0) {
#ifdef _WIN32
		if (SetFilePointer(env->me_lfd, rsize, NULL, FILE_BEGIN) != (DWORD)rsize
			|| !SetEndOfFile(env->me_lfd))
			goto fail_errno;
#else
		if (ftruncate(env->me_lfd, rsize) != 0) goto fail_errno;
#endif
	} else {
		rsize = size;
		size = rsize - sizeof(MDB_txninfo);
		env->me_maxreaders = size/sizeof(MDB_reader) + 1;
	}
	{
#ifdef _WIN32
		HANDLE mh;
		mh = CreateFileMapping(env->me_lfd, NULL, PAGE_READWRITE,
			0, 0, NULL);
		if (!mh) goto fail_errno;
		env->me_txns = MapViewOfFileEx(mh, FILE_MAP_WRITE, 0, 0, rsize, NULL);
		CloseHandle(mh);
		if (!env->me_txns) goto fail_errno;
#else
		void *m = mmap(NULL, rsize, PROT_READ|PROT_WRITE, MAP_SHARED,
			env->me_lfd, 0);
		if (m == MAP_FAILED) goto fail_errno;
		env->me_txns = m;
#endif
	}
	if (*excl > 0) {
#ifdef _WIN32
		BY_HANDLE_FILE_INFORMATION stbuf;
		struct {
			DWORD volume;
			DWORD nhigh;
			DWORD nlow;
		} idbuf;

		if (!mdb_sec_inited) {
			InitializeSecurityDescriptor(&mdb_null_sd,
				SECURITY_DESCRIPTOR_REVISION);
			SetSecurityDescriptorDacl(&mdb_null_sd, TRUE, 0, FALSE);
			mdb_all_sa.nLength = sizeof(SECURITY_ATTRIBUTES);
			mdb_all_sa.bInheritHandle = FALSE;
			mdb_all_sa.lpSecurityDescriptor = &mdb_null_sd;
			mdb_sec_inited = 1;
		}
		if (!GetFileInformationByHandle(env->me_lfd, &stbuf)) goto fail_errno;
		idbuf.volume = stbuf.dwVolumeSerialNumber;
		idbuf.nhigh	= stbuf.nFileIndexHigh;
		idbuf.nlow	 = stbuf.nFileIndexLow;
		env->me_txns->mti_mutexid = mdb_hash(&idbuf, sizeof(idbuf));
		mdb_env_mname_init(env);
		env->me_rmutex = CreateMutexA(&mdb_all_sa, FALSE, MUTEXNAME(env, 'r'));
		if (!env->me_rmutex) goto fail_errno;
		env->me_wmutex = CreateMutexA(&mdb_all_sa, FALSE, MUTEXNAME(env, 'w'));
		if (!env->me_wmutex) goto fail_errno;
		env->me_sync_mutex = CreateMutexA(&mdb_all_sa, FALSE, MUTEXNAME(env, 's'));
		if (!env->me_sync_mutex) goto fail_errno;
#elif defined(MDB_USE_POSIX_SEM)
		struct stat stbuf;
		struct {
			dev_t dev;
			ino_t ino;
		} idbuf;

#if defined(__NetBSD__)
#define	MDB_SHORT_SEMNAMES	1	/* limited to 14 chars */
#endif
		if (fstat(env->me_lfd, &stbuf)) goto fail_errno;
		memset(&idbuf, 0, sizeof(idbuf));
		idbuf.dev = stbuf.st_dev;
		idbuf.ino = stbuf.st_ino;
		env->me_txns->mti_mutexid = mdb_hash(&idbuf, sizeof(idbuf))
#ifdef MDB_SHORT_SEMNAMES
			/* Max 9 base85-digits.	We truncate here instead of in
			 * mdb_env_mname_init() to keep the latter portable.
			 */
			% ((mdb_hash_t)85*85*85*85*85*85*85*85*85)
#endif
			;
		mdb_env_mname_init(env);
		/* Clean up after a previous run, if needed:	Try to
		 * remove both semaphores before doing anything else.
		 */
		sem_unlink(MUTEXNAME(env, 'r'));
		sem_unlink(MUTEXNAME(env, 'w'));
		sem_unlink(MUTEXNAME(env, 's'));
		env->me_rmutex = sem_open(MUTEXNAME(env, 'r'), O_CREAT|O_EXCL, mode, 1);
		if (env->me_rmutex == SEM_FAILED) goto fail_errno;
		env->me_wmutex = sem_open(MUTEXNAME(env, 'w'), O_CREAT|O_EXCL, mode, 1);
		if (env->me_wmutex == SEM_FAILED) goto fail_errno;
		env->me_sync_mutex = sem_open(MUTEXNAME(env, 's'), O_CREAT|O_EXCL, mode, 1);
		if (env->me_sync_mutex == SEM_FAILED) goto fail_errno;
#elif defined(MDB_USE_SYSV_SEM)
		unsigned short vals[3] = {1, 1, 1};
		key_t key = ftok(fname->mn_val, 'M'); /* fname is lockfile path now */
		if (key == -1)
			goto fail_errno;
		semid = semget(key, 3, (mode & 0777) | IPC_CREAT);
		if (semid < 0)
			goto fail_errno;
		semu.array = vals;
		if (semctl(semid, 0, SETALL, semu) < 0)
			goto fail_errno;
		env->me_txns->mti_semid = semid;
		env->me_txns->mti_rlocked = 0;
		env->me_txns->mti_wlocked = 0;
		env->me_txns->mti_sync_locked = 0;
#else	/* MDB_USE_POSIX_MUTEX: */
		pthread_mutexattr_t mattr;

		/* Solaris needs this before initing a robust mutex.	Otherwise
		 * it may skip the init and return EBUSY "seems someone already
		 * inited" or EINVAL "it was inited differently".
		 */
		memset(env->me_txns->mti_rmutex, 0, sizeof(*env->me_txns->mti_rmutex));
		memset(env->me_txns->mti_wmutex, 0, sizeof(*env->me_txns->mti_wmutex));
		memset(env->me_txns->mti_sync_mutex, 0, sizeof(*env->me_txns->mti_sync_mutex));

		if ((rc = pthread_mutexattr_init(&mattr)) != 0)
			goto fail;
		rc = pthread_mutexattr_setpshared(&mattr, PTHREAD_PROCESS_SHARED);
#ifdef MDB_ROBUST_SUPPORTED
		if (!rc) pthread_mutexattr_setrobust(&mattr, PTHREAD_MUTEX_ROBUST); // don't record rc, we don't care if it fails
#endif
		if (!rc) rc = pthread_mutex_init(env->me_txns->mti_rmutex, &mattr);
		if (!rc) rc = pthread_mutex_init(env->me_txns->mti_wmutex, &mattr);
		if (!rc) rc = pthread_mutex_init(env->me_txns->mti_sync_mutex, &mattr);
		pthread_mutexattr_destroy(&mattr);
		if (rc)
			goto fail;
#endif	/* _WIN32 || ... */

		env->me_txns->mti_magic = MDB_MAGIC;
		env->me_txns->mti_format = MDB_LOCK_FORMAT;
		env->me_txns->mti_txnid = 0;
		env->me_txns->mti_numreaders = 0;

	} else {
#ifdef MDB_USE_SYSV_SEM
		struct semid_ds buf;
#endif
		if (env->me_txns->mti_magic != MDB_MAGIC) {
			DPUTS("lock region has invalid magic");
			rc = MDB_INVALID;
			goto fail;
		}
		if (env->me_txns->mti_format != MDB_LOCK_FORMAT) {
			DPRINTF(("lock region has format+version 0x%x, expected 0x%x",
				env->me_txns->mti_format, MDB_LOCK_FORMAT));
			rc = MDB_VERSION_MISMATCH;
			goto fail;
		}
		rc = ErrCode();
		if (rc && rc != EACCES && rc != EAGAIN) {
			goto fail;
		}
#ifdef _WIN32
		mdb_env_mname_init(env);
		env->me_rmutex = OpenMutexA(SYNCHRONIZE, FALSE, MUTEXNAME(env, 'r'));
		if (!env->me_rmutex) goto fail_errno;
		env->me_wmutex = OpenMutexA(SYNCHRONIZE, FALSE, MUTEXNAME(env, 'w'));
		if (!env->me_wmutex) goto fail_errno;
		env->me_sync_mutex = OpenMutexA(SYNCHRONIZE, FALSE, MUTEXNAME(env, 's'));
		if (!env->me_sync_mutex) goto fail_errno;
#elif defined(MDB_USE_POSIX_SEM)
		mdb_env_mname_init(env);
		env->me_rmutex = sem_open(MUTEXNAME(env, 'r'), 0);
		if (env->me_rmutex == SEM_FAILED) goto fail_errno;
		env->me_wmutex = sem_open(MUTEXNAME(env, 'w'), 0);
		if (env->me_wmutex == SEM_FAILED) goto fail_errno;
		env->me_sync_mutex = sem_open(MUTEXNAME(env, 's'), 0);
		if (env->me_sync_mutex == SEM_FAILED) goto fail_errno;
#elif defined(MDB_USE_SYSV_SEM)
		semid = env->me_txns->mti_semid;
		semu.buf = &buf;
		/* check for read access */
		if (semctl(semid, 0, IPC_STAT, semu) < 0)
			goto fail_errno;
		/* check for write access */
		if (semctl(semid, 0, IPC_SET, semu) < 0)
			goto fail_errno;
#endif
	}
#ifdef MDB_USE_SYSV_SEM
	env->me_rmutex->semid = semid;
	env->me_wmutex->semid = semid;
	env->me_sync_mutex->semid = semid;
	env->me_rmutex->semnum = 0;
	env->me_wmutex->semnum = 1;
	env->me_sync_mutex->semnum = 2;
	env->me_rmutex->locked = &env->me_txns->mti_rlocked;
	env->me_wmutex->locked = &env->me_txns->mti_wlocked;
	env->me_sync_mutex->locked = &env->me_txns->mti_sync_locked;
#endif

	return MDB_SUCCESS;

fail_errno:
	rc = ErrCode();
fail:
	return rc;
}

#ifdef MDB_TEST
#if MDB_RPAGE_CACHE
/** Trivial encryption for testing */
static void ESECT
mdb_enctest(const MDB_val *src, MDB_val *dst, const MDB_val *key, int encdec)
{
	mdb_size_t *sptr = src->mv_data, *dptr = dst->mv_data;
	mdb_size_t x=*(mdb_size_t*)key[0].mv_data, v=*(mdb_size_t*)key[1].mv_data;
	int i, len = dst->mv_size / sizeof(mdb_size_t);

	for (i = 0; i < len; i++)
		x += v += i + sptr[i] + (dptr[i] = sptr[i] ^ x);
}
#endif	/* MDB_RPAGE_CACHE */

/** Add #mdb_env_open() flags from environment variable $LMDB_FLAGS.
 *
 *	Supports the normal flags plus 'e' = trivial encryption for testing.
 */
static int ESECT
mdb_env_envflags(MDB_env *env)
{
	static const char names[] = "ace" "fhi" "lmn" "rst" "vw";
	static const unsigned f[] = {
		/*a*/ MDB_MAPASYNC, /*c*/ MDB_REMAP_CHUNKS, /*e*/ MDB_ENCRYPT,
		/*f*/ MDB_FIXEDMAP, /*h*/ MDB_NORDAHEAD,		/*i*/ MDB_NOMEMINIT,
		/*l*/ MDB_NOLOCK,	 /*m*/ MDB_NOMETASYNC,	 /*n*/ MDB_NOSUBDIR,
		/*r*/ MDB_RDONLY,	 /*s*/ MDB_NOSYNC,			 /*t*/ MDB_NOTLS,
		/*v*/ MDB_PREVSNAPSHOT, /*w*/ MDB_WRITEMAP,
	};
	unsigned flags = 0;
	const char *s, *opts = getenv("LMDB_FLAGS");
	if (opts) {
		for (; *opts; opts++) {
			if ((s = strchr(names, *opts)) == NULL)
				return EINVAL;
			flags |= f[s - names];
		}
		if (flags & MDB_ENCRYPT) {
#if MDB_RPAGE_CACHE
			if (!env->me_encfunc) {
				static mdb_size_t k = (MDB_SIZE_MAX/67*73) | 1;
				MDB_val key = {sizeof(k), &k};
				int rc;
				rc = mdb_env_set_encrypt(env, mdb_enctest, &key, 0);
				if (rc)
					return rc;
			}
#else
			return EINVAL;
#endif
		}
		env->me_flags |= flags;
	}
	return MDB_SUCCESS;
}
#else
#define mdb_env_envflags(env) MDB_SUCCESS
#endif	/* MDB_TEST */

	/** Only a subset of the @ref mdb_env flags can be changed
	 *	at runtime. Changing other flags requires closing the
	 *	environment and re-opening it with the new flags.
	 */
#define	CHANGEABLE	(MDB_NOSYNC|MDB_NOMETASYNC|MDB_MAPASYNC|MDB_NOMEMINIT)
#define	CHANGELESS	(MDB_FIXEDMAP|MDB_NOSUBDIR|MDB_RDONLY| \
	MDB_WRITEMAP|MDB_NOTLS|MDB_NOLOCK|MDB_NORDAHEAD|MDB_PREVSNAPSHOT|MDB_REMAP_CHUNKS|MDB_OVERLAPPINGSYNC|MDB_SAFE_RESTORE|MDB_TRACK_METRICS)
#define EXPOSED		(CHANGEABLE|CHANGELESS | MDB_ENCRYPT)

#if VALID_FLAGS & PERSISTENT_FLAGS & EXPOSED
# error "Persistent DB flags & env flags overlap, but both go in mm_flags"
#endif

int ESECT
mdb_env_open(MDB_env *env, const char *path, unsigned int flags, mdb_mode_t mode)
{
	int rc, excl = -1;
	MDB_name fname;
	if (env->me_fd!=INVALID_HANDLE_VALUE || (flags & ~(CHANGEABLE|CHANGELESS)))
		return EINVAL;

	if ((rc = mdb_env_envflags(env)) != MDB_SUCCESS)
		return rc;
	flags |= env->me_flags;

	if (MDB_REMAPPING(0))			/* if we always remap chunks */
		flags |= MDB_REMAP_CHUNKS;
	if (MDB_REMAPPING(flags)) {
		/* silently ignore WRITEMAP with REMAP_CHUNKS */
		flags &= ~MDB_WRITEMAP;
#if (MDB_RPAGE_CACHE) & 2
		/* TEST: silently ignore FIXEDMAP, so mtest*.c will work */
		flags &= ~MDB_FIXEDMAP;
#else
		/* cannot support FIXEDMAP */
		if (flags & MDB_FIXEDMAP)
			return EINVAL;
#endif
	}

	rc = mdb_fname_init(path, flags, &fname);
	if (rc)
		return rc;

#if MDB_RPAGE_CACHE
	if (MDB_REMAPPING(flags)) {
#ifdef _WIN32
	env->me_rpmutex = CreateMutex(NULL, FALSE, NULL);
	if (!env->me_rpmutex) {
		rc = ErrCode();
		goto leave;
	}
#else
	rc = pthread_mutex_init(&env->me_rpmutex, NULL);
	if (rc) {
		last_error = "Attempting to initialize mutex";
		goto leave;
	}
#endif
	}
#endif
#ifndef _WIN32
	{
		struct stat st;
		flags &= ~MDB_RAWPART;
		if (!stat(path, &st) && (S_ISBLK(st.st_mode) || S_ISCHR(st.st_mode)))
			flags |= MDB_RAWPART | MDB_NOSUBDIR;
	}
#endif
	flags |= MDB_ENV_ACTIVE; /* tell mdb_env_close_active() to clean up */

	if (flags & MDB_RDONLY) {
		/* silently ignore WRITEMAP when we're only getting read access */
		flags &= ~MDB_WRITEMAP;
	} else {
		/* WRITEMAP has a dummy element to match dirty_room = 1 */
		size_t dl_size = (flags & MDB_WRITEMAP) ? 2 : MDB_IDL_UM_SIZE;
		if (!((env->me_free_pgs = mdb_midl_alloc(MDB_IDL_UM_MAX)) &&
				(env->me_dirty_list = calloc(dl_size, sizeof(MDB_ID2)))))
			rc = ENOMEM;
		if (env->me_dirty_list && !(env->me_dirty_nump = malloc(dl_size * sizeof(int))))
			rc = ENOMEM;
	}

	env->me_flags = flags;
	if (rc)
		goto leave;

#if MDB_RPAGE_CACHE
	if (MDB_REMAPPING(flags))
	{
		env->me_rpages = malloc(MDB_ERPAGE_SIZE * sizeof(MDB_ID3));
		if (!env->me_rpages) {
			rc = ENOMEM;
			goto leave;
		}
		env->me_rpages[0].mid = 0;
		env->me_rpcheck = MDB_ERPAGE_SIZE/2;
	}
#endif
	/*<lmdb-js>*/
	env->me_last_map = NULL;
	env->boot_id = 0;
	char boot_uuid[42];
	char* endptr;
// copied/adapted from https://github.com/erthink/libmdbx/blob/master/src/osal.c#L2199 (more OSes are handled there, might add windows when full overlappingSync support is added)
#if defined(__linux__) || defined(__gnu_linux__)
		{
	 const int fd =
				open("/proc/sys/kernel/random/boot_id", O_RDONLY | O_NOFOLLOW);
	if (fd != -1) {
		struct statfs fs;
		const ssize_t len =
				(fstatfs(fd, &fs) == 0 && fs.f_type == /* procfs */ 0x9FA0)
					? read(fd, boot_uuid, sizeof(boot_uuid))
					: -1;
		close(fd);
		if (len > 0)
			env->boot_id = strtoll(boot_uuid, &endptr, 16);
	}
	}
#endif /* Linux */

#if defined(__APPLE__) || defined(__MACH__)
	{
		size_t len = sizeof(boot_uuid);
		if (!sysctlbyname("kern.bootsessionuuid", boot_uuid, &len, NULL, 0))
	 	env->boot_id = strtoll(boot_uuid, &endptr, 16);
	}
#endif
	/*</lmdb-js>*/
	env->me_path = strdup(path);
	env->me_dbxs = calloc(env->me_maxdbs, sizeof(MDB_dbx));
	env->me_dbflags = calloc(env->me_maxdbs, sizeof(uint16_t));
	env->me_dbiseqs = calloc(env->me_maxdbs, sizeof(unsigned int));
	if (!(env->me_dbxs && env->me_path && env->me_dbflags && env->me_dbiseqs)) {
		rc = ENOMEM;
		goto leave;
	}
	env->me_dbxs[FREE_DBI].md_cmp = mdb_cmp_long; /* aligned MDB_INTEGERKEY */

	/* For RDONLY, get lockfile after we know datafile exists */
	if (!(flags & (MDB_RDONLY|MDB_NOLOCK))) {
		rc = mdb_env_setup_locks(env, &fname, mode, &excl);
		if (rc) {
			if (rc != 10) last_error = "Attempting to setup locks"; // lmdb-js uses 10 for existing environment found
			goto leave;
		}
		if ((flags & MDB_PREVSNAPSHOT) && !excl) {
			// <lmdb-js>
			flags ^= MDB_PREVSNAPSHOT;
			env->me_flags = flags;
			// </lmdb-js>
		}
	}

	rc = mdb_fopen(env, &fname,
		(flags & MDB_RDONLY) ? MDB_O_RDONLY : MDB_O_RDWR,
		mode, &env->me_fd);
	if (rc) {
		last_error = "Attempting to open main database file";
		goto leave;
	}
#ifdef _WIN32
	rc = mdb_fopen(env, &fname, MDB_O_OVERLAPPED, mode, &env->me_ovfd);
	if (rc)
		goto leave;
#endif
	if ((flags & (MDB_RDONLY|MDB_NOLOCK)) == MDB_RDONLY) {
		rc = mdb_env_setup_locks(env, &fname, mode, &excl);
		if (rc) {
			last_error = "Attempting to setup locks after open";
			goto leave;
		}
	}

	if ((rc = mdb_env_open2(env, flags & MDB_PREVSNAPSHOT)) == MDB_SUCCESS) {
		/* Synchronous fd for meta writes. Needed even with
		 * MDB_NOSYNC/MDB_NOMETASYNC, in case these get reset.
		 */
		if (!(flags & (MDB_RDONLY|MDB_WRITEMAP))) {
			rc = mdb_fopen(env, &fname, MDB_O_META, mode, &env->me_mfd);
			if (rc) {
				last_error = "Attempting to open sync file descriptor";
				goto leave;
			}
		}
		DPRINTF(("opened dbenv %p", (void *) env));
		// <lmdb-js>
		if (excl > 0) {
			if (flags & MDB_PREVSNAPSHOT) {
				MDB_meta* safe_meta = mdb_env_pick_meta(env);
				flags &= ~MDB_PREVSNAPSHOT; // clear the flag now, so we can compare to the latest
				env->me_flags = flags;
				MDB_meta* latest = mdb_env_pick_meta(env);
				if (latest->mm_txnid != safe_meta->mm_txnid) {
					MDB_txn rollback_txn;
					MDB_db dbs[2];
					rollback_txn.mt_env = env;
					rollback_txn.mt_flags = 0;
					rollback_txn.mt_dbs = dbs;
					rollback_txn.mt_dbs[FREE_DBI] = safe_meta->mm_dbs[FREE_DBI];
					rollback_txn.mt_dbs[MAIN_DBI] = safe_meta->mm_dbs[MAIN_DBI];
					rollback_txn.mt_txnid = safe_meta->mm_txnid;
					rollback_txn.mt_next_pgno = safe_meta->mm_last_pg + 1;
					mdb_env_write_meta(&rollback_txn);
					rollback_txn.mt_txnid--; // overwrite both meta pages to safe meta data
					mdb_env_write_meta(&rollback_txn);
				}
			}
			rc = mdb_env_share_locks(env, &excl);
			if (rc) {
				last_error = "Attempting to setup shared locks";
				goto leave;
			}
		}
		// </lmdb-js>
		if (!(flags & MDB_RDONLY)) {
			MDB_txn *txn;
			int tsize = sizeof(MDB_txn), size = tsize + env->me_maxdbs *
				(sizeof(MDB_db)+sizeof(MDB_cursor *)+sizeof(unsigned int)+1);
			if ((env->me_pbuf = calloc(1, env->me_psize)) &&
				(txn = calloc(1, size)))
			{
				txn->mt_dbs = (MDB_db *)((char *)txn + tsize);
				txn->mt_cursors = (MDB_cursor **)(txn->mt_dbs + env->me_maxdbs);
				txn->mt_dbiseqs = (unsigned int *)(txn->mt_cursors + env->me_maxdbs);
				txn->mt_dbflags = (unsigned char *)(txn->mt_dbiseqs + env->me_maxdbs);
				txn->mt_env = env;
#if MDB_RPAGE_CACHE
				if (MDB_REMAPPING(env->me_flags)) {
				txn->mt_rpages = malloc(MDB_TRPAGE_SIZE * sizeof(MDB_ID3));
				if (!txn->mt_rpages) {
					free(txn);
					rc = ENOMEM;
					goto leave;
				}
				txn->mt_rpages[0].mid = 0;
				txn->mt_rpcheck = MDB_TRPAGE_SIZE/2;
				}
#endif
				txn->mt_dbxs = env->me_dbxs;
				txn->mt_flags = MDB_TXN_FINISHED;
				env->me_txn0 = txn;
				MDB_meta* latest = mdb_env_pick_meta(env);
				txn->mt_env->me_synced_txn_id = latest->mm_txnid;
			} else {
				rc = ENOMEM;
			}
		}
	}

leave:
	if (rc) {
		mdb_env_close_active(env, excl);
	}
	mdb_fname_destroy(fname);
	return rc;
}

/** When #MDB_ENV_ACTIVE: Clear #mdb_env_open()ed resources, release readers */
static void ESECT
mdb_env_close_active(MDB_env *env, int excl)
{
	int i;

	if (!(env->me_flags & MDB_ENV_ACTIVE))
		return;

	/* Doing this here since me_dbxs may not exist during mdb_env_close */
	if (env->me_dbxs) {
		for (i = env->me_maxdbs; --i >= CORE_DBS; )
			free(env->me_dbxs[i].md_name.mv_data);
		free(env->me_dbxs);
	}

	free(env->me_pbuf);
	free(env->me_dbiseqs);
	free(env->me_dbflags);
	free(env->me_path);
	free(env->me_dirty_list);
	free(env->me_dirty_nump);
#if MDB_RPAGE_CACHE
	if (MDB_REMAPPING(env->me_flags)) {
	if (env->me_txn0 && env->me_txn0->mt_rpages)
		free(env->me_txn0->mt_rpages);
	if (env->me_rpages) {
		MDB_ID3L el = env->me_rpages;
		unsigned int x;
		for (x=1; x<=el[0].mid; x++)
			munmap(el[x].mptr, el[x].mcnt * env->me_psize);
		free(el);
	}
	}
#endif
	free(env->me_txn0);
	mdb_midl_free(env->me_free_pgs);

	if (env->me_flags & MDB_ENV_TXKEY) {
		pthread_key_delete(env->me_txkey);
#ifdef _WIN32
		/* Delete our key from the global list */
		for (i=0; i<mdb_tls_nkeys; i++)
			if (mdb_tls_keys[i] == env->me_txkey) {
				mdb_tls_keys[i] = mdb_tls_keys[mdb_tls_nkeys-1];
				mdb_tls_nkeys--;
				break;
			}
#endif
	}

	if (env->me_map) {
		if (MDB_REMAPPING(env->me_flags))
			munmap(env->me_map, NUM_METAS*env->me_psize);
		else {
			munmap(env->me_map, env->me_mapsize);
			/*<lmdb-js>*/
			MDB_last_map *last_map = env->me_last_map;
			while(last_map) { // unmap all of the previous maps as well
				munmap(last_map->map, last_map->mapsize);
				MDB_last_map *last_last_map = last_map;
				last_map = last_map->last_map;
				free(last_last_map);
			}
			/*</lmdb-js>*/
		}
	}
	if (env->me_mfd != INVALID_HANDLE_VALUE)
		(void) close(env->me_mfd);
#ifdef _WIN32
	if (env->me_ovs > 0) {
		for (i = 0; i < env->me_ovs; i++) {
			CloseHandle(env->me_ov[i].hEvent);
		}
		free(env->me_ov);
	}
	if (env->me_ovfd != INVALID_HANDLE_VALUE)
		(void) close(env->me_ovfd);
#endif
	if (env->me_fd != INVALID_HANDLE_VALUE)
		(void) close(env->me_fd);
	if (env->me_txns) {
		MDB_PID_T pid = getpid();
		/* Clearing readers is done in this function because
		 * me_txkey with its destructor must be disabled first.
		 *
		 * We skip the the reader mutex, so we touch only
		 * data owned by this process (me_close_readers and
		 * our readers), and clear each reader atomically.
		 */
		for (i = env->me_close_readers; --i >= 0; )
			if (env->me_txns->mti_readers[i].mr_pid == pid)
				env->me_txns->mti_readers[i].mr_pid = 0;
#ifdef _WIN32
		if (env->me_rmutex) {
			CloseHandle(env->me_rmutex);
			if (env->me_wmutex) CloseHandle(env->me_wmutex);
			if (env->me_sync_mutex) CloseHandle(env->me_sync_mutex);
		}
		/* Windows automatically destroys the mutexes when
		 * the last handle closes.
		 */
#elif defined(MDB_USE_POSIX_SEM)
		if (env->me_rmutex != SEM_FAILED) {
			sem_close(env->me_rmutex);
			if (env->me_wmutex != SEM_FAILED)
				sem_close(env->me_wmutex);
			if (env->me_sync_mutex != SEM_FAILED)
				sem_close(env->me_sync_mutex);
			/* If we have the filelock:	If we are the
			 * only remaining user, clean up semaphores.
			 */
			if (excl == 0)
				mdb_env_excl_lock(env, &excl);
			if (excl > 0) {
				sem_unlink(MUTEXNAME(env, 'r'));
				sem_unlink(MUTEXNAME(env, 'w'));
				sem_unlink(MUTEXNAME(env, 's'));
			}
		}
#elif defined(MDB_USE_SYSV_SEM)
		if (env->me_rmutex->semid != -1) {
			/* If we have the filelock:	If we are the
			 * only remaining user, clean up semaphores.
			 */
			if (excl == 0)
				mdb_env_excl_lock(env, &excl);
			if (excl > 0)
				semctl(env->me_rmutex->semid, 0, IPC_RMID);
		}
#elif defined(MDB_ROBUST_SUPPORTED)
		/* If we have the filelock:	If we are the
		 * only remaining user, clean up robust
		 * mutexes.
		 */
		if (excl == 0)
			mdb_env_excl_lock(env, &excl);
		if (excl > 0) {
			pthread_mutex_destroy(env->me_txns->mti_rmutex);
			pthread_mutex_destroy(env->me_txns->mti_wmutex);
			pthread_mutex_destroy(env->me_txns->mti_sync_mutex);
		}
#endif
		munmap((void *)env->me_txns, (env->me_maxreaders-1)*sizeof(MDB_reader)+sizeof(MDB_txninfo));
	}
	if (env->me_lfd != INVALID_HANDLE_VALUE) {
#ifdef _WIN32
		if (excl >= 0) {
			/* Unlock the lockfile.	Windows would have unlocked it
			 * after closing anyway, but not necessarily at once.
			 */
			UnlockFile(env->me_lfd, 0, 0, 1, 0);
		}
#endif
		(void) close(env->me_lfd);
	}
#if MDB_RPAGE_CACHE
	if (MDB_REMAPPING(env->me_flags))
	{
#ifdef _WIN32
	if (env->me_fmh) CloseHandle(env->me_fmh);
	if (env->me_rpmutex) CloseHandle(env->me_rpmutex);
#else
	pthread_mutex_destroy(&env->me_rpmutex);
#endif
	}
#endif

	env->me_flags &= ~(MDB_ENV_ACTIVE|MDB_ENV_TXKEY);
}

void ESECT
mdb_env_close(MDB_env *env)
{
	MDB_page *dp;

	if (env == NULL)
		return;

	VGMEMP_DESTROY(env);
	while ((dp = env->me_dpages) != NULL) {
		VGMEMP_DEFINED(&dp->mp_next, sizeof(dp->mp_next));
		env->me_dpages = dp->mp_next;
		free(dp);
	}

	mdb_env_close_active(env, 0);
#if MDB_RPAGE_CACHE
	free(env->me_enckey.mv_data);
#endif
	free(env);
}

/** Compare two items pointing at aligned #mdb_size_t's */
static int
mdb_cmp_long(const MDB_val *a, const MDB_val *b)
{
	return (*(mdb_size_t *)a->mv_data < *(mdb_size_t *)b->mv_data) ? -1 :
		*(mdb_size_t *)a->mv_data > *(mdb_size_t *)b->mv_data;
}

/** Compare two items pointing at aligned unsigned int's.
 *
 *	This is also set as #MDB_INTEGERDUP|#MDB_DUPFIXED's #MDB_dbx.%md_dcmp,
 *	but #mdb_cmp_clong() is called instead if the data type is #mdb_size_t.
 */
static int
mdb_cmp_int(const MDB_val *a, const MDB_val *b)
{
	return (*(unsigned int *)a->mv_data < *(unsigned int *)b->mv_data) ? -1 :
		*(unsigned int *)a->mv_data > *(unsigned int *)b->mv_data;
}

/** Compare two items pointing at unsigned ints of unknown alignment.
 *	Nodes and keys are guaranteed to be 2-byte aligned.
 */
static int
mdb_cmp_cint(const MDB_val *a, const MDB_val *b)
{
#if BYTE_ORDER == LITTLE_ENDIAN
	unsigned short *u, *c;
	int x;

	u = (unsigned short *) ((char *) a->mv_data + a->mv_size);
	c = (unsigned short *) ((char *) b->mv_data + a->mv_size);
	do {
		x = *--u - *--c;
	} while(!x && u > (unsigned short *)a->mv_data);
	return x;
#else
	unsigned short *u, *c, *end;
	int x;

	end = (unsigned short *) ((char *) a->mv_data + a->mv_size);
	u = (unsigned short *)a->mv_data;
	c = (unsigned short *)b->mv_data;
	do {
		x = *u++ - *c++;
	} while(!x && u < end);
	return x;
#endif
}

/** Compare two items lexically */
static int
mdb_cmp_memn(const MDB_val *a, const MDB_val *b)
{
	int diff;
	ssize_t len_diff;
	unsigned int len;

	len = a->mv_size;
	len_diff = (ssize_t) a->mv_size - (ssize_t) b->mv_size;
	if (len_diff > 0) {
		len = b->mv_size;
		len_diff = 1;
	}

	diff = memcmp(a->mv_data, b->mv_data, len);
	return diff ? diff : len_diff<0 ? -1 : len_diff;
}

/** Compare two items in reverse byte order */
static int
mdb_cmp_memnr(const MDB_val *a, const MDB_val *b)
{
	const unsigned char	*p1, *p2, *p1_lim;
	ssize_t len_diff;
	int diff;

	p1_lim = (const unsigned char *)a->mv_data;
	p1 = (const unsigned char *)a->mv_data + a->mv_size;
	p2 = (const unsigned char *)b->mv_data + b->mv_size;

	len_diff = (ssize_t) a->mv_size - (ssize_t) b->mv_size;
	if (len_diff > 0) {
		p1_lim += len_diff;
		len_diff = 1;
	}

	while (p1 > p1_lim) {
		diff = *--p1 - *--p2;
		if (diff)
			return diff;
	}
	return len_diff<0 ? -1 : len_diff;
}

/** Search for key within a page, using binary search.
 * Returns the smallest entry larger or equal to the key.
 * If exactp is non-null, stores whether the found entry was an exact match
 * in *exactp (1 or 0).
 * Updates the cursor index with the index of the found entry.
 * If no entry larger or equal to the key is found, returns NULL.
 */
static MDB_node *
mdb_node_search(MDB_cursor *mc, MDB_val *key, int *exactp)
{
	unsigned int	 i = 0, nkeys;
	int		 low, high;
	int		 rc = 0;
	MDB_page *mp = mc->mc_pg[mc->mc_top];
	MDB_node	*node = NULL;
	MDB_val	 nodekey;
	MDB_cmp_func *cmp;
	DKBUF;

	nkeys = NUMKEYS(mp);

	DPRINTF(("searching %u keys in %s %spage %"Yu,
			nkeys, IS_LEAF(mp) ? "leaf" : "branch", IS_SUBP(mp) ? "sub-" : "",
			mdb_dbg_pgno(mp)));

	low = IS_LEAF(mp) ? 0 : 1;
	high = nkeys - 1;
	cmp = mc->mc_dbx->md_cmp;

	/* Branch pages have no data, so if using integer keys,
	 * alignment is guaranteed. Use faster mdb_cmp_int.
	 */
	if (cmp == mdb_cmp_cint && IS_BRANCH(mp)) {
		if (NODEPTR(mp, 1)->mn_ksize == sizeof(mdb_size_t))
			cmp = mdb_cmp_long;
		else
			cmp = mdb_cmp_int;
	}

	if (IS_LEAF2(mp)) {
		nodekey.mv_size = mc->mc_db->md_pad;
		node = NODEPTR(mp, 0);	/* fake */
		while (low <= high) {
			i = (low + high) >> 1;
			nodekey.mv_data = LEAF2KEY(mp, i, nodekey.mv_size);
			rc = cmp(key, &nodekey);
			DPRINTF(("found leaf index %u [%s], rc = %i",
					i, DKEY(&nodekey), rc));
			if (rc == 0)
				break;
			if (rc > 0)
				low = i + 1;
			else
				high = i - 1;
		}
	} else {
		while (low <= high) {
			i = (low + high) >> 1;

			node = NODEPTR(mp, i);
			nodekey.mv_size = NODEKSZ(node);
			nodekey.mv_data = NODEKEY(node);

			rc = cmp(key, &nodekey);
#if MDB_DEBUG
			if (IS_LEAF(mp))
				DPRINTF(("found leaf index %u [%s], rc = %i",
						i, DKEY(&nodekey), rc));
			else
				DPRINTF(("found branch index %u [%s -> %"Yu"], rc = %i",
						i, DKEY(&nodekey), NODEPGNO(node), rc));
#endif
			if (rc == 0)
				break;
			if (rc > 0)
				low = i + 1;
			else
				high = i - 1;
		}
	}

	if (rc > 0) {	/* Found entry is less than the key. */
		i++;	/* Skip to get the smallest entry larger than key. */
		if (!IS_LEAF2(mp))
			node = NODEPTR(mp, i);
	}
	if (exactp)
		*exactp = (rc == 0 && nkeys > 0);
	/* store the key index */
	mc->mc_ki[mc->mc_top] = i;
	if (i >= nkeys)
		/* There is no entry larger or equal to the key. */
		return NULL;

	/* nodeptr is fake for LEAF2 */
	return node